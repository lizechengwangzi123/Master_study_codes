{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5dFJaXgWNso"
      },
      "source": [
        "# COMP5046 Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIbJtBc1WY7t"
      },
      "source": [
        "## 1. Data Download and Load\n",
        "\n",
        "\n",
        "Use the re3d dataset from Defense Science and Technology Laboratory, U.K., which focuses on named entity extraction relevant to somebody operating in the role of a defence and security intelligent analyst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV4CSVlCWEJ4",
        "outputId": "498f85b5-eb00-4377-e474-ec4557e24d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '16HRH0MKKq08lbBOzPa5ctpOcA9VLXv0p'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('train.csv')\n",
        "\n",
        "id = '1c0hihZpv6r3Ldi5lSEmEoW0aeua5LkPf'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('val.csv')\n",
        "\n",
        "id = '1sFb9c2n4UuleIwjuojc6w3YiTraVfSJw'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('test_without_labels.csv')\n",
        "\n",
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXHe8hxEGBpj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create Dataframes for each file\n",
        "df_train = pd.read_csv('train.csv', sep=\",\")\n",
        "df_val = pd.read_csv('val.csv', sep=\",\")\n",
        "df_test_without_labels = pd.read_csv('test_without_labels.csv', sep=\",\")\n",
        "\n",
        "# Convert the columns to the lists\n",
        "# Training Data\n",
        "train_sents = list(df_train[\"sents\"])\n",
        "train_labels = list(df_train[\"labels\"])\n",
        "# Validation Data\n",
        "val_sents = list(df_val[\"sents\"])\n",
        "val_labels = list(df_val[\"labels\"])\n",
        "# Test Data\n",
        "test_sents = list(df_test_without_labels[\"sents\"])\n",
        "\n",
        "train_labels = [label.split(' ') for label in train_labels]\n",
        "val_labels = [label.split(' ') for label in val_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhf7FidzgRk"
      },
      "source": [
        "## Data Preprocessing - Extract Word Features And Tokenising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juxv_Qj-zIEx",
        "outputId": "d41517b6-6006-40d9-a4d3-7f3278ce17c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_features(sents):\n",
        "    pos = []\n",
        "    lemma = []\n",
        "    dep = []\n",
        "    ent = []\n",
        "    \n",
        "    for sent in sents:\n",
        "      pos_temp = []\n",
        "      lemma_temp = []\n",
        "      dep_temp = []\n",
        "      ent_temp = []\n",
        "      \n",
        "      for word in nlp(sent):\n",
        "        pos_temp.append(word.tag_)\n",
        "        lemma_temp.append(word.lemma_)\n",
        "        dep_temp.append(word.dep_)\n",
        "        ent_temp.append(word.ent_type_)\n",
        "      \n",
        "      pos.append(pos_temp)\n",
        "      lemma.append(lemma_temp)\n",
        "      dep.append(dep_temp)\n",
        "      ent.append(ent_temp)\n",
        "        \n",
        "    return pos, lemma, dep, ent\n",
        "\n",
        "train_pos, train_lemma, train_dep, train_ent = extract_features(train_sents)\n",
        "val_pos, val_lemma, val_dep, val_ent = extract_features(val_sents)\n",
        "test_pos, test_lemma, test_dep, test_ent = extract_features(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2X8fxF1EXZ"
      },
      "source": [
        "## Data Preprocessing - Encoding and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1VBaEQQ1DhX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "PAD_TAG = \"<PAD>\"\n",
        "UNKNOWN_TAG = \"<UNKNOWN>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUb7X07SybCY"
      },
      "source": [
        "#### Text Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATz_MxeTy8YY"
      },
      "outputs": [],
      "source": [
        "word_to_ix = {}\n",
        "\n",
        "for sentence in train_lemma + val_lemma + test_lemma:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_to_ix[START_TAG] = len(word_to_ix)\n",
        "word_to_ix[STOP_TAG] = len(word_to_ix)\n",
        "word_to_ix[PAD_TAG] = len(word_to_ix)\n",
        "word_to_ix[UNKNOWN_TAG] = len(word_to_ix)\n",
        "\n",
        "word_list = list(word_to_ix.keys())\n",
        "\n",
        "pos_to_ix = {}\n",
        "for sentence in train_pos + val_pos + test_pos:\n",
        "    for pos in sentence:\n",
        "        if pos not in pos_to_ix:\n",
        "            pos_to_ix[pos] = len(pos_to_ix)\n",
        "\n",
        "dep_to_ix = {}\n",
        "for sentence in train_dep + val_dep + test_dep:\n",
        "    for dep in sentence:\n",
        "        if dep not in dep_to_ix:\n",
        "            dep_to_ix[dep] = len(dep_to_ix)\n",
        "\n",
        "ent_to_ix = {}\n",
        "for sentence in train_ent + val_ent + test_ent:\n",
        "    for ent in sentence:\n",
        "        if ent not in ent_to_ix:\n",
        "            ent_to_ix[ent] = len(ent_to_ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjYO_V-yq_Ln"
      },
      "source": [
        "#### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_zDMShNRJXC",
        "outputId": "2e5c2f52-7d38-4a03-aaf4-fc0722858e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B-DocumentReference', 'B-Location', 'B-MilitaryPlatform', 'B-Money', 'B-Nationality', 'B-Organisation', 'B-Person', 'B-Quantity', 'B-Temporal', 'B-Weapon', 'I-DocumentReference', 'I-Location', 'I-MilitaryPlatform', 'I-Money', 'I-Nationality', 'I-Organisation', 'I-Person', 'I-Quantity', 'I-Temporal', 'I-Weapon', 'O']\n",
            "{'B-DocumentReference': 0, 'B-Location': 1, 'B-MilitaryPlatform': 2, 'B-Money': 3, 'B-Nationality': 4, 'B-Organisation': 5, 'B-Person': 6, 'B-Quantity': 7, 'B-Temporal': 8, 'B-Weapon': 9, 'I-DocumentReference': 10, 'I-Location': 11, 'I-MilitaryPlatform': 12, 'I-Money': 13, 'I-Nationality': 14, 'I-Organisation': 15, 'I-Person': 16, 'I-Quantity': 17, 'I-Temporal': 18, 'I-Weapon': 19, 'O': 20, '<START>': 21, '<STOP>': 22, '<PAD>': 23}\n"
          ]
        }
      ],
      "source": [
        "# Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#-----------------------------------\n",
        "#|Encoded|   Entity                |\n",
        "#-----------------------------------\n",
        "#|   0   |   'B-DocumentReference' | \n",
        "#|   1   |   'B-Location'          |\n",
        "#|   2   |   'B-MilitaryPlatform'  |\n",
        "#|   3   |   'B-Money'             |\n",
        "#|   4   |   'B-Nationality'       |\n",
        "#|   5   |   'B-Organisation'      |\n",
        "#|   6   |   'B-Person'            |\n",
        "#|   7   |   'B-Quantity'          |\n",
        "#|   8   |   'B-Temporal'          |\n",
        "#|   9   |   'B-Weapon'            |\n",
        "#|  10   |   'I-DocumentReference' | \n",
        "#|  11   |   'I-Location'          |\n",
        "#|  12   |   'I-MilitaryPlatform'  |\n",
        "#|  13   |   'I-Money'             |\n",
        "#|  14   |   'I-Nationality'       |\n",
        "#|  15   |   'I-Organisation'      |\n",
        "#|  16   |   'I-Person'            |\n",
        "#|  17   |   'I-Quantity'          |\n",
        "#|  18   |   'I-Temporal'          |\n",
        "#|  19   |   'I-Weapon'            |\n",
        "#|  20   |   'O'                   |\n",
        "#|  21   |   '<START>'             |\n",
        "#|  22   |   '<STOP>'              |\n",
        "#|  23   |   '<PAD>'               |\n",
        "#-----------------------------------\n",
        "\n",
        "#unique_labels = np.unique(np.unique(np.array(train_labels).flatten()) + np.unique(np.array(val_labels).flatten()))\n",
        "unique_labels_list = np.unique(np.concatenate((np.array(train_labels, dtype=object), np.array(val_labels, dtype=object)), axis=None))\n",
        "\n",
        "unique_labels = []\n",
        "for label in unique_labels_list:\n",
        "  for s in label:\n",
        "    unique_labels.append(s)\n",
        "\n",
        "unique_labels = np.unique(unique_labels)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "classes = list(lEnc.classes_)\n",
        "print(classes)\n",
        "\n",
        "label_to_idx = {t: i for i, t in enumerate(list(classes))}\n",
        "label_to_idx[START_TAG] = len(label_to_idx)\n",
        "label_to_idx[STOP_TAG] = len(label_to_idx)\n",
        "label_to_idx[PAD_TAG] = len(label_to_idx)\n",
        "print(label_to_idx)\n",
        "\n",
        "train_labels_encoded = []\n",
        "val_labels_encoded = []\n",
        "\n",
        "for label in train_labels:\n",
        "  train_labels_encoded.append(lEnc.transform(label))\n",
        "\n",
        "for label in val_labels:\n",
        "  val_labels_encoded.append(lEnc.transform(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_354AHyTtK"
      },
      "source": [
        "#### Align With The Labels\n",
        "\n",
        "NOTE: **You may run the snippet of code for multiple times.** Mind to run this snippet of code anytime when the length of the sentences are **not same** with the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljxDt7uwxQ4c",
        "outputId": "bea390d1-fb2a-41dc-ea6c-4759cbf3e3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "573\n",
            "[[0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 4, 5, 6, 7, 3, 8, 9, 3, 10, 9, 7, 0, 11, 12, 13, 6, 3, 9, 7, 10, 14, 6, 3, 15, 14, 6, 7, 3, 16], [6, 3, 17, 10, 9, 0, 15, 0, 3, 18, 6, 3, 3, 15, 3, 3, 16], [6, 3, 5, 7, 9, 9, 19, 5, 6, 7, 7, 3, 3, 9, 6, 0, 0, 0, 16], [6, 5, 20, 7, 3, 9, 0, 15, 6, 7, 3, 12, 13, 15, 13, 7, 10, 9, 6, 0, 15, 0, 9, 6, 3, 9, 10, 7, 9, 19, 6, 16], [6, 3, 5, 8, 9, 0, 0, 0, 0, 21, 0, 16, 22], [15, 23, 18, 9, 14, 18, 19, 24, 13, 6, 16, 22, 22, 22], [15, 3, 24, 13, 23, 16], [6, 10, 9, 6, 0, 0, 25, 8, 9, 6, 3, 9, 7, 0, 0, 0, 12, 13, 6, 7, 0, 0, 21, 0, 16], [0], [19, 25, 20, 26, 10, 9, 6, 10, 9, 6, 10, 15, 3, 9, 6, 7, 3, 9, 27, 6, 8, 16], [6, 3, 9, 0, 9, 3, 15, 7, 7, 7, 10, 5, 23, 11, 10, 23, 16], [28, 6, 3, 9, 14, 20, 3, 17, 23, 8, 9, 10, 17, 23, 10, 9, 3, 23, 16], [28, 6, 3, 22, 3, 3, 17, 29, 19, 25, 23, 23, 18, 22, 5, 0, 0, 0, 0, 18, 7, 0, 0, 0], [6, 3, 17, 9, 3, 16], [0, 0, 0, 0, 3, 9, 6, 0, 23, 14, 8, 3, 9, 7, 0, 16], [6, 3, 5, 6, 11, 10, 5, 6, 7, 10, 9, 6, 0, 3, 18, 14, 6, 3, 12, 13, 6, 7, 3, 9, 6, 7, 3, 15, 6, 0, 7, 3, 16], [0, 0, 0, 18, 0, 0, 18, 5, 2], [19, 5, 20, 7, 3, 24, 13, 6, 3, 9, 10, 15, 14, 3, 9, 7, 3, 16], [3, 21, 8, 3, 5, 9, 3, 3, 3, 15, 9, 3, 3, 12, 13, 10, 25, 23, 8, 9, 6, 3, 9, 8, 9, 0, 21, 0, 16], [6, 3, 9, 6, 3, 10, 9, 6, 3, 9, 7, 3, 16, 22], [19, 25, 8, 3, 15, 3, 9, 6, 7, 3, 9, 11, 3, 16], [19, 25, 14, 9, 6, 0, 9, 0, 12, 13, 6, 7, 3, 9, 6, 7, 3, 16], [9, 6, 7, 3, 23, 17, 19, 30, 9, 14, 10, 9, 6, 7, 10, 18, 19, 25, 6, 7, 3, 23, 18, 15, 25, 7, 12, 13, 9, 6, 7, 3, 31, 17, 6, 3, 6, 3, 16], [0, 9, 6, 0, 0, 18, 0, 0, 5, 2], [9, 11, 9, 20, 0, 10, 18, 0, 17, 19, 9, 23, 23, 9, 14, 10, 18, 10, 15, 10, 14, 9, 7, 15, 7, 10, 18, 7, 23, 23, 5, 9, 6, 3, 9, 19, 16], [6, 0, 0, 17, 8, 6, 7, 3, 3, 32, 5, 8, 9, 0, 9, 0, 0, 16], [6, 7, 3, 9, 20, 3, 8, 9, 6, 7, 3, 9, 7, 15, 7, 10, 31, 23, 25, 6, 3, 23, 8, 9, 20, 7, 18, 14, 3, 10, 15, 10, 14, 3, 9, 3, 9, 7, 10, 9, 10, 16], [15, 0, 17, 19, 17, 6, 3, 9, 6, 10, 9, 10, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [9, 6, 7, 11, 10, 18, 6, 0, 21, 8, 3, 22, 33, 11, 10, 5, 11, 10, 9, 0, 10, 18, 31, 25, 11, 7, 10, 18, 6, 3, 21, 15, 21, 3, 7, 15, 11, 3, 10, 9, 3, 9, 0, 10, 12, 13, 0, 18, 0, 5, 16], [9, 3, 9, 6, 10, 15, 10, 9, 0, 0, 0, 18, 19, 25, 20, 7, 3, 15, 20, 26, 10, 9, 20, 3, 18, 10, 15, 3, 3, 10, 16, 22], [19, 17, 3, 25, 12, 13, 9, 6, 7, 3, 23, 9, 6, 0, 0, 3, 16], [6, 3, 9, 0, 18, 29, 7, 18, 24, 13, 23, 7, 3, 14, 0, 15, 13, 6, 3, 9, 6, 3, 16], [19, 25, 7, 9, 6, 3, 9, 20, 7, 3, 31, 17, 6, 7, 3, 9, 0, 9, 0, 15, 0, 16, 22], [6, 7, 3, 17, 8, 9, 6, 0, 3, 17, 8, 7, 10, 9, 6, 7, 10, 9, 23, 34, 11, 10, 18, 15, 9, 0, 17, 8, 7, 10, 9, 26, 23, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [19, 24, 6, 13, 9, 14, 12, 13, 36, 20, 3, 18, 14, 20, 3, 9, 3, 15, 14, 6, 3, 9, 7, 10, 18, 9, 3, 9, 6, 7, 3, 19, 25, 14, 9, 6, 7, 3, 16], [0, 0, 0, 0, 0, 18, 3, 18, 0, 0, 0, 23, 5, 9, 0, 0, 0, 7, 10, 15, 5, 6, 3, 9, 6, 0, 3, 18, 6, 7, 3, 18, 15, 7, 3, 16], [6, 3, 3, 23, 5, 36, 6, 7, 7, 10, 9, 6, 3, 16, 1, 11, 4], [28, 28, 28, 19, 17, 23, 7, 12, 23, 13, 27, 6, 10, 19, 25, 14, 18, 23, 6, 14, 10, 24, 13, 8, 9, 7, 15, 23, 7, 18, 22, 22, 19, 5, 16, 22], [28, 6, 0, 21, 3, 3, 3, 18, 0, 0, 0, 0, 0, 0, 18, 5, 19, 7, 9, 20, 0, 7, 3, 3, 9, 19, 24, 13, 6, 3, 9, 19, 5, 6, 3, 12, 7, 7, 0, 10, 18, 22, 5, 0, 0, 0, 33, 0, 33, 0, 18, 6, 0, 21, 3, 3], [0, 5, 0, 0, 0, 22, 33, 7, 10, 2], [9, 6, 23, 26, 3, 9, 6, 0, 7, 10, 18, 6, 0, 17, 8, 9, 11, 10, 18, 8, 9, 11, 3, 10, 9, 0, 10, 18, 15, 17, 23, 14, 6, 7, 11, 10, 12, 13, 9, 0, 21, 0, 3, 9, 6, 0, 0, 14, 19, 12, 13, 3, 23, 15, 13, 19, 9, 7, 10, 16], [19, 5, 8, 9, 7, 3, 0, 0, 0, 9, 6, 3, 9, 7, 3, 10, 9, 0, 11, 18, 15, 8, 9, 14, 0, 0, 0, 21, 0, 33, 3, 3, 31, 0, 5, 16, 1, 11, 4, 0, 5, 6, 3, 9, 19, 5, 28, 7, 22, 16], [6, 35, 23, 25, 20, 3, 17, 23, 15, 23, 23, 16], [28, 9, 6, 0, 18, 37, 13, 7, 9, 38, 25, 3, 10, 7, 12, 13, 36, 6, 3, 12, 13, 20, 3, 9, 0, 33, 33, 7, 15, 7, 3, 16, 22], [6, 3, 23, 17, 6, 3, 9, 0, 22, 0, 7, 3, 9, 0, 16], [15, 9, 7, 35, 32, 5, 9, 6, 11, 3, 5, 0, 0, 18, 0, 17, 19, 24, 13, 16], [9, 0, 18, 0, 11, 18, 3, 3, 9, 0, 21, 0, 5, 11, 0, 10, 15, 7, 3, 8, 9, 0, 18, 0, 16], [19, 5, 8, 19, 24, 13, 36, 10, 18, 13, 10, 9, 35, 15, 13, 8, 12, 13, 23, 18, 14, 36, 3, 16], [19, 5, 23, 8, 9, 6, 10, 9, 0, 22, 3, 3, 9, 0, 15, 8, 9, 3, 10, 18, 3, 15, 10, 9, 6, 0, 21, 0, 3, 16], [9, 0, 11, 6, 0, 0, 5, 3, 12, 13, 6, 0, 0, 0, 23, 9, 6, 3, 3, 16], [6, 0, 0, 24, 13, 6, 19, 24, 12, 13, 6, 0, 3, 12, 13, 10, 9, 3, 9, 0, 15, 13, 7, 3, 16], [6, 0, 0, 23, 17, 6, 7, 10, 3, 9, 6, 10, 9, 0, 9, 6, 7, 3, 9, 0, 18, 15, 6, 7, 3, 9, 0, 18, 9, 31, 0, 22, 0, 17, 8, 3, 15, 29, 39, 9, 11, 10, 5, 8, 15, 8, 9, 7, 10, 31, 5, 3, 10, 15, 6, 3, 16], [9, 0, 11, 18, 11, 18, 6, 3, 3, 8, 9, 6, 0, 0, 0, 5, 20, 3, 16], [9, 0, 18, 6, 17, 23, 6, 7, 3, 9, 0, 16], [9, 11, 18, 9, 14, 9, 6, 3, 9, 20, 7, 3, 18, 19, 5, 8, 15, 8, 9, 0, 21, 8, 3, 3, 16, 1, 11, 4], [6, 3, 5, 19, 5, 8, 8, 6, 3, 14, 9, 6, 3, 16], [6, 3, 3, 5, 8, 9, 11, 16], [0, 10, 5, 9, 6, 3, 5, 11, 10, 15, 5, 11, 39, 10, 18, 15, 8, 19, 7, 16], [6, 0, 9, 0, 5, 3, 6, 3, 9, 6, 3, 32, 5, 14, 0, 0, 0, 16], [6, 3, 17, 7, 9, 6, 0, 3, 17, 23, 8, 12, 13, 13, 13, 9, 6, 3, 16], [0, 7, 10, 3, 25, 8, 10, 9, 10, 14, 8, 9, 0, 0, 10, 9, 7, 3, 10, 7, 36, 9, 0, 16], [6, 3, 15, 20, 10, 18, 23, 0, 18, 25, 14, 6, 26, 3, 8, 23, 9, 0, 15, 7, 10, 15, 10, 9, 0, 18, 14, 7, 7, 3, 16], [14, 9, 10, 9, 0, 10, 25, 8, 6, 0, 0, 0, 0, 9, 0, 9, 0, 18, 0, 0, 0, 0, 0, 5, 2], [6, 0, 18, 9, 20, 0, 0, 9, 0, 0, 18, 24, 13, 23, 8, 9, 6, 7, 3, 12, 13, 6, 10, 9, 0, 0, 9, 7, 18, 7, 10, 23, 23, 9, 7, 16], [3, 2, 0, 0, 9, 0, 0, 0, 24, 13, 9, 0, 9, 3, 22, 33, 0, 0, 18, 8, 9, 6, 7, 3, 18, 12, 13, 10, 12, 13, 6, 3, 9, 0, 16], [28, 9, 3, 9, 6, 10, 18, 6, 3, 9, 6, 7, 11, 10, 8, 11, 10, 18, 14, 10, 9, 6, 7, 10, 15, 6, 3, 3, 31, 17, 0, 22, 33, 14, 7, 3, 16, 22], [0, 17, 23, 6, 9, 20, 11, 40, 11, 10, 9, 21, 3, 16], [28, 9, 11, 0, 18, 35, 9, 0, 0, 18, 7, 35, 15, 7, 3, 18, 7, 35, 5, 20, 3, 9, 3, 21, 8, 0, 18, 15, 5, 8, 6, 3, 11, 10, 23, 16], [28, 9, 19, 25, 22, 3, 25, 23, 18, 10, 18, 10, 18, 15, 10, 5, 22, 3, 13, 23, 9, 6, 3, 16], [6, 0, 0, 5, 6, 0, 0, 9, 0, 15, 7, 10, 9, 10, 9, 6, 7, 3, 9, 0, 15, 9, 0, 16], [0, 5, 6, 10, 15, 7, 7, 3, 9, 0, 0, 0, 9, 0, 18, 0, 18, 9, 11, 40, 11, 0], [2, 13, 6, 7, 3, 16], [6, 3, 5, 36, 7, 10, 9, 0, 18, 8, 20, 0, 21, 3, 3, 9, 6, 3, 15, 5, 19, 24, 13, 6, 3, 9, 6, 7, 3, 9, 6, 23, 21, 8, 0, 16], [13, 6, 3, 10, 9, 10, 14, 3, 21, 3, 18, 3, 18, 3, 15, 7, 3, 16], [6, 3, 17, 23, 6, 3, 9, 0, 22, 6, 28, 3, 22, 9, 0, 17, 6, 3, 15, 9, 19, 17, 19, 23, 16], [19, 25, 6, 3, 22, 33, 3, 24, 13, 23, 9, 6, 3, 16], [9, 10, 9, 3, 15, 3, 10, 18, 6, 11, 10, 5, 7, 12, 13, 6, 10, 9, 6, 3, 9, 10, 15, 10, 9, 10, 9, 14, 20, 3, 23, 5, 6, 7, 3, 9, 6, 10, 12, 13, 6, 7, 10, 16], [15, 18, 6, 0, 11, 0, 3, 5, 6, 3, 3, 19, 18, 19, 5, 16], [9, 0, 33, 33, 3, 18, 0, 5, 10, 9, 20, 3, 18, 0, 0, 0, 21, 0, 18, 15, 23, 15, 9, 6, 3, 16], [15, 9, 0, 18, 10, 9, 0, 25, 8, 12, 13, 8, 11, 7, 7, 3, 10, 19, 5, 8, 14, 9, 6, 3, 9, 0, 16], [16, 9, 0, 18, 11, 10, 5, 11, 0, 7, 10, 18, 5, 6, 3, 3, 18, 6, 7, 3, 3, 18, 11, 0, 21, 8, 10, 18, 11, 35, 18, 6, 0, 3, 18, 11, 10, 10, 18, 11, 7, 10, 18, 6, 3, 3, 18, 15, 6, 3, 18, 8, 11, 3, 10, 2, 15, 5, 11, 3, 10, 15, 6, 0, 7, 3, 16], [6, 0, 17, 12, 13, 3, 12, 13, 9, 6, 3, 15, 10, 9, 0, 9, 6, 3, 9, 3, 16], [6, 3, 5, 12, 13, 6, 7, 3, 10, 17, 8, 9, 28, 0, 22, 15, 0, 0, 0, 16], [0, 0, 24, 13, 9, 6, 3, 9, 6, 0, 0, 0, 15, 13, 8, 36, 12, 13, 20, 10, 9, 0, 0, 16, 0, 0, 32, 24, 23, 13, 9, 6, 3, 9, 6, 0], [8, 10, 9, 6, 3, 5, 7, 12, 13, 3, 9, 7, 10, 9, 6, 3, 3, 9, 7, 3, 10, 16], [6, 3, 19, 17, 9, 6, 7, 3, 18, 19, 17, 36, 9, 10, 23, 16], [8, 11, 10, 18, 14, 11, 10, 16], [19, 5, 8, 15, 8, 9, 6, 7, 7, 7, 3, 8, 9, 6, 0, 3, 9, 0, 11, 18, 11, 16, 1, 11, 4, 1, 11, 4], [23, 23, 38, 25, 7, 10, 23, 15, 6, 3, 17, 8, 6, 7, 3, 23, 9, 14, 6, 7, 3, 16], [0, 5, 6, 3, 28, 3, 3, 9, 6, 0, 21, 0, 3, 9, 0, 18, 6, 0, 21, 8, 3, 23, 11, 10, 1, 11, 10, 4, 23, 9, 0, 18, 5, 8, 8, 9, 0, 7, 10, 29, 19, 5, 8, 9, 6, 3], [19, 17, 8, 12, 13, 10, 15, 13, 10, 9, 7, 10, 15, 17, 9, 7, 3, 16], [19, 25, 23, 7, 15, 8, 9, 10, 9, 0, 17, 8, 11, 10, 9, 6, 0, 0, 0, 3, 9, 0, 3, 18, 0, 16], [19, 23, 17, 7, 18, 7, 18, 7, 15, 7, 3, 18, 3, 15, 3, 16], [6, 7, 3, 2, 14, 0, 15, 0, 2, 24, 23, 13, 12, 13, 6, 7, 3, 9, 35, 12, 13, 13, 6, 3, 9, 6, 7, 3, 8, 9, 7, 3, 16, 22], [6, 0, 0, 18, 0, 0, 18, 5, 2], [9, 20, 0, 3, 3, 24, 13, 7, 18, 35, 25, 7, 10, 14, 12, 13, 20, 10, 18, 15, 19, 25, 12, 13, 20, 7, 10, 12, 13, 15, 13, 6, 0, 13, 20, 7, 3, 16], [15, 11, 10, 9, 20, 3, 5, 9, 6, 14, 0, 3, 8, 36, 9, 6, 3, 16], [19, 25, 19, 13, 9, 3, 9, 0, 17, 8, 16], [28, 19, 22, 6, 7, 10, 25, 6, 3, 9, 7, 3, 18, 28, 0, 5, 16], [0, 0, 5, 8, 9, 6, 3, 9, 3, 10, 18, 10, 15, 7, 10, 9, 6, 0, 16], [9, 39, 3, 18, 10, 24, 13, 6, 0, 0, 0, 35, 0, 0, 35, 0, 9, 11, 21, 11, 21, 11, 16], [13, 41, 42, 9, 0, 15, 9, 0], [15, 3, 17, 23, 8, 19, 39, 16], [7, 10, 10, 25, 8, 9, 6, 3, 9, 7, 10, 24, 13, 9, 6, 7, 3, 12, 13, 0, 16], [19, 23, 5, 6, 7, 3, 9, 10, 9, 6, 0, 35, 9, 0, 18, 0, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 9, 6, 3, 9, 6, 3, 16], [28, 28, 28, 23, 6, 10, 25, 9, 3, 18, 22, 22, 19, 17, 16], [9, 20, 7, 3, 10, 15, 14, 9, 7, 7, 3, 9, 0, 15, 0, 18, 0, 17, 8, 10, 9, 10, 9, 7, 3, 18, 14, 19, 12, 13, 15, 13, 7, 10, 16], [3, 2, 24, 6, 0, 0, 25, 9, 6, 0, 3, 9, 0, 9, 0, 11, 18, 32, 24, 13, 40, 9, 31, 3, 18, 31, 7, 10, 24, 19, 13, 18, 43, 16, 16], [19, 25, 9, 6, 3, 16], [6, 7, 3, 5, 9, 3, 10, 9, 11, 10, 9, 3, 18, 14, 6, 3, 9, 0, 0, 0, 16], [9, 19, 25, 23, 8, 7, 18, 6, 0, 0, 17, 7, 3, 15, 3, 16], [6, 3, 5, 9, 0, 18, 15, 5, 36, 9, 6, 7, 3, 9, 6, 3, 9, 7, 17, 3, 15, 10, 10, 10, 16], [19, 5, 15, 5, 11, 10, 18, 14, 11, 3, 32, 5, 8, 16], [19, 5, 9, 0, 12, 13, 20, 3, 18, 6, 11, 21, 3, 21, 7, 3, 19, 5, 23, 9, 10, 5, 16], [6, 3, 24, 13, 6, 3, 9, 0, 10, 12, 13, 7, 10, 9, 0, 15, 0, 15, 12, 13, 10, 12, 23, 13, 6, 3, 9, 0, 22, 3, 9, 0, 15, 0, 15, 20, 3, 9, 6, 7, 10, 16], [19, 5, 19, 9, 14, 12, 13, 36, 7, 10, 9, 23, 16], [9, 6, 7, 21, 3, 0, 18, 6, 0, 0, 17, 7, 9, 3, 9, 0, 22, 19, 23, 8, 3, 15, 10, 2, 10, 8, 9, 0, 22, 33, 3, 16], [6, 3, 5, 6, 7, 3, 9, 3, 9, 7, 10, 9, 6, 7, 10, 9, 0, 3, 9, 0, 18, 0, 16], [19, 23, 17, 12, 13, 7, 3, 15, 3, 3, 2, 9, 6, 23, 17, 8, 30, 7, 9, 6, 3, 9, 6, 0, 0, 3, 15, 7, 7, 7, 10, 9, 0, 16], [6, 3, 5, 8, 9, 6, 3, 9, 6, 3, 18, 23, 9, 6, 3, 9, 6, 3, 16], [6, 0, 0, 23, 17, 6, 7, 7, 10, 9, 0, 3, 18, 14, 11, 9, 0, 5, 9, 0, 31, 23, 5, 10, 9, 6, 7, 3, 16], [28, 19, 25, 23, 8, 9, 6, 3, 9, 6, 0, 0, 9, 11, 9, 20, 7, 3, 10, 17, 8, 8, 9, 0, 9, 14, 19, 9, 6, 3, 9, 0, 18, 22, 0, 5, 9, 6, 0, 11, 3, 16], [16, 0, 0, 18, 11, 3, 5, 6, 0, 21, 8, 3, 16], [0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 11, 3, 4, 17, 6, 7, 7, 3, 3, 3, 31, 17, 9, 0, 18, 0, 18, 15, 0, 40, 0, 16], [0, 0, 0, 21, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 2, 5, 11, 0, 11, 4, 6, 3, 9, 0, 21, 0, 22, 19, 25, 18, 5, 6, 7, 1, 11, 4, 7, 7, 3, 15, 6, 3, 9, 6, 7, 7, 3, 9, 6, 0, 21, 0, 3, 9, 6, 0], [15, 10, 25, 8, 6, 7, 3, 9, 0, 9, 10, 8, 9, 8, 10, 2, 15, 10, 0, 10, 15, 10, 12, 13, 19, 16], [6, 7, 10, 25, 6, 7, 3, 9, 20, 7, 10, 9, 19, 2, 9, 3, 15, 3, 18, 7, 10, 18, 15, 7, 15, 7, 10, 16], [19, 17, 7, 9, 6, 7, 3, 9, 6, 3, 9, 3, 18, 7, 14, 9, 20, 3, 15, 9, 6, 35, 9, 0, 16, 1, 11, 4], [6, 17, 6, 7, 3, 2, 0, 17, 8, 8, 9, 20, 7, 15, 7, 3, 9, 6, 7, 3, 0, 16], [28, 38, 17, 23, 11, 3, 12, 13, 6, 3, 2, 6, 0, 21, 8, 7, 3, 8, 9, 6, 3, 23, 9, 6, 0, 3, 16], [28, 6, 7, 3, 23, 17, 7, 3, 9, 0, 18, 15, 6, 17, 6, 7, 3, 16], [9, 11, 0, 11, 18, 19, 5, 9, 19, 5, 8, 15, 5, 10, 9, 0, 15, 0, 18, 15, 5, 8, 11, 35, 9, 0, 11, 16, 1, 11, 4, 1, 11, 4], [6, 0, 3, 17, 6, 10, 9, 7, 10, 10, 23, 16], [0, 5, 6, 3, 9, 6, 3, 9, 0, 0, 33, 33, 3, 18, 15, 5, 3, 9, 6, 0, 21, 0, 0, 9, 11, 40, 11, 18, 15, 6, 3, 9, 0, 9, 11, 40, 11, 16], [10, 25, 6, 3, 9, 0, 5, 6, 3, 3, 9, 9, 6, 3, 9, 3, 15, 3, 9, 3, 16], [6, 3, 17, 6, 7, 3, 9, 6, 3, 9, 0, 12, 13, 39, 3, 9, 7, 3, 10, 16], [6, 7, 3, 24, 13, 8, 9, 6, 0, 35, 0, 3, 3, 8, 9, 6, 3, 9, 0, 9, 6, 0, 0, 0, 16], [19, 25, 23, 8, 39, 9, 11, 3, 9, 6, 3, 19, 23, 5, 23, 16], [28, 6, 3, 17, 8, 7, 10, 9, 6, 3, 18, 15, 19, 28, 13, 14, 12, 13, 7, 19, 25, 14, 6, 3, 12, 13, 6, 3, 9, 20, 10, 18, 22, 5, 0, 16, 0, 0, 0, 0, 18, 3, 18, 0, 0, 35, 0, 0, 0, 21, 0, 0, 0], [0, 9, 0, 0, 0], [19, 24, 13, 12, 13, 6, 7, 3, 12, 13, 15, 13, 0, 22, 3, 9, 9, 19, 24, 23, 23, 13, 6, 32, 25, 20, 8, 3, 16], [23, 17, 11, 18, 11, 16], [14, 6, 3, 9, 0, 6, 0, 0, 24, 13, 9, 6, 0, 18, 14, 0, 0, 9, 11, 0, 16], [6, 7, 10, 9, 7, 3, 25, 6, 7, 3, 9, 6, 3, 12, 13, 3, 10, 9, 0, 22, 3, 15, 12, 13, 6, 3, 6, 7, 3, 17, 16], [6, 3, 9, 0, 17, 6, 26, 3, 8, 9, 0, 9, 0, 16], [0, 9, 6, 0, 0, 0, 0, 10, 9, 10, 9, 10, 9, 6, 3, 9, 0, 8, 9, 0, 15, 0, 9, 11, 0, 16], [0, 9, 6, 0, 0, 0, 0, 10, 9, 6, 3, 9, 6, 7, 3, 12, 13, 6, 3, 9, 0, 9, 0, 16], [6, 3, 5, 9, 6, 3, 9, 6, 3, 18, 23, 11, 10, 1, 11, 10, 4, 9, 6, 7, 3], [7, 3, 10, 15, 8, 0, 7, 7, 10, 25, 8, 8, 9, 14, 7, 10, 9, 7, 10, 9, 0, 16], [16, 0, 0, 18, 11, 3, 5, 6, 0, 7, 3, 16], [9, 3, 9, 6, 0, 0, 35, 18, 0, 3, 5, 6, 3, 3, 9, 6, 3, 12, 13, 3, 10, 14, 9, 7, 10, 16], [0, 5, 3, 3, 15, 10, 9, 6, 3, 18, 31, 19, 5, 5, 23, 13, 6, 7, 10, 15, 10, 9, 6, 3, 16], [19, 25, 14, 10, 18, 10, 15, 10, 9, 7, 10, 18, 9, 9, 10, 15, 10, 14, 6, 7, 7, 10, 16], [6, 10, 5, 8, 9, 3, 9, 0, 0, 0, 18, 6, 3, 12, 13, 6, 0, 3, 3, 15, 6, 3, 19, 25, 9, 0, 18, 0, 18, 15, 6, 39, 7, 3, 16], [19, 23, 17, 9, 6, 10, 9, 10, 12, 13, 9, 19, 17, 9, 18, 9, 6, 3, 6, 3, 17, 36, 18, 6, 3, 39, 9, 6, 3, 9, 3, 9, 6, 3, 17, 16], [6, 17, 19, 24, 13, 6, 3, 9, 7, 3, 12, 13, 18, 15, 13, 9, 18, 0, 3, 9, 6, 7, 3, 16], [9, 6, 7, 11, 10, 18, 7, 10, 5, 20, 7, 10, 9, 0, 0, 9, 0, 15, 6, 7, 10, 9, 11, 10, 9, 0, 18, 6, 3, 22, 42, 23, 21, 26, 3, 18, 0, 3, 10, 3, 0, 0, 16, 0, 0, 5, 3], [0], [23, 23, 6, 10, 18, 9, 6, 3, 19, 25, 7, 2, 6, 0, 0, 17, 7, 9, 14, 6, 0, 0, 0, 9, 35, 9, 6, 3, 9, 6, 0, 0, 10, 9, 6, 7, 15, 7, 3, 9, 9, 0, 16], [3, 10, 25, 7, 10, 18, 14, 7, 18, 7, 18, 7, 15, 3, 16], [10, 5, 16], [16, 0, 0, 0, 0, 18, 11, 3, 5, 6, 3, 3, 3, 16], [0, 17, 14, 8, 23, 9, 6, 7, 3, 18, 6, 3, 9, 7, 10, 18, 15, 6, 3, 15, 3, 9, 20, 10, 16], [28, 28, 28, 19, 25, 12, 13, 9, 8, 21, 36, 10, 12, 13, 19, 9, 10, 16], [16, 0, 0, 18, 11, 10, 5, 6, 0, 7, 3, 15, 5, 11, 7, 10, 16], [6, 0, 0, 17, 7, 9, 6, 3, 16], [23, 34, 11, 0, 35, 15, 6, 7, 3, 9, 7, 10, 5, 16, 1, 11, 4, 1, 11, 4, 1, 11, 4, 6, 7, 3, 9, 0, 18, 0, 0, 0, 18, 5, 9, 7, 10, 5, 8, 11, 10, 8, 9, 6, 7, 3, 9, 6, 3, 16, 1, 11, 4], [6, 0, 17, 14, 9, 20, 10, 12, 13, 6, 7, 0, 0, 12, 13, 6, 7, 10, 9, 0, 22, 33, 3, 15, 0, 22, 33, 3, 16], [23, 23, 19, 17, 7, 9, 20, 3, 9, 6, 3, 9, 11, 3, 15, 6, 3, 9, 6, 7, 16], [28, 6, 7, 3, 21, 7, 3, 17, 20, 10, 9, 6, 3, 28, 33, 3, 18, 10, 15, 10, 18, 22, 0, 0, 0, 0, 0, 0, 0, 18, 3, 9, 0, 0, 0, 0, 21, 0, 0, 0, 18, 5, 9, 6, 3], [6, 3, 9, 7, 10, 12, 13, 6, 3, 9, 0, 17, 6, 3, 23, 9, 14, 0, 9, 0, 16], [0, 0, 17, 7, 0, 10, 10, 31, 25, 7, 3, 9, 0, 22, 17, 7, 3, 16], [7, 10, 5, 6, 0, 3, 5, 8, 10, 9, 6, 3, 3, 12, 13, 7, 10, 16], [19, 5, 8, 9, 0, 0, 21, 0, 9, 11, 18, 15, 5, 23, 8, 9, 20, 3, 0, 0, 21, 0, 18, 1, 11, 4, 32, 24, 23, 13, 6, 0, 9, 0, 16], [6, 17, 39, 9, 23, 6, 26, 3, 3, 9, 6, 7, 3, 16], [8, 9, 6, 3, 21, 8, 3, 18, 19, 28, 17, 23, 9, 6, 7, 3, 31, 17, 3, 18, 3, 15, 3, 9, 7, 9, 6, 8, 0, 3, 9, 0, 16], [28, 28, 6, 3, 9, 6, 0, 9, 0, 17, 23, 18, 8, 20, 7, 18, 7, 15, 7, 3, 16, 22, 22, 22], [0, 0, 0, 0, 9, 0, 17, 11, 9, 6, 26, 3, 10, 9, 0, 16], [6, 3, 18, 9, 8, 9, 6, 0, 10, 18, 17, 11, 15, 39, 7, 10, 31, 25, 9, 23, 6, 7, 7, 3, 12, 13, 6, 7, 18, 23, 7, 3, 9, 6, 3, 16], [23, 9, 0, 18, 20, 7, 10, 15, 6, 10, 9, 0, 18, 19, 24, 23, 13, 7, 3, 9, 3, 16], [19, 17, 23, 35, 7, 16, 1, 11, 4], [0, 0, 17, 3, 9, 0, 15, 0, 0, 10, 9, 3, 10, 15, 3, 3, 16], [28, 28, 7, 10, 24, 13, 9, 6, 5, 6, 7, 7, 3, 31, 5, 10, 15, 13, 20, 10, 16], [11, 3, 19, 5, 9, 5, 9, 9, 23, 11, 7, 10, 9, 3, 0, 24, 13, 9, 10, 16], [28, 0, 22, 17, 7, 15, 7, 3, 5, 9, 3, 9, 6, 26, 10, 9, 7, 3, 15, 23, 24, 23, 13, 8, 9, 19, 25, 12, 13, 3, 9, 15, 23, 13, 6, 10, 8, 9, 0, 16], [38, 24, 13, 6, 7, 3, 9, 0, 16], [0, 0, 1, 0, 2, 3, 0, 3, 3, 3, 0, 0, 0, 11, 3, 4, 18, 8, 0, 0, 0, 0, 21, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 3, 4, 23, 9, 11, 40, 11, 9, 0, 18, 5, 6, 7, 7, 7, 9, 0, 21, 0, 9, 0, 1, 0, 4, 16], [22], [16, 9, 0, 18, 11, 10, 5, 6, 0, 7, 3, 15, 5, 11, 0, 21, 8, 3, 15, 6, 3, 16], [28, 28, 28, 6, 19, 25, 17, 19, 28, 3, 23, 14, 8, 9, 18, 22, 22, 19, 17, 16, 22], [6, 3, 9, 6, 10, 25, 9, 6, 7, 3, 18, 23, 14, 19, 17, 23, 7, 18, 22, 22, 19, 17, 16, 22], [0, 21, 0, 3], [19, 23, 17, 6, 7, 10, 9, 31, 0, 22, 17, 3, 9, 6, 18, 15, 7, 18, 8, 10, 24, 13, 30, 8, 16], [6, 3, 17, 19, 6, 3, 15, 6, 7, 3, 23, 16], [6, 3, 17, 12, 13, 11, 3, 10, 9, 3, 9, 14, 10, 12, 13, 7, 3, 9, 6, 7, 3, 15, 12, 13, 10, 15, 10, 16], [9, 6, 3, 18, 10, 5, 6, 14, 10, 2], [23, 11, 18, 11, 9, 0, 33, 33, 3, 9, 11, 18, 11, 25, 8, 16], [19, 24, 13, 9, 0, 9, 7, 0, 6, 3, 9, 7, 10, 12, 13, 7, 3, 9, 6, 3, 3, 9, 0, 18, 15, 9, 3, 15, 3, 18, 9, 7, 3, 15, 3, 9, 6, 10, 31, 24, 13, 8, 9, 0, 9, 0, 16, 9, 3, 18, 6, 0, 24, 13, 14, 6, 3, 9, 0, 18, 3, 21, 3, 9, 7, 3, 22, 33, 0, 3, 9, 0, 18, 9, 3, 16], [9, 6, 3, 18, 6, 3, 9, 6, 3, 9, 3, 9, 10, 9, 6, 3, 9, 6, 0, 0, 17, 7, 16], [0, 0], [0, 0, 0, 5, 14, 15, 14, 20, 7, 10, 18, 9, 3, 9, 0, 0, 0, 16, 22], [19, 28, 3, 23, 9, 20, 3, 18, 19, 25, 10, 16, 22, 22, 22], [9, 9, 20, 10, 9, 0, 3, 3, 10, 15, 0, 0, 10, 18, 20, 3, 24, 13, 0, 28, 33, 3, 12, 13, 0, 15, 13, 7, 10, 9, 6, 0, 16], [23, 18, 9, 0, 18, 19, 24, 13, 8, 23, 6, 3, 3, 16], [19, 23, 25, 6, 3, 9, 0, 12, 13, 10, 15, 25, 23, 23, 8, 35, 18, 6, 19, 25, 12, 13, 14, 16], [0, 14, 18, 10, 9, 6, 0, 24, 23, 13, 7, 12, 13, 10, 16], [28, 9, 11, 6, 7, 21, 3, 3, 18, 31, 5, 6, 3, 15, 3, 9, 6, 0, 0, 18, 5, 8, 9, 6, 3, 16], [0, 0, 2, 19, 23, 5, 9, 6, 23, 7, 3, 9, 0, 15, 19, 5, 6, 10, 9, 19, 18, 15, 19, 25, 12, 13, 9, 6, 3, 12, 13, 29, 19, 25, 9, 19, 16], [28, 9, 0, 11, 18, 11, 10, 8, 9, 35, 18, 23, 8, 9, 10, 9, 0, 17, 8, 9, 6, 0, 0, 0, 18, 1, 11, 4, 5, 8, 12, 13, 8, 8, 9, 6, 0, 3, 3, 3, 0, 35, 16, 1, 44, 4, 6, 3, 10, 5, 8, 9, 28, 28, 0, 0, 22, 22, 18, 8, 12, 13, 6, 3, 9, 6, 0, 7, 3, 22, 0, 0, 9, 0, 35, 0, 16, 1, 11, 4, 22], [9, 19, 24, 13, 9, 6, 23, 16], [6, 3, 17, 8, 12, 13, 10, 18, 9, 23, 10, 18, 14, 9, 29, 7, 3, 19, 25, 9, 6, 11, 18, 11, 12, 11, 18, 11, 10, 8, 12, 13, 9, 0, 16], [9, 0, 18, 10, 5, 7, 11, 10, 15, 11, 10, 9, 0, 18, 23, 23, 9, 0, 18, 9, 19, 5, 14, 9, 6, 3, 9, 7, 10, 9, 6, 7, 3, 32, 5, 14, 8, 9, 17, 12, 13, 9, 6, 3, 16], [19, 24, 13, 14, 9, 6, 3, 9, 6, 7, 3, 9, 6, 3, 7, 9, 6, 10, 12, 13, 9, 6, 3, 9, 27, 6, 3, 16], [9, 14, 8, 9, 7, 10, 18, 0, 5, 23, 8, 9, 0, 9, 20, 3, 9, 3, 9, 6, 3, 16, 1, 11, 4, 9, 6, 3, 18, 0, 5, 8, 9, 0, 18, 15, 5, 8, 9, 7, 10, 9, 19, 5, 12, 13, 6, 11, 7, 3, 16, 1, 11, 4], [2, 13, 10, 9, 10, 16], [19, 25, 6, 8, 9, 20, 3, 12, 13, 6, 23, 21, 14, 3, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [28, 6, 0, 0, 5, 6, 0, 22, 17, 3, 9, 14, 6, 3, 9, 0, 12, 13, 3, 15, 12, 13, 6, 35, 7, 3, 9, 6, 3, 16], [0, 0, 1, 0, 2, 11, 3, 0, 0, 3, 3, 3, 0, 0, 0, 4, 18, 23, 8, 9, 0, 0, 18, 1, 11, 4, 17, 6, 0, 0, 16], [3, 10, 25, 8, 9, 7, 10, 16], [0, 0, 2, 10, 18, 3, 16], [19, 17, 17, 6, 7, 3, 9, 14, 10, 23, 23, 9, 10, 16, 22, 22, 22], [19, 5, 6, 7, 3, 9, 6, 0, 22, 0, 0, 0, 15, 5, 8, 9, 20, 3, 9, 0, 18, 0, 18, 9, 14, 8, 9, 10, 12, 13, 12, 13, 0, 22, 3, 10, 9, 6, 7, 0, 35, 16], [6, 0, 24, 13, 12, 13, 9, 6, 7, 3, 12, 13, 6, 3, 9, 6, 3, 22, 0, 0, 3, 3, 18, 7, 15, 7, 7, 3, 18, 6, 7, 15, 7, 3, 18, 15, 6, 3, 9, 6, 3, 3, 16], [6, 0, 0, 3, 5, 2], [20, 10, 25, 9, 6, 3, 15, 10, 9, 6, 32, 25, 8, 8, 15, 8, 9, 27, 6, 7, 3, 16], [6, 3, 17, 6, 3, 9, 6, 7, 3, 9, 10, 9, 6, 3, 9, 6, 0, 0, 22, 0, 0, 0, 9, 6, 3, 9, 0, 18, 0, 0, 0, 0, 22, 0, 3, 12, 13, 7, 7, 7, 10, 9, 6, 3, 16], [28, 28, 9, 19, 25, 22, 3, 3, 23, 23, 16], [15, 23, 38, 17, 17, 6, 7, 3, 32, 25, 19, 25, 6, 3, 9, 12, 13, 36, 18, 23, 23, 32, 16], [9, 3, 18, 6, 10, 5, 3, 9, 6, 3, 3, 9, 0, 9, 6, 3, 15, 0, 9, 6, 3, 16], [6, 7, 0, 35, 5, 9, 19, 5, 20, 3, 12, 13, 0, 18, 0, 18, 0, 11, 18, 11, 16], [6, 7, 3, 9, 6, 3, 17, 12, 13, 3, 9, 6, 0, 0, 9, 0, 0, 9, 6, 0, 3, 9, 9, 3, 15, 6, 11, 0, 10, 21, 3, 3, 16], [23, 18, 6, 3, 9, 0, 24, 13, 35, 12, 13, 9, 6, 3, 7, 9, 0, 0, 9, 0, 15, 7, 10, 15, 13, 0, 6, 7, 3, 12, 13, 7, 10, 9, 6, 0, 16], [19, 25, 20, 7, 10, 9, 6, 10, 15, 10, 9, 6, 10, 18, 15, 25, 6, 8, 9, 6, 3, 25, 23, 16], [9, 14, 8, 9, 0, 18, 19, 5, 23, 8, 9, 0, 0, 9, 20, 3, 16], [6, 0, 0, 17, 12, 13, 3, 12, 13, 9, 6, 7, 10, 9, 19, 25, 0, 15, 6, 3, 19, 17, 16], [14, 6, 3, 6, 0, 0, 5, 2], [9, 0, 11, 18, 0, 7, 10, 5, 11, 10, 9, 0, 10, 9, 0, 15, 0, 16], [6, 0, 22, 33, 3, 12, 13, 0, 17, 14, 16], [9, 0, 11, 6, 0, 0, 5, 8, 9, 6, 0, 0, 9, 7, 10, 9, 3, 31, 5, 11, 7, 10, 15, 6, 7, 7, 3, 8, 16], [6, 0, 0, 23, 17, 3, 22, 17, 7, 18, 7, 7, 3, 9, 0, 0, 31, 5, 15, 5, 3, 10, 16], [28, 28, 22, 23, 18, 9, 3, 18, 29, 19, 25, 8, 14, 6, 3, 19, 25, 23, 9, 11, 9, 6, 10, 9, 6, 7, 3, 15, 13, 19, 9, 11, 19, 17, 23, 8, 16], [6, 3, 17, 23, 8, 12, 13, 10, 9, 14, 3, 15, 10, 10, 23, 15, 9, 6, 3, 15, 29, 14, 9, 3, 3, 16], [6, 0, 0, 0, 17, 7, 9, 6, 3, 9, 3, 9, 10, 9, 6, 3, 9, 3, 18, 14, 6, 3, 9, 7, 10, 18, 17, 7, 16], [19, 25, 7, 9, 6, 7, 3, 17, 8, 6, 0, 0, 35, 9, 20, 10, 12, 13, 0, 15, 9, 20, 3, 3, 18, 0, 25, 14, 6, 3, 16], [0, 16, 0, 0, 18, 3, 9, 0, 0, 0, 5, 18, 28, 19, 22, 0, 8, 23, 7, 9, 6, 3, 9, 6, 3, 6, 12, 13, 0, 19, 24, 13, 12, 13, 9, 7, 7, 10, 14, 9, 6, 3, 9, 0, 15, 0, 16], [6, 10, 9, 0, 18, 0, 15, 35, 25, 6, 7, 3, 9, 6, 0, 3, 9, 6, 3, 16], [0], [19, 25, 19, 22, 28, 6, 0, 22, 22, 16, 22], [9, 6, 11, 10, 9, 6, 3, 18, 6, 10, 9, 0, 25, 8, 9, 6, 7, 3, 18, 9, 3, 9, 6, 0, 0, 18, 31, 17, 6, 10, 9, 0, 9, 6, 7, 3, 16], [19, 24, 23, 13, 9, 7, 0, 7, 10, 12, 13, 20, 7, 3, 9, 6, 7, 0, 35, 18, 0, 0, 35, 18, 15, 10, 9, 3, 9, 6, 3, 3, 9, 0, 16], [9, 6, 7, 3, 18, 19, 28, 17, 23, 8, 7, 9, 10, 12, 13, 0, 18, 15, 19, 25, 8, 14, 10, 9, 0, 21, 20, 10, 23, 8, 9, 14, 10, 18, 23, 7, 9, 19, 25, 6, 3, 22, 3, 10, 16], [0, 17, 8, 23, 7, 9, 7, 10, 15, 7, 3, 3, 18, 15, 17, 8, 3, 9, 6, 7, 3, 14, 7, 10, 9, 6, 7, 10, 16], [3, 2, 0, 0, 18, 5, 19, 13, 0, 16], [0, 17, 8, 3, 9, 0, 9, 11, 10, 18, 15, 6, 35, 25, 8, 11, 12, 11, 3, 9, 6, 3, 9, 3, 10, 9, 6, 3, 9, 6, 3, 5, 0, 11, 18, 0, 5, 16], [28, 19, 25, 6, 3, 9, 11, 9, 20, 7, 18, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 18, 32, 5, 8, 3, 9, 7, 0, 16], [9, 6, 3, 12, 13, 6, 7, 3, 9, 0, 18, 7, 3, 10, 25, 8, 3, 9, 14, 10, 15, 9, 3, 10, 23, 8, 18, 19, 5, 16], [6, 10, 5, 20, 10, 14, 9, 6, 3, 33, 33, 3, 3, 18, 23, 9, 20, 10, 15, 10, 16], [19, 25, 6, 7, 10, 31, 25, 8, 8, 9, 0, 0, 0, 15, 20, 3, 9, 7, 10, 9, 3, 9, 6, 18, 14, 3, 22, 33, 3, 16], [28, 19, 25, 7, 9, 0, 0, 24, 13, 14, 9, 3, 9, 0, 11, 15, 14, 23, 9, 6, 3, 19, 24, 13, 9, 7, 0, 18, 0, 15, 7, 3, 3, 10, 9, 6, 0, 18, 22, 5, 0, 16], [28, 38, 17, 11, 3, 32, 17, 6, 3, 6, 3, 18, 23, 18, 9, 19, 17, 7, 3, 16], [2, 3, 15, 3, 3, 3, 16], [16, 9, 0, 0, 18, 11, 10, 5, 11, 10, 16], [19, 5, 8, 9, 11, 9, 6, 7, 3, 0, 16], [0, 9, 6, 0, 0, 18, 0, 0, 18, 17, 9, 6, 3, 9, 6, 0, 3, 9, 0], [9, 6, 3, 19, 5, 12, 13, 19, 12, 13, 16, 22, 22, 22], [19, 17, 3, 23, 5, 36, 6, 3, 3, 9, 7, 10, 18, 10, 18, 15, 18, 9, 3, 18, 10, 16], [6, 7, 3, 17, 23, 15, 8, 15, 5, 9, 19, 17, 36, 10, 9, 0, 16], [9, 7, 15, 7, 18, 6, 5, 6, 7, 3, 18, 9, 23, 26, 11, 10, 8, 15, 7, 3, 9, 7, 10, 8, 16], [28, 19, 25, 30, 8, 6, 7, 10, 9, 3, 9, 0, 18, 31, 9, 19, 22, 0, 5, 23, 17, 6, 0, 22, 33, 3, 39, 15, 0, 22, 19, 3, 30, 7, 18, 22, 0, 5, 16], [23, 9, 0, 18, 0, 7, 10, 5, 11, 10, 8, 9, 15, 9, 3, 9, 6, 3, 9, 0, 14, 3, 18, 3, 18, 3, 18, 15, 23, 8, 3, 23, 23, 9, 3, 3, 9, 0, 10, 16], [19, 25, 8, 7, 18, 14, 0, 24, 13, 3, 15, 3, 15, 19, 17, 6, 3, 19, 24, 13, 16], [22], [19, 25, 8, 9, 10, 9, 6, 3, 9, 10, 9, 10, 18, 6, 7, 3, 9, 10, 15, 6, 3, 9, 10, 23, 7, 9, 11, 9, 10, 18, 10, 15, 3, 10, 16], [19, 17, 8, 12, 13, 8, 10, 10, 8, 9, 0, 33, 33, 0, 3, 7, 10, 9, 0, 33, 33, 7, 3, 5, 9, 11, 16], [19, 17, 22, 3, 25, 6, 30, 7, 9, 6, 16, 22], [0, 0, 0, 0, 17, 7, 3, 3, 9, 7, 3, 9, 14, 6, 3, 9, 6, 3, 9, 7, 3, 10], [14, 23, 11, 10, 9, 6, 3, 9, 0, 12, 0, 10, 18, 0, 0, 5, 2], [19, 17, 7, 9, 6, 10, 9, 6, 3, 25, 20, 3, 12, 13, 19, 17, 8, 9, 18, 15, 9, 6, 10, 25, 39, 10, 12, 13, 7, 3, 17, 27, 6, 9, 3, 9, 0, 16], [0, 0, 0, 5, 2], [6, 3, 9, 0, 17, 7, 9, 7, 10, 9, 3, 15, 6, 7, 3, 9, 7, 3, 14, 8, 9, 6, 10, 9, 6, 7, 3, 16], [28, 23, 23, 18, 6, 7, 3, 17, 12, 13, 7, 10, 9, 3, 10, 2, 2, 11, 9, 6, 7, 11, 10, 9, 3, 9, 6, 0, 10, 18, 2, 23, 23, 9, 3, 9, 6, 3, 16, 22], [19, 5, 7, 12, 13, 6, 3, 9, 10, 12, 13, 9, 6, 7, 3, 19, 25, 16], [9, 7, 7, 10, 15, 3, 10, 25, 8, 8, 18, 0, 24, 13, 7, 12, 13, 9, 6, 0, 7, 3, 3, 16], [39, 9, 11, 11, 10, 9, 0, 23, 25, 9, 3, 9, 6, 7, 3, 3, 16], [20, 8, 3, 17, 23, 8, 20, 3, 23, 18, 22, 22, 19, 17, 16, 22], [19, 24, 13, 6, 7, 3, 9, 6, 10, 8, 12, 13, 6, 3, 9, 6, 3, 9, 20, 7, 10, 9, 3, 18, 14, 7, 18, 7, 7, 10, 18, 3, 3, 18, 3, 3, 3, 15, 3, 9, 7, 10, 16], [0, 1, 0, 2, 3, 3, 3, 0, 0, 0, 0, 0, 11, 11, 3, 18, 0, 0, 4, 17, 6, 3, 9, 7, 0, 18, 8, 11, 10, 1, 11, 16, 41, 0, 4, 3, 9, 0, 16], [6, 3, 23, 23, 17, 6, 3, 9, 7, 10, 31, 0, 17, 8, 9, 10, 9, 10, 9, 7, 10, 9, 0, 15, 0, 15, 17, 6, 3, 9, 7, 3, 12, 13, 6, 9, 6, 3, 14, 0, 16], [7, 3, 18, 6, 7, 3, 5, 8, 12, 13, 6, 23, 21, 8, 10, 9, 3, 9, 7, 10, 14, 9, 6, 0, 0, 16], [6, 10, 9, 6, 0, 0, 25, 20, 3, 9, 0, 22, 0, 7, 3, 15, 20, 3, 9, 6, 0, 10, 9, 20, 3, 9, 3, 18, 9, 7, 9, 0, 16], [28, 28, 28, 19, 5, 9, 19, 5, 7, 12, 13, 6, 7, 3, 9, 9, 19, 24, 13, 20, 3, 16], [28, 19, 17, 6, 7, 3, 9, 6, 10, 20, 10, 15, 10, 9, 7, 3, 9, 6, 3, 12, 13, 19, 7, 16, 22], [0, 0, 16, 0, 23, 18, 6, 3, 3, 3, 9, 0, 0, 18, 0, 0, 18, 5, 28, 19, 5, 7, 12, 13, 10, 9, 6, 7, 10, 19, 25, 9, 6, 10, 16, 22, 6, 3, 5, 6, 14, 3, 9, 6, 11, 10], [19, 25, 6, 3, 9, 6, 0, 9, 0, 17, 8, 6, 7, 3, 23, 9, 0, 16], [8, 9, 6, 3, 9, 7, 10, 9, 11, 18, 19, 17, 11, 10, 9, 0, 18, 0, 18, 6, 0, 0, 0, 15, 0, 16], [19, 24, 13, 12, 13, 10, 12, 13, 6, 7, 3, 9, 0, 21, 0, 9, 0, 16], [19, 25, 23, 7, 9, 14, 23, 9, 0, 9, 0, 15, 6, 3, 7, 0, 3, 23, 23, 9, 23, 12, 13, 7, 10, 9, 3, 15, 3, 16, 22], [28, 0, 0, 23, 5, 6, 3, 9, 0, 18, 29, 38, 25, 8, 10, 9, 6, 7, 10, 18, 15, 5, 6, 3, 9, 6, 7, 3, 9, 0, 0, 16], [0, 0, 0, 0, 0, 5, 2], [0, 17, 7, 9, 23, 11, 16, 11, 11, 7, 10, 9, 3, 14, 6, 0, 0, 18, 0, 9, 0, 18, 0, 0, 0, 18, 0, 9, 0, 15, 6, 0, 0, 16], [9, 0, 18, 0, 0, 0, 24, 13, 10, 9, 6, 0, 22, 33, 25, 12, 13, 7, 10, 9, 6, 0, 21, 11, 0, 0, 16], [7, 10, 5, 12, 13, 3, 10, 9, 3, 16], [17, 14, 6, 7, 0, 3, 25, 6, 10, 3, 16], [6, 3, 28, 3, 3, 3, 17, 11, 18, 11, 18, 11, 3, 1, 11, 18, 11, 3, 41, 3, 4, 15, 7, 9, 6, 17, 8, 9, 3, 16, 1, 11, 4, 1, 11, 4], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [28, 28, 28, 9, 7, 10, 19, 25, 10, 9, 3, 15, 3, 21, 8, 10, 16], [7, 3, 11, 0, 0, 11, 16], [23, 6, 10, 25, 14, 9, 6, 3, 2, 9, 6, 3, 9, 23, 7, 35, 24, 13, 6, 7, 3, 16], [9, 6, 3, 21, 8, 3, 23, 9, 6, 7, 3, 9, 0, 18, 6, 7, 3, 9, 7, 10, 15, 10, 25, 20, 3, 9, 15, 9, 10, 9, 7, 10, 16], [0, 22, 33, 3, 9, 6, 7, 3, 9, 0, 17, 6, 3, 9, 6, 35, 15, 20, 3, 24, 13, 6, 7, 3, 9, 3, 16], [0, 5, 20, 3, 9, 6, 3, 17, 15, 17, 6, 23, 26, 10, 12, 13, 6, 7, 10, 15, 9, 19, 25, 23, 8, 12, 13, 15, 13, 9, 6, 3, 16], [20, 10, 25, 8, 12, 13, 7, 10, 32, 25, 14, 20, 7, 10, 9, 3, 18, 3, 15, 3, 18, 9, 14, 10, 9, 10, 15, 10, 9, 7, 10, 24, 13, 9, 20, 10, 16], [0, 0, 0, 0, 5, 2], [0, 17, 6, 7, 3, 9, 6, 3, 14, 9, 6, 7, 3, 16], [20, 10, 25, 36, 9, 6, 10, 9, 6, 10, 18, 15, 19, 25, 9, 6, 7, 3, 9, 6, 8, 16], [0, 0, 35, 0, 0, 24, 13, 6, 10, 9, 6, 0, 11, 0, 0, 0, 18, 0, 3, 3, 9, 6, 3, 18, 9, 21, 6, 21, 3, 3, 3, 36, 9, 10, 18, 14, 9, 11, 3, 0, 11, 18, 11, 16], [0, 17, 6, 7, 3, 9, 6, 3, 12, 13, 6, 3, 12, 13, 20, 7, 10, 15, 10, 31, 25, 10, 18, 23, 13, 3, 10, 18, 9, 8, 23, 9, 6, 0, 18, 12, 13, 6, 9, 3, 18, 15, 12, 13, 23, 9, 6, 0, 9, 0, 18, 9, 9, 7, 10, 8, 9, 14, 6, 3, 13, 6, 3, 12, 13, 16], [0, 22, 17, 7, 3, 24, 23, 13, 0, 15, 13, 3, 16], [6, 3, 9, 3, 0, 10, 9, 0, 21, 0, 17, 6, 3, 9, 7, 0, 22, 17, 7, 3, 5, 39, 9, 11, 10, 15, 5, 11, 10, 15, 11, 10, 18, 0, 0, 0, 0, 0, 5, 10, 3, 16], [7, 10, 25, 8, 7, 3, 9, 0, 0, 9, 20, 3, 9, 23, 21, 8, 0, 0, 18, 7, 3, 3, 5, 16], [9, 6, 3, 18, 0, 16, 0, 0, 33, 0, 22, 0, 18, 6, 3, 14, 3, 18, 24, 13, 6, 10, 15, 10], [6, 3, 5, 12, 13, 6, 7, 3, 10, 17, 8, 9, 28, 0, 22, 15, 0, 0, 0, 16], [6, 10, 5, 8, 9, 3, 3, 10, 9, 0, 9, 6, 3, 9, 14, 6, 3, 9, 17, 12, 13, 15, 13, 20, 10, 9, 6, 3, 16], [8, 9, 0, 35, 18, 22, 28, 0, 0, 0, 22, 22, 18, 5, 6, 7, 3, 15, 7, 3, 23, 22, 28, 3, 3, 22, 22, 9, 11, 9, 6, 10, 5, 6, 0, 0, 3, 16, 22], [3, 21, 8, 7, 3, 3, 0, 5, 10, 9, 6, 7, 7, 3, 15, 5, 10, 5, 14, 9, 6, 3, 16], [9, 6, 7, 3, 6, 0, 0, 17, 8, 23, 11, 18, 11, 10, 9, 0, 9, 0, 15, 0, 15, 25, 8, 23, 11, 3, 9, 3, 0, 23, 8, 9, 0, 16], [6, 10, 5, 24, 13, 3, 3, 18, 3, 15, 3, 18, 3, 3, 15, 3, 3, 16], [7, 9, 20, 10, 23, 25, 20, 7, 7, 7, 3, 10, 18, 19, 17, 18, 14, 9, 9, 23, 23, 6, 3, 3, 18, 31, 25, 8, 9, 11, 10, 23, 18, 19, 24, 13, 6, 3, 9, 8, 10], [7, 10, 5, 8, 6, 3, 9, 0, 9, 3, 9, 6, 7, 3, 9, 7, 3, 16], [9, 23, 23, 18, 0, 17, 6, 3, 17, 14, 12, 13, 6, 3, 28, 3, 10, 23, 23, 9, 20, 3, 16], [19, 17, 26, 9, 6, 12, 13, 18, 9, 3, 9, 10, 18, 9, 6, 8, 3, 9, 6, 7, 15, 7, 0, 18, 15, 12, 13, 6, 7, 10, 12, 13, 20, 3, 9, 6, 7, 10, 25, 9, 3, 16], [28, 9, 3, 9, 6, 0, 9, 0, 18, 19, 25, 12, 13, 20, 26, 10, 9, 6, 10, 9, 6, 10, 9, 0, 33, 3, 23, 7, 3, 9, 0, 18, 22, 6, 3, 5, 9, 6, 3, 16], [6, 3, 5, 18, 28, 37, 13, 6, 3, 3, 22, 33, 3, 18, 10, 15, 10, 9, 20, 10, 15, 10, 18, 15, 6, 0, 19, 25, 19, 24, 13, 19, 9, 14, 10, 9, 6, 9, 20, 7, 10, 32, 23, 25, 19, 6, 3, 16, 22], [0, 0, 17, 8, 19, 17, 6, 3, 21, 3, 3, 16], [6, 34, 7, 10, 25, 6, 7, 3, 9, 10, 9, 3, 9, 6, 0, 9, 0, 11, 9, 11, 18, 19, 5, 16], [0, 0, 0, 17, 8, 20, 10, 9, 0, 15, 17, 14, 9, 0, 16], [6, 0, 0, 18, 23, 6, 0, 0, 9, 0, 17, 6, 3, 8, 9, 0, 18, 0, 0, 18, 23, 9, 0, 18, 0, 16], [9, 0, 3, 18, 7, 9, 6, 3, 17, 8, 8, 15, 8, 18, 3, 3, 3, 17, 8, 8, 18, 15, 5, 6, 3, 3, 3, 16], [0, 3, 17, 14, 13, 10, 7, 9, 7, 10, 18, 14, 3, 9, 3, 15, 3, 3, 18, 14, 7, 10, 15, 7, 3, 12, 13, 20, 7, 10, 7, 18, 15, 14, 6, 3, 9, 6, 7, 3, 12, 13, 6, 3, 9, 6, 7, 0, 31, 17, 7, 10, 15, 6, 3, 9, 3, 16], [6, 0, 35, 0, 0, 17, 8, 6, 3, 9, 6, 7, 3, 9, 14, 12, 13, 0, 16], [19, 22, 17, 7, 9, 6, 3, 15, 20, 10, 25, 6, 0, 0, 31, 3, 9, 7, 3, 16], [6, 17, 23, 7, 3, 9, 6, 7, 15, 13, 10, 6, 0, 3, 17, 14, 12, 13, 15, 13, 7, 10, 18, 14, 10, 16], [7, 9, 0, 33, 33, 0, 0, 0, 25, 23, 8, 16], [28, 6, 3, 23, 17, 20, 3, 12, 13, 3, 21, 9, 21, 3, 9, 20, 7, 10, 9, 14, 6, 10, 15, 23, 14, 3, 16], [9, 6, 7, 3, 18, 19, 25, 23, 7, 9, 6, 7, 3, 8, 9, 20, 0, 10, 18, 23, 6, 35, 16], [9, 3, 9, 14, 10, 9, 6, 7, 3, 9, 0, 18, 0, 22, 0, 3, 21, 8, 3, 18, 6, 3, 24, 23, 13, 36, 10, 9, 0, 9, 0, 18, 0, 16], [7, 3, 22, 3, 3, 9, 10, 9, 10, 9, 0, 23, 5, 9, 10, 9, 3, 3, 15, 3, 10, 16], [9, 0, 18, 0, 11, 18, 0, 0, 0, 21, 0, 18, 6, 0, 3, 3, 9, 0, 5, 8, 15, 8, 9, 6, 3, 3, 9, 35, 0, 18, 0, 16], [28, 3, 3, 17, 20, 3, 11, 3, 23, 16, 22], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [6, 7, 3, 17, 8, 19, 17, 14, 9, 6, 3, 16], [32, 24, 13, 29, 20, 10, 24, 13, 23, 30, 15, 32, 0, 24, 13, 9, 23, 16], [15, 7, 10, 9, 6, 3, 24, 13, 36, 7, 0, 18, 31, 17, 23, 9, 0, 3, 18, 9, 7, 10, 14, 12, 13, 16], [10, 23, 25, 12, 13, 3, 9, 10, 16], [6, 0, 9, 0, 17, 8, 6, 3, 9, 9, 12, 45, 11, 11, 9, 3, 14, 9, 6, 7, 3, 9, 6, 3, 15, 40, 15, 3, 9, 3, 15, 10, 9, 18, 9, 18, 9, 3, 9, 18, 15, 12, 13, 0, 18, 23, 8, 9, 20, 7, 3, 9, 0, 16], [0, 0, 17, 0, 22, 33, 3, 9, 10, 9, 0, 15, 17, 6, 0, 22, 17, 3, 9, 14, 6, 7, 3, 16], [19, 5, 23, 23, 8, 9, 10, 15, 3, 3, 16], [6, 0, 18, 6, 39, 0, 0, 15, 6, 0, 9, 0, 25, 8, 9, 14, 6, 10, 15, 25, 23, 8, 6, 3, 3, 3, 9, 7, 3, 10, 9, 9, 11, 3, 9, 7, 3, 18, 14, 0, 16], [28, 28, 28, 19, 5, 12, 13, 35, 9, 11, 7, 10, 1, 45, 11, 16, 11, 2, 45, 11, 16, 11, 4, 6, 3, 16], [19, 25, 8, 9, 3, 9, 7, 7, 10, 12, 13, 9, 6, 0, 9, 0, 17, 6, 7, 7, 15, 3, 3, 9, 6, 10, 16], [19, 23, 5, 7, 18, 7, 15, 3, 3, 9, 7, 10, 12, 13, 6, 3, 9, 0, 16], [9, 0, 18, 0, 0, 0, 24, 13, 9, 7, 15, 7, 10, 12, 13, 7, 10, 9, 6, 7, 7, 10, 15, 12, 13, 6, 3, 9, 7, 7, 3, 3, 10, 14, 9, 0, 9, 0, 15, 0, 16], [6, 0, 0, 5, 2], [6, 0, 23, 5, 3, 11, 7, 0, 18, 14, 19, 9, 6, 3, 21, 8, 3, 3, 0, 0, 9, 0, 16], [6, 10, 9, 6, 0, 0, 0, 5, 9, 6, 7, 3, 9, 10, 9, 6, 0, 0, 0, 16, 1, 11, 4], [6, 0, 0, 15, 0, 0, 5, 0, 21, 0, 3, 3, 3, 9, 3, 3, 3, 18, 3, 18, 3, 15, 7, 3, 16], [28, 14, 9, 19, 5, 7, 2, 6, 35, 25, 23, 7, 9, 20, 10, 16, 22], [6, 10, 25, 6, 3, 9, 6, 0, 9, 0, 18, 0, 0, 0, 0, 11, 18, 15, 10, 12, 13, 6, 7, 3, 9, 6, 3, 18, 23, 23, 9, 20, 7, 10, 12, 13, 7, 3, 9, 7, 3, 10, 16], [9, 3, 9, 11, 3, 10, 9, 6, 7, 11, 10, 18, 6, 0, 17, 14, 9, 11, 10, 31, 25, 11, 11, 35, 9, 6, 10, 9, 3, 18, 3, 3, 18, 3, 18, 3, 18, 3, 18, 15, 13, 3, 9, 10, 9, 7, 3, 3, 9, 0, 16], [15, 18, 19, 25, 19, 18, 9, 6, 7, 3, 31, 17, 23, 5, 3, 9, 11, 3, 10, 9, 23, 11, 10, 18, 0, 33, 3, 19, 25, 39, 3, 9, 6, 3, 10, 12, 13, 20, 3, 14, 9, 3, 23, 9, 10, 16], [6, 7, 7, 7, 7, 7, 0, 0, 5, 0, 0, 35, 0, 0, 22, 3, 1, 0, 4, 0, 0, 1, 0, 4, 11, 9, 0, 11, 16], [0, 25, 7, 10, 12, 13, 10, 18, 10, 18, 10, 18, 10, 18, 3, 18, 15, 10, 10, 16], [6, 0, 17, 6, 17, 17, 31, 17, 24, 13, 10, 9, 7, 10, 16], [39, 9, 11, 18, 11, 35, 25, 8, 9, 10, 9, 0, 16], [19, 25, 29, 9, 6, 7, 3, 5, 20, 10, 9, 6, 7, 3, 18, 15, 19, 23, 5, 19, 16], [19, 17, 30, 8, 7, 10, 12, 13, 8, 9, 7, 10, 31, 25, 8, 9, 0, 21, 8, 18, 9, 19, 25, 3, 21, 7, 3, 9, 0, 10, 16], [38, 17, 3, 6, 3, 16, 22, 22, 22], [6, 0, 3, 9, 0, 11, 5, 11, 0, 21, 0, 10, 31, 5, 8, 6, 7, 0, 21, 0, 3, 9, 0, 18, 0, 18, 0, 5, 16], [9, 3, 9, 14, 3, 9, 7, 3, 9, 0, 17, 7, 7, 10, 18, 6, 3, 3, 17, 7, 7, 15, 7, 10, 8, 9, 14, 6, 3, 9, 3, 15, 3, 16], [9, 6, 3, 9, 7, 10, 18, 23, 23, 9, 19, 28, 17, 7, 12, 13, 9, 6, 14, 3, 18, 3, 15, 10, 33, 10, 23, 18, 17, 6, 7, 3, 8, 3, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [6, 10, 25, 9, 0, 17, 8, 9, 20, 3, 9, 0, 22, 33, 7, 10, 16], [16, 9, 0, 0, 18, 11, 10, 5, 6, 0, 21, 8, 3, 15, 11, 7, 3, 10, 16], [6, 5, 11, 9, 6, 34, 7, 10, 9, 6, 3, 23, 23, 16], [9, 6, 3, 8, 9, 15, 0, 15, 23, 9, 0, 18, 6, 0, 0, 0, 0, 17, 12, 13, 6, 3, 9, 7, 10, 9, 0, 9, 0, 10, 18, 14, 6, 7, 3, 9, 3, 9, 6, 0, 0, 9, 11, 16], [15, 9, 8, 23, 18, 6, 3, 24, 13, 6, 3, 9, 7, 15, 13, 7, 3, 9, 6, 9, 3, 16, 15, 18, 26, 9, 6, 18, 9, 7, 19, 24, 13, 6, 3, 9, 6, 3, 9, 7, 7, 7, 10, 9, 0, 10, 18, 9, 6, 3, 8, 9, 6, 0, 0, 0, 16, 9, 6, 3, 18, 19, 24, 13, 6, 7, 3, 8, 9, 7, 7, 10, 18, 7, 9, 6, 8, 3, 9, 0, 18, 9, 23, 7, 15, 8, 9, 14, 6, 7, 15, 7, 3, 9, 7, 7, 7, 10, 9, 0, 9, 0, 10, 16, 9, 6, 3, 18, 19, 25, 8, 3, 9, 7, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 3, 15, 9, 6, 0, 0, 0, 0, 0, 0, 18, 12, 13, 20, 3, 9, 0, 16, 6, 0, 0, 17, 14, 20, 3, 9, 6, 7, 10, 16], [7, 7, 10, 25, 19, 23, 25, 6, 7, 3, 9, 6, 3, 33, 3, 0, 3, 16], [9, 3, 9, 20, 7, 3, 12, 13, 6, 7, 3, 18, 6, 3, 24, 13, 7, 15, 7, 10, 18, 7, 3, 10, 18, 3, 7, 10, 18, 15, 10, 12, 13, 6, 10, 9, 6, 7, 10, 16], [9, 6, 7, 3, 39, 9, 11, 18, 11, 10, 2, 7, 9, 19, 10, 18, 25, 8, 8, 16], [0, 0, 18, 6, 0, 0, 9, 0, 0, 9, 0, 10, 9, 6, 0, 0, 15, 0, 15, 0, 0, 18, 24, 13, 9, 0, 0, 11, 40, 11, 15, 9, 0, 0, 11, 40, 11, 16], [19, 17, 7, 9, 6, 10, 12, 13, 23, 9, 6, 14, 10, 15, 10, 12, 13, 3, 9, 3, 9, 14, 0, 15, 12, 13, 9, 6, 14, 3, 9, 6, 7, 10, 16], [9, 6, 3, 9, 6, 3, 12, 13, 0, 18, 6, 0, 24, 23, 13, 9, 10, 15, 10, 9, 19, 25, 9, 6, 3, 22, 17, 7, 3, 15, 3, 9, 0, 17, 8, 16], [23, 19, 25, 6, 7, 3, 16, 22, 22, 22], [28, 6, 0, 0, 5, 6, 7, 0, 0, 18, 0, 0, 21, 0, 18, 12, 13, 6, 3, 9, 10, 14, 6, 7, 3, 15, 6, 3, 14, 8, 9, 0, 9, 0, 16], [23, 11, 18, 11, 7, 3, 3, 10, 18, 7, 10, 18, 0, 7, 10, 15, 0, 10, 18, 8, 9, 0, 21, 8, 3, 3, 10, 18, 5, 6, 23, 21, 8, 3, 12, 13, 0, 11, 10, 23, 16], [0], [19, 23, 23, 25, 6, 3, 9, 11, 0, 9, 6, 0, 0, 9, 0, 18, 8, 9, 0, 0, 0, 0, 0, 18, 15, 6, 0, 0, 0, 18, 8, 9, 0, 0, 0, 0, 18, 9, 7, 10, 9, 3, 10, 15, 3, 3, 16], [19, 25, 3, 22, 33, 3, 9, 7, 3, 10, 5, 6, 7, 3, 9, 0, 9, 0, 3, 18, 8, 9, 7, 3, 9, 20, 3, 0, 15, 20, 7, 3, 16], [23, 9, 0, 18, 0, 7, 10, 5, 11, 10, 8, 9, 15, 9, 3, 9, 6, 3, 9, 0, 14, 3, 18, 3, 18, 3, 18, 15, 23, 8, 3, 23, 23, 9, 3, 3, 9, 0, 10, 16], [6, 3, 9, 7, 10, 17, 7, 18, 15, 6, 3, 9, 7, 3, 15, 0, 0, 0, 10, 16], [6, 3, 3, 9, 6, 39, 0, 10, 18, 1, 11, 4, 6, 0, 10, 25, 9, 6, 0, 0, 14, 0, 18, 23, 9, 6, 0, 3, 18, 14, 7, 0, 3, 0, 0, 15, 7, 0, 0, 16], [23, 3, 18, 0, 5, 9, 3, 22, 3, 7, 10, 9, 0, 18, 0, 16], [9, 0, 18, 7, 3, 10, 5, 6, 7, 3, 3, 5, 8, 6, 3, 9, 0, 18, 14, 10, 15, 6, 10, 16], [14, 0, 3, 9, 6, 7, 3, 17, 6, 3, 9, 0, 21, 0, 3, 9, 0, 9, 0, 11, 16], [7, 7, 3, 9, 0, 15, 0, 9, 0, 3], [0, 0, 18, 14, 3, 9, 6, 0, 0, 18, 5, 9, 0, 12, 13, 10, 15, 7, 10, 16], [23, 8, 9, 6, 7, 3, 9, 7, 7, 7, 10, 18, 0, 21, 0, 18, 5, 19, 24, 13, 36, 9, 0, 9, 6, 28, 28, 3, 3, 22, 22, 16], [6, 24, 13, 6, 26, 3, 31, 0, 22, 0, 0, 35, 25, 23, 8, 2, 19, 25, 9, 9, 6, 3, 16], [19, 5, 9, 6, 3, 16], [0, 16], [6, 0, 0, 5, 11, 10, 9, 3, 21, 14, 7, 3, 16], [19, 23, 17, 6, 3, 9, 0, 10, 12, 13, 7, 3, 10, 9, 20, 3, 12, 13, 6, 7, 3, 16], [9, 0, 11, 0, 6, 0, 0, 0, 5, 0, 11, 16], [19, 24, 13, 12, 13, 20, 10, 7, 9, 6, 3, 9, 0, 16], [9, 6, 7, 11, 10, 7, 3, 10, 15, 20, 0, 10, 25, 8, 9, 27, 6, 3, 9, 6, 3, 15, 23, 6, 7, 7, 10, 16], [19, 17, 6, 3, 3, 3, 15, 6, 7, 3, 16], [6, 3, 9, 6, 0, 0, 17, 6, 34, 7, 7, 3, 9, 6, 0, 9, 0, 3, 2, 6, 3, 9, 11, 9, 6, 10, 9, 0, 15, 6, 0, 0, 15, 0, 12, 13, 7, 10, 12, 13, 18, 3, 18, 15, 13, 6, 7, 3, 8, 23, 9, 0, 16], [0, 0, 18, 0, 9, 6, 0, 0, 17, 6, 3, 9, 0, 15, 17, 25, 9, 6, 0, 9, 0, 9, 6, 3, 9, 0], [9, 6, 3, 18, 19, 25, 6, 7, 10, 9, 0, 24, 13, 23, 9, 6, 3, 9, 15, 9, 3, 9, 6, 0, 9, 0, 18, 9, 6, 3, 9, 6, 0, 16], [9, 6, 17, 6, 7, 3, 18, 19, 17, 6, 7, 15, 7, 7, 3, 31, 17, 9, 6, 10, 25, 7, 15, 7, 12, 13, 23, 15, 9, 7, 3, 16], [6, 3, 9, 0, 0, 18, 8, 36, 9, 6, 3, 9, 10, 3, 15, 3, 3, 10, 17, 11, 9, 6, 26, 10, 12, 13, 9, 0, 6, 3, 12, 13, 7, 10, 9, 6, 0, 0, 9, 0, 15, 6, 0, 16], [19, 17, 19, 5, 19, 12, 13, 10, 30, 7, 9, 19, 18, 10, 32, 19, 24, 13, 10, 9, 15, 13, 10, 9, 16], [7, 3, 18, 19, 23, 5, 9, 19, 24, 13, 20, 7, 3, 9, 0, 9, 45, 11, 11, 18, 14, 20, 7, 3, 9, 45, 11, 16, 11, 11, 18, 14, 10, 9, 0, 32, 25, 8, 8, 12, 13, 9, 0, 22, 33, 3, 16], [6, 10, 9, 6, 0, 0, 25, 6, 0, 0, 21, 0, 18, 0, 0, 21, 0, 18, 12, 13, 23, 12, 13, 27, 6, 0, 23, 23, 9, 7, 15, 9, 6, 7, 3, 21, 3, 16], [6, 3, 9, 0, 0, 17, 6, 7, 3, 6, 7, 3, 17, 8, 9, 6, 3, 3, 9, 14, 8, 9, 6, 0, 0, 16], [28, 28, 28, 0, 22, 33, 3, 9, 3, 17, 23, 7, 16], [6, 0, 9, 6, 0, 0, 5, 8, 9, 11, 9, 20, 7, 10, 18, 9, 9, 0, 21, 0, 9, 0, 18, 0, 35, 35, 18, 0, 0, 21, 0, 0, 18, 0, 21, 0, 35, 18, 15, 0, 21, 0, 35, 16], [6, 0, 17, 20, 3, 12, 13, 6, 3, 15, 10, 9, 0, 9, 6, 3, 18, 15, 17, 7, 9, 20, 3, 9, 6, 0, 0, 10, 12, 13, 15, 13, 0, 22, 3, 16], [28, 3, 11, 17, 6, 3, 9, 10, 18, 15, 17, 6, 3, 14, 6, 28, 28, 3, 22, 22, 9, 7, 10, 23, 7, 9, 6, 9, 6, 35, 16, 22], [0, 0, 5, 2], [9, 6, 7, 3, 5, 18, 6, 0, 0, 17, 8, 9, 9, 45, 11, 11, 9, 7, 3, 16], [9, 23, 0, 15, 7, 10, 18, 7, 9, 19, 23, 7, 10, 46, 10, 5, 8, 9, 6, 3, 9, 6, 3, 18, 25, 8, 39, 9, 11, 18, 11, 10, 9, 23, 23, 6, 7, 7, 16], [19, 5, 39, 10, 15, 19, 5, 12, 13, 39, 3, 16], [6, 0, 24, 13, 8, 12, 13, 14, 3, 9, 9, 11, 3, 0, 15, 7, 10, 9, 7, 10, 10, 15, 7, 3, 10, 24, 13, 8, 12, 13, 16], [9, 3, 18, 23, 11, 7, 15, 7, 10, 25, 8, 23, 12, 13, 0, 22, 33, 3, 9, 0, 15, 0, 28, 0, 17, 12, 13, 6, 3, 9, 6, 7, 3, 18, 15, 6, 35, 15, 35, 9, 6, 7, 0, 18, 23, 9, 20, 0, 15, 0, 10, 18, 25, 14, 23, 12, 13, 15, 13, 0, 22, 3, 18, 22, 10, 5, 16], [16, 9, 0, 0, 18, 11, 3, 5, 6, 0, 3, 3, 16], [29, 6, 35, 5, 19, 8, 10, 16], [3], [13, 9, 6, 3, 18, 19, 25, 7, 9, 14, 6, 7, 3, 9, 0, 31, 17, 6, 10, 9, 10, 9, 6, 10, 9, 6, 0, 15, 0, 16], [19, 25, 6, 0, 9, 0, 22, 0, 3, 18, 31, 5, 9, 6, 3, 3, 5, 8, 9, 6, 3, 18, 14, 19, 7, 9, 6, 3, 5, 8, 36, 9, 10, 9, 15, 6, 0, 3, 15, 0, 16], [10, 5, 6, 0, 3, 15, 3, 9, 0], [6, 17, 29, 6, 0, 17, 14, 9, 3, 9, 6, 0, 0, 12, 13, 15, 13, 0, 9, 0, 15, 0, 15, 18, 29, 7, 18, 14, 9, 6, 32, 25, 6, 3, 9, 6, 0, 18, 15, 20, 10, 15, 10, 16], [6, 0, 17, 8, 6, 10, 9, 3, 9, 16], [28, 28, 28, 9, 6, 3, 6, 3, 5, 19, 23, 20, 3, 16], [15, 6, 7, 3, 19, 5, 9, 6, 3, 10, 23, 23, 17, 19, 17, 12, 13, 16], [6, 7, 10, 9, 0, 25, 8, 6, 3, 9, 6, 7, 3, 9, 20, 3, 12, 13, 0, 18, 46, 10, 9, 3, 9, 0, 15, 6, 0, 0, 25, 12, 13, 20, 7, 10, 15, 3, 9, 3, 16], [19, 25, 19, 9, 6, 3, 12, 13, 13, 19, 14, 8, 9, 10, 15, 10, 9, 23, 7, 7, 10, 23, 18, 22, 22, 17, 0, 0, 18, 6, 7, 7, 3, 3, 32, 5, 11, 9, 6, 10, 16, 22], [6, 0, 9, 0, 17, 11, 10, 9, 3, 9, 14, 3, 16], [12, 13, 7, 19, 5, 23, 7, 9, 0, 9, 19, 5, 14, 19, 9, 3, 16], [6, 3, 9, 0, 10, 9, 0, 15, 0, 23, 17, 6, 3, 28, 33, 3, 12, 13, 3, 15, 13, 10, 16], [19, 24, 13, 12, 13, 27, 6, 8, 9, 6, 7, 10, 9, 3, 9, 6, 11, 3, 3, 8, 12, 13, 0, 22, 33, 7, 3, 16], [0, 0, 22, 17, 5, 3, 3, 18, 15, 0, 0, 0, 0, 0, 18, 0, 0, 3, 24, 13], [3, 17, 8, 12, 13, 8, 9, 11, 16], [0, 17, 6, 3, 9, 6, 35, 15, 19, 17, 7, 9, 0, 22, 33, 10, 25, 9, 6, 3, 9, 6, 7, 3, 16], [19, 25, 8, 9, 6, 26, 7, 3, 9, 0, 22, 3, 3, 18, 23, 9, 6, 3, 9, 3, 3, 5, 14, 7, 3, 5, 0, 3, 10, 9, 9, 10, 16], [23, 23, 18, 19, 5, 12, 13, 6, 0, 21, 8, 3, 16], [6, 3, 9, 6, 0, 0, 0, 9, 0, 35, 18, 0, 0, 18, 5, 6, 3, 3, 9, 0, 9, 0, 9, 19, 5, 14, 12, 13, 10, 9, 10, 9, 10, 15, 10, 18, 23, 23, 9, 7, 10, 16], [19, 23, 5, 12, 13, 23, 9, 7, 10, 15, 9, 6, 0, 9, 6, 7, 3, 12, 13, 0, 9, 3, 16, 22], [28, 0, 17, 8, 9, 14, 23, 9, 6, 0, 15, 0, 0, 18, 22, 5, 0, 16, 0, 0, 18, 3, 9, 0], [28, 28, 28, 19, 22, 24, 13, 14, 9, 19, 18, 14, 6, 3, 14, 8, 15, 19, 22, 3, 3, 18, 28, 29, 25, 19, 14, 6, 3, 16, 22, 23, 19, 25, 9, 3, 17, 8, 9, 9, 20, 10, 15, 19, 25, 19, 18, 28, 29, 5, 19, 13, 18, 32, 5, 19, 13, 16, 22, 19, 25, 22, 3, 25, 16, 22, 22, 22], [23, 9, 11, 0, 6, 3, 10, 5, 12, 13, 6, 3, 9, 0, 15, 13, 6, 10, 32, 5, 0, 16], [28, 0, 0, 21, 0, 22, 0, 0, 21, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 3, 11, 0, 6, 0, 21, 0, 0, 22, 0, 0, 21, 0, 0, 3, 3, 18, 22, 28, 0, 9, 6, 0, 0, 22, 22, 4, 17, 6, 7, 7, 3, 8, 12, 13, 3, 9, 6, 7, 3, 16], [28, 6, 10, 5, 6, 3, 9, 0, 15, 6, 3, 9, 3, 9, 7, 10, 18, 14, 6, 7, 7, 3, 9, 6, 3, 3, 16], [9, 6, 3, 9, 0, 22, 33, 0, 0, 0, 18, 0, 17, 8, 6, 3, 14, 6, 3, 9, 6, 3, 16], [19, 25, 8, 6, 3, 9, 7, 10, 9, 7, 0, 14, 0, 15, 0, 18, 5, 6, 3, 9, 6, 0, 9, 0, 0, 18, 15, 5, 0, 9, 9, 0, 16], [23, 11, 18, 11, 7, 10, 18, 14, 10, 9, 7, 10, 15, 20, 3, 18, 3, 15, 3, 10, 18, 24, 13, 6, 3, 16], [6, 17, 23, 6, 7, 3, 12, 13, 6, 7, 3, 9, 0, 9, 0, 16], [28, 28, 28, 38, 22, 33, 7, 3, 23, 9, 6, 32, 25, 18, 22, 28, 6, 3, 17, 16], [6, 0, 10, 9, 6, 0, 9, 0, 0, 15, 17, 6, 7, 10, 9, 6, 3, 15, 3, 9, 10, 12, 13, 6, 3, 9, 3, 9, 10, 10, 10, 16], [9, 7, 7, 18, 6, 10, 24, 13, 13, 6, 3, 9, 11, 18, 11, 10, 5, 9, 6, 3, 16], [14, 6, 3, 18, 6, 0, 0, 0, 0, 5, 2], [28, 28, 28, 23, 19, 25, 6, 3, 17, 6, 7, 3, 9, 27, 20, 10, 16], [9, 3, 9, 6, 11, 10, 18, 11, 3, 10, 18, 11, 0, 21, 8, 10, 18, 11, 7, 10, 18, 15, 6, 3, 3, 3, 3, 5, 23, 8, 9, 6, 3, 16], [23, 20, 3, 17, 6, 3, 12, 13, 0, 6, 7, 3, 7, 3, 9, 0, 16], [15, 19, 17, 23, 28, 3, 23, 7, 3, 9, 10, 31, 25, 30, 7, 9, 6, 0, 16], [3, 2, 11, 0, 0, 2, 0, 0, 3, 2, 11, 16, 11, 10, 40, 3, 0, 2, 11, 40, 11, 0, 11, 40, 11, 0, 0, 0, 2, 11, 0, 2, 11, 16, 11, 10, 3, 2, 0, 0, 0, 2, 11, 16, 11, 0, 0, 0, 2, 44, 40, 11], [28, 28, 28, 19, 25, 6, 3, 18, 22, 22, 17, 0, 16], [9, 6, 3, 9, 6, 3, 9, 0, 24, 13, 6, 3, 9, 6, 7, 3, 19, 24, 23, 13, 6, 10, 9, 6, 3, 16], [9, 0, 0, 9, 0, 0, 0, 5, 3, 9, 6, 0, 0, 18, 6, 0, 0, 24, 13, 23, 45, 11, 11, 9, 7, 3, 9, 6, 7, 3, 18, 14, 6, 7, 3, 19, 25, 8, 15, 25, 14, 9, 6, 7, 3, 9, 11, 9, 23, 45, 11, 11, 16], [19, 28, 9, 6, 11, 10, 9, 0, 19, 25, 23, 19, 25, 16], [6, 3, 9, 0, 0, 0, 0, 5, 9, 3, 9, 0, 23, 23, 9, 7, 3, 15, 3, 16], [6, 3, 9, 20, 34, 7, 10, 25, 9, 23, 21, 8, 7, 10, 7, 9, 6, 3, 15, 3, 0, 0, 18, 8, 9, 6, 0, 9, 35, 18, 15, 7, 3, 0, 21, 0, 18, 32, 5, 10, 9, 6, 7, 3, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [9, 20, 3, 9, 0, 18, 6, 0, 17, 23, 8, 10, 9, 10, 15, 39, 9, 11, 7, 10, 9, 0, 21, 8, 3, 16], [0, 0, 0, 11, 24, 13, 9, 6, 3, 21, 7, 3, 9, 7, 10, 12, 13, 6, 7, 7, 7, 3, 9, 7, 15, 7, 10, 16], [6, 0, 17, 6, 0, 9, 3, 18, 13, 15, 13, 10, 18, 10, 15, 3, 3, 9, 6, 0, 16], [6, 3, 17, 15, 6, 3, 9, 3, 3, 8, 15, 6, 9, 0, 22, 7, 7, 10, 16], [29, 6, 3, 17, 8, 8, 9, 10, 15, 3, 10, 6, 10, 18, 19, 24, 13, 8, 10, 24, 13, 7, 3, 9, 10, 16], [19, 24, 13, 9, 23, 9, 6, 3, 17, 3, 18, 6, 3, 17, 10, 16, 22, 22, 22], [6, 3, 9, 6, 3, 17, 7, 9, 0, 15, 7, 10, 25, 19, 24, 13, 8, 9, 0, 33, 3, 10, 9, 19, 5, 29, 19, 5, 16], [23, 11, 35, 25, 9, 0, 21, 0, 0, 16], [9, 19, 5, 9, 0, 0, 0, 21, 0, 3, 18, 19, 25, 0, 0, 22, 33, 3, 9, 14, 6, 7, 15, 7, 3, 9, 11, 10, 18, 32, 5, 3, 9, 6, 3, 9, 20, 14, 3, 12, 13, 19, 9, 6, 7, 3, 16], [0, 5, 23, 8, 9, 6, 3, 9, 0, 0, 9, 7, 0, 18, 14, 9, 6, 7, 0, 0, 0, 21, 0, 0, 0, 3, 3, 16], [19, 17, 8, 9, 6, 7, 3, 9, 6, 0, 9, 0, 17, 8, 9, 6, 0, 3, 9, 0, 33, 0, 15, 6, 7, 3, 9, 6, 0, 0, 0, 17, 8, 9, 6, 3, 9, 0, 33, 11, 9, 21, 3, 16, 1, 11, 4], [12, 13, 2, 0, 10, 25, 14, 15, 14, 7, 0, 35, 1, 0, 4, 18, 23, 7, 7, 10, 14, 9, 6, 7, 3, 9, 6, 3, 32, 25, 14, 10, 9, 6, 3, 12, 13, 0, 16], [28, 28, 28, 19, 23, 23, 25, 9, 6, 24, 23, 13, 6, 7, 7, 10, 19, 25, 9, 7, 7, 10, 9, 1, 0, 4, 18, 15, 13, 20, 3, 9, 3, 10, 15, 20, 10, 12, 13, 20, 10, 25, 23, 13, 3, 9, 6, 9, 6, 10, 32, 25, 9, 10, 9, 0, 3, 18, 15, 13, 6, 8, 17, 10, 19, 25, 9, 3, 9, 7, 7, 3, 18, 22, 22, 5, 0, 0, 16, 22], [19, 24, 23, 13, 0, 21, 0, 12, 13, 20, 3, 12, 13, 6, 0, 0, 15, 20, 10, 9, 6, 3, 18, 15, 6, 10, 25, 6, 3, 16, 22], [3, 17, 9, 21, 6, 21, 3, 9, 0, 0, 0, 16], [6, 0, 0, 23, 17, 0, 28, 7, 7, 3, 10, 9, 6, 3, 9, 0, 0, 9, 0, 0, 18, 31, 5, 15, 5, 10, 9, 7, 10, 18, 14, 9, 6, 3, 15, 3, 16], [28, 28, 28, 19, 25, 22, 3, 25, 3, 15, 19, 5, 6, 20, 10, 9, 3, 3, 9, 20, 3, 18, 22, 28, 19, 17, 36, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [6, 0, 0, 15, 0, 0, 5, 9, 11, 0, 9, 6, 11, 0, 21, 0, 0, 9, 35, 9, 0, 16], [20, 3, 17, 19, 9, 14, 6, 7, 18, 7, 21, 7, 3, 16], [0, 15, 0, 5, 23, 6, 3, 9, 6, 7, 3, 16], [19, 17, 3, 23, 6, 3, 9, 0, 16], [6, 0, 0, 17, 8, 9, 6, 14, 9, 6, 7, 3, 9, 10, 9, 0, 23, 9, 6, 7, 3, 5, 36, 9, 0, 16], [7, 7, 10, 32, 25, 8, 8, 9, 10, 9, 0, 25, 14, 12, 13, 6, 0, 22, 19, 3, 9, 0, 18, 0, 5, 16], [16, 9, 0, 0, 18, 11, 10, 5, 11, 0, 7, 10, 15, 5, 6, 7, 21, 3, 3, 18, 6, 0, 18, 6, 3, 3, 18, 15, 6, 3, 3, 16], [29, 8, 18, 25, 0, 11, 16], [6, 0, 17, 9, 6, 3, 9, 7, 10, 12, 13, 7, 3, 14, 8, 9, 45, 11, 16, 3, 12, 7, 15, 7, 10, 9, 0, 15, 9, 10, 9, 3, 10, 16], [27, 6, 3, 24, 13, 6, 10, 12, 13, 9, 6, 8, 3, 8, 9, 0, 22, 3, 15, 7, 10, 16], [23, 9, 0, 18, 6, 7, 7, 3, 5, 6, 0, 21, 8, 3, 9, 14, 39, 9, 11, 10, 15, 14, 11, 10, 9, 3, 10, 9, 15, 9, 0, 9, 6, 7, 11, 10, 16], [3, 18, 7, 10, 18, 23, 8, 9, 0, 22, 0, 18, 25, 8, 9, 26, 11, 30, 7, 10, 15, 7, 10, 9, 10, 16], [6, 0, 0, 3, 5, 2, 28, 6, 0, 0, 5, 7, 0, 0, 0, 0, 30, 3, 9, 6, 10, 9, 6, 0, 3, 9, 0, 0, 18, 20, 7, 3, 21, 9, 21, 3, 3, 9, 0, 0, 5, 8, 16], [19, 25, 10, 16], [9, 0, 18, 0, 22, 0, 5, 7, 10, 9, 6, 7, 3, 9, 6, 0, 21, 0, 3, 9, 0, 31, 5, 23, 34, 11, 10, 16], [9, 11, 10, 9, 7, 3, 18, 6, 10, 9, 0, 24, 13, 12, 13, 3, 9, 6, 39, 3, 16], [28, 28, 19, 25, 6, 3, 3, 15, 19, 25, 22, 3, 3, 15, 6, 3, 19, 25, 22, 3, 3, 18, 19, 25, 22, 3, 3, 16, 22, 22, 22], [28, 20, 3, 24, 13, 20, 23, 7, 10, 15, 13, 6, 3, 9, 3, 8, 9, 3, 16, 22], [10, 2, 11, 3, 11, 40, 11, 3, 11, 11, 10, 11, 40, 11, 3, 10, 40, 11, 16, 11, 18, 0, 11, 16, 11, 0, 0, 40, 11, 16, 11, 2, 11, 16, 11], [0, 22, 33, 7, 3, 17, 8, 36, 23, 16], [9, 6, 7, 18, 19, 5, 6, 3, 9, 6, 0, 3, 9, 0, 18, 15, 5, 8, 3, 9, 0, 35, 16], [0, 0, 5, 6, 7, 0, 0, 18, 0, 0, 21, 0, 9, 0, 3, 16], [19, 5, 6, 3, 9, 6, 3, 9, 14, 9, 6, 3, 16], [14, 3, 18, 0, 0, 5, 2], [6, 10, 9, 6, 0, 0, 25, 6, 7, 10, 15, 20, 10, 12, 13, 7, 15, 13, 6, 7, 3, 8, 9, 6, 0, 16], [6, 0, 25, 8, 23, 11, 7, 10, 9, 6, 3, 9, 0, 5, 23, 7, 3, 18, 0, 5, 16], [28, 28, 6, 10, 0, 33, 3, 23, 15, 6, 0, 0, 0, 28, 3, 14, 23, 16], [6, 3, 46, 3, 5, 8, 5, 6, 3, 3, 31, 19, 5, 8, 6, 3, 14, 9, 20, 3, 18, 11, 11, 1, 11, 3, 4, 9, 6, 3, 18, 23, 9, 6, 3], [6, 3, 18, 9, 8, 9, 6, 0, 10, 18, 17, 11, 15, 39, 7, 10, 31, 25, 9, 23, 6, 7, 7, 3, 12, 13, 6, 7, 18, 23, 7, 3, 9, 6, 3, 16], [20, 3, 17, 23, 7, 9, 23, 2, 19, 24, 13, 12, 13, 9, 6, 7, 3, 9, 6, 3, 9, 31, 0, 17, 6, 3, 16], [9, 14, 7, 3, 9, 7, 7, 7, 10, 18, 6, 3, 17, 14, 6, 3, 9, 3, 15, 3, 23, 6, 7, 7, 3, 17, 23, 9, 3, 16], [5, 38, 6, 10, 16], [19, 17, 7, 9, 6, 8, 3, 18, 31, 5, 10, 18, 17, 8, 6, 10, 16], [6, 3, 17, 11, 10, 18, 11, 9, 31, 25, 9, 7, 3, 18, 15, 17, 6, 7, 3, 3, 9, 7, 10, 16, 1, 11, 4], [0, 0, 3, 9, 0, 3, 10, 3], [28, 19, 22, 13, 7, 9, 9, 6, 3, 3, 12, 13, 6, 3, 9, 10, 2, 23, 23, 10, 2, 19, 22, 9, 14, 6, 3, 9, 6, 35, 15, 9, 6, 0, 12, 13, 7, 19, 22, 13, 14, 6, 3, 9, 6, 3, 16, 22], [6, 3, 17, 23, 6, 3, 9, 0, 22, 33, 3, 15, 3, 9, 7, 3, 15, 20, 3, 12, 13, 7, 3, 9, 6, 10, 9, 0, 16], [6, 14, 10, 9, 6, 3, 25, 9, 7, 3, 9, 3, 12, 13, 7, 3, 15, 12, 13, 7, 21, 3, 10, 31, 25, 3, 9, 6, 3, 15, 31, 25, 23, 23, 7, 16], [19, 25, 12, 13, 23, 7, 9, 10, 23, 25, 19, 9, 20, 10, 18, 14, 6, 7, 3, 19, 22, 24, 13, 16, 22, 22, 22], [6, 5, 3, 9, 0, 22, 0, 24, 13, 8, 9, 35, 14, 6, 8, 7, 3, 23, 9, 6, 0, 3, 46, 10, 25, 8, 0, 22, 13, 25, 3, 16], [3, 9, 6, 0, 0, 9, 0, 18, 6, 0, 0, 0, 3, 22, 35, 18, 0, 0, 18, 0, 0, 15, 0, 0, 0, 0, 18, 0, 0, 0, 9, 6, 0, 9, 35, 9, 6, 0, 0, 9, 0, 18, 0, 0, 9, 35, 15, 0, 1, 0, 4, 5, 6, 3, 9, 6, 3, 9, 6, 3, 15, 3, 9, 6, 0, 0, 9, 6, 10, 9, 6, 7, 3, 9, 0, 9, 0], [6, 3, 17, 8, 12, 13, 11, 10, 16], [0, 10, 18, 0, 5, 19, 16], [0, 5, 8, 9, 6, 11, 3, 9, 0, 0, 35, 9, 7, 3, 16, 1, 11, 4], [9, 6, 3, 18, 11, 0, 3, 5, 11, 10, 14, 11, 10, 16], [0, 0, 5, 6, 7, 3, 23, 16], [0, 0, 5, 2, 6, 0, 17, 8, 23, 9, 7, 3, 10, 12, 13, 11, 7, 0, 10, 10, 9, 0, 16], [6, 0, 0, 0, 5, 23, 12, 13, 0, 11, 18, 31, 5, 6, 11, 0, 10, 9, 0, 15, 10, 9, 6, 0, 0, 18, 14, 6, 9, 0, 9, 11, 0, 9, 31, 11, 10, 5, 8, 16], [19, 24, 13, 14, 3, 9, 6, 3, 9, 19, 17, 20, 3, 18, 17, 6, 3, 15, 10, 9, 7, 10, 9, 6, 7, 3, 16], [3, 11, 17, 6, 3, 9, 0, 16], [6, 3, 5, 9, 3, 23, 16], [0, 0, 9, 0, 0, 0, 24, 13, 9, 0, 12, 13, 7, 7, 18, 7, 18, 15, 3, 10, 9, 3, 10, 16], [9, 14, 6, 3, 18, 6, 0, 5, 36, 7, 10, 15, 11, 10, 6, 7, 3, 9, 14, 6, 7, 3, 18, 9, 7, 3, 3, 18, 12, 13, 20, 3, 16], [6, 10, 5, 9, 3, 10, 9, 7, 3, 9, 7, 3, 9, 0, 16], [0, 0, 10, 25, 9, 0, 10, 9, 7, 3, 3, 17, 23], [19, 23, 5, 19, 3, 18, 22, 22, 17, 6, 10, 33, 3, 18, 0, 0, 16, 22], [0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 4, 15, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 3, 4, 17, 6, 7, 7, 7, 3, 9, 6, 0, 0, 7, 1, 0, 4, 9, 0, 15, 23, 9, 0, 18, 0, 16], [14, 9, 6, 3, 9, 3, 10, 9, 0, 18, 6, 0, 0, 0, 0, 0, 5, 6, 3, 5, 14, 9, 8, 16], [19, 23, 5, 9, 38, 5, 6, 3, 9, 0, 0, 10, 9, 0, 15, 9, 6, 3, 5, 9, 14, 3, 9, 6, 26, 23, 21, 8, 3, 18, 23, 11, 10, 23], [16, 9, 0, 0, 18, 11, 3, 5, 6, 0, 7, 3, 15, 5, 6, 3, 3, 16], [19, 5, 29, 12, 13, 19, 2, 9, 0, 33, 33, 7, 0, 3, 18, 6, 3, 29, 6, 9, 0, 5, 18, 10, 9, 6, 3, 18, 15, 6, 3, 16], [23, 18, 0, 21, 0, 17, 23, 13, 6, 3, 15, 3, 9, 3, 8, 9, 6, 3, 18, 6, 3, 9, 10, 5, 9, 6, 3, 18, 15, 6, 3, 9, 7, 7, 3, 10, 9, 6, 3, 16], [6, 0, 3, 5, 8, 18, 14, 15, 14, 9, 3, 29, 19, 5, 8, 36, 0, 0, 9, 14, 9, 20, 10, 16], [6, 7, 3, 9, 6, 0, 17, 7, 9, 6, 3, 15, 24, 13, 13, 7, 3, 9, 0, 10, 16], [19, 25, 10, 9, 0, 33, 7, 10, 9, 6, 7, 3, 1, 3, 5, 4, 15, 23, 5, 3, 9, 6, 3, 9, 32, 19, 5, 5, 0, 15, 0, 0, 10, 9, 6, 0, 3, 9, 0, 14, 3, 3, 10, 15, 7, 3, 10, 16], [28, 28, 28, 19, 5, 19, 6, 3, 16], [3, 9, 6, 0, 0, 23, 17, 6, 10, 39, 3, 15, 39, 3, 12, 13, 3, 16], [9, 0, 11, 18, 11, 18, 19, 5, 8, 9, 0, 0, 0, 0, 9, 6, 7, 3, 9, 6, 0, 0, 0, 18, 31, 17, 6, 7, 3, 10, 9, 0, 15, 17, 8, 9, 14, 6, 3, 16], [19, 5, 6, 3, 3, 9, 6, 0, 9, 6, 11, 0, 0, 16]]\n",
            "[[0, 2, 2, 2, 2, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 7, 7, 7, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [7, 7, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 7, 7, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 0, 0, 6, 0, 0, 2, 2, 0], [3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 7, 7, 0, 0, 0, 0, 6, 0, 0, 0, 0, 6, 0, 0, 6, 0, 0, 6, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0], [2, 2, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 14, 14, 14, 0, 0, 6], [0, 0, 0, 0, 4, 4, 0, 6, 0, 3, 0, 0, 0, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 4, 0, 0, 8, 0, 0, 8, 0], [0, 0, 0, 0, 7, 0, 0, 0, 6, 0, 0, 0, 0, 0], [0, 6, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 4, 4, 4, 4, 0, 0, 4, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 8, 8, 8, 0, 14, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 4, 4, 0, 2, 2, 0, 2, 0, 0, 10, 10, 0, 0, 0, 0, 4, 0], [0, 2, 2, 0, 4, 4, 4, 0, 0, 0, 0, 10, 10, 0, 0, 0, 4, 0, 0, 0, 0, 0, 6, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 6, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0], [0, 2, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0], [8, 8, 8, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13, 0, 6, 0, 0, 4, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 0, 0], [0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 14, 14, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 7, 0, 0], [7, 7, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 4, 4, 4, 4, 0], [7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 7, 7, 0, 4, 0, 2, 0, 0, 6, 0, 6, 0, 0, 2, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 4, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 8, 0, 2, 0, 8, 0, 0, 8, 0, 8, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 2, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0], [0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 2, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [8, 8, 8, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 4, 4, 4, 0], [4, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 6, 6, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 6, 0, 2, 0, 0, 0, 0, 0, 4, 4, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 4, 7, 7, 0, 2, 0], [0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0], [0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 18, 18, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 9, 9, 0, 2, 2], [0, 8, 0, 5, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 13, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 14, 14, 14, 0, 6], [0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 8, 0], [4, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 6, 0, 0, 6, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 8, 0, 15, 15, 15, 0], [0, 0, 0, 7, 7, 0, 8, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 2, 2, 2, 2, 0, 0, 3, 0, 7, 7, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13, 0, 6, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 8, 0, 0], [0, 2, 0, 4, 4, 4, 4, 0, 0, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 6, 0, 6, 6, 6, 0, 0, 6, 6, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 5, 0, 0, 6, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 0, 0, 0], [0, 0, 0, 7, 0, 7, 0, 0, 0, 12, 12, 12, 12, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0], [0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 16, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [11, 11, 11, 11, 11, 0, 4, 4, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 4, 0], [0, 0, 4, 4, 4, 4, 4, 0, 4, 0, 0, 6, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 4, 4, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 7, 7, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 3, 0, 0], [8, 8, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [0, 4, 0, 4, 4, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0], [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 3, 0, 0, 7, 7, 7, 7, 0, 0, 0, 0, 6, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 7, 0, 0, 5, 0, 0, 7, 7, 7, 7, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 2, 0, 0, 8, 0], [6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 8, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 0, 2, 2, 2, 2, 0], [4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 4, 4, 4, 4, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 6, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 4, 0, 0, 0, 10, 10, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0], [0, 6, 0, 3, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 0, 6, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 13, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0], [4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 4, 0, 0, 8, 0, 0, 13, 13, 13, 13, 0, 0, 6, 0, 6, 0], [4, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0], [0, 0, 7, 0, 0, 0, 17, 17, 17, 17, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 0, 3, 0, 0, 0, 0, 0, 7, 7, 0, 0, 8, 0, 8, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 17, 17, 17, 0, 7, 7, 0, 0, 3, 0, 0, 0, 8, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 2, 0, 7, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 12, 12, 12, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0], [0, 0, 0, 8, 4, 0, 0, 0, 8, 4, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 7, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 0, 6, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6], [0, 0, 0, 0, 5, 0, 2, 2, 0, 0, 0, 0, 0, 6, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 7, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 10, 0, 6], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0], [0, 7, 7, 7, 0, 8, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 7, 7, 2, 0, 0, 3, 0, 0, 0, 7, 7, 7, 7, 0, 6, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 6, 6, 6, 6, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0, 0, 6, 0], [0, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 2, 0, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 4, 0, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 4, 4, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 2, 2, 0], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 0, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 4, 0, 6, 0], [0, 0, 8, 0, 8, 0, 0, 2, 2, 8, 0, 0, 0, 8, 0, 8, 11, 0, 2, 0, 8, 0, 0, 0, 8, 0, 8, 0, 8, 0, 8], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 0, 0, 0, 0, 6, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 6, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 7, 7, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "def align_with_labels(lemma, pos, dep, ent, labels):\n",
        "    len_sent = [len(s) for s in lemma]\n",
        "    len_label = [len(s) for s in labels]\n",
        "\n",
        "    print(len(len_sent) == len(len_label))\n",
        "\n",
        "    for i in range(len(len_sent)):\n",
        "      if len_sent[i] != len_label[i]:\n",
        "          #print(\"Length difference between sent and labels exists. You may execute the block one more time to make them aligned.\")\n",
        "          lemma[i] = lemma[i][:-1]\n",
        "          pos[i] = pos[i][:-1]\n",
        "          dep[i] = dep[i][:-1]\n",
        "          ent[i] = ent[i][:-1]\n",
        "          \n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "align_with_labels(train_lemma, train_pos, train_dep, train_ent, train_labels_encoded)\n",
        "align_with_labels(val_lemma, val_pos, val_dep, val_ent, val_labels_encoded)\n",
        "\n",
        "train_input_index = to_index(train_lemma, word_to_ix)\n",
        "train_output_index = to_index(train_labels, label_to_idx)\n",
        "train_ent_index =  to_index(train_ent, ent_to_ix)\n",
        "train_dep_index = to_index(train_dep, dep_to_ix)\n",
        "train_pos_index =  to_index(train_pos, pos_to_ix)\n",
        "print(len(train_input_index))\n",
        "print(train_pos_index)\n",
        "\n",
        "val_input_index = to_index(val_lemma, word_to_ix)\n",
        "val_output_index = to_index(val_labels, label_to_idx)\n",
        "val_ent_index =  to_index(val_ent, ent_to_ix)\n",
        "val_dep_index = to_index(val_dep, dep_to_ix)\n",
        "val_pos_index =  to_index(val_pos, pos_to_ix)\n",
        "print(val_ent_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehvsSM9urdFG"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## 2. Input Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehBiKSkdWpL2"
      },
      "source": [
        "### *2.1* Semantic Textual Feature Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vogT1tvKTyd"
      },
      "source": [
        "Here, several input embedding models are into consideration:\n",
        "- ~~ELMo(ELMo only captures the meaning of the words)~~\n",
        "- ~~BERT~~\n",
        "\n",
        "**NOTE: sudden crash due to RAM overflow**\n",
        "\n",
        "- ~~Word2Vec~~\n",
        "- Glove(glove-wiki-gigaword-50)\n",
        "- ~~FastText~~\n",
        "\n",
        "In contrast to BERT, XLNET, and ALBERT which are trained on masking random words in a sentence, ELMo is trained on predicting the next word in a sequence. ELMo is relying on bidirectional LSTMâ€™s under the hood and is not transformer-based, like BERT, XLNET, ALBERT, and USE. \n",
        "\n",
        "NOTE: The PoS tag for each word is from **SpaCy**. Dependency tag is of no need and can be very costly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcVZkZI5tfX9",
        "outputId": "5d46525b-c4df-4695-abfe-c6de15905d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-50\")\n",
        "\n",
        "SEM_EMBEDDING_DIM = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM9tISxcxh3X",
        "outputId": "aa100052-f26c-4362-d390-b816cd1f1960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3480, 50)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "sem_embedding_matrix = []\n",
        "\n",
        "for word in word_list:\n",
        "    if word in word_emb_model:\n",
        "        sem_embedding_matrix.append(word_emb_model[word])\n",
        "    else:\n",
        "        sem_embedding_matrix.append([0] * SEM_EMBEDDING_DIM)\n",
        "\n",
        "sem_embedding_matrix = np.array(sem_embedding_matrix)\n",
        "print(sem_embedding_matrix.shape)\n",
        "print(sem_embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dkd5n1WpZDj"
      },
      "source": [
        "### 2.2 Syntactic Textual Feature Embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuAFaovlRpG2",
        "outputId": "1aebceaa-30ec-4df4-cb5c-b5de7e401907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NNP': 0, '-LRB-': 1, ':': 2, 'NN': 3, '-RRB-': 4, 'VBD': 5, 'DT': 6, 'JJ': 7, 'VBN': 8, 'IN': 9, 'NNS': 10, 'CD': 11, 'TO': 12, 'VB': 13, 'VBG': 14, 'CC': 15, '.': 16, 'VBZ': 17, ',': 18, 'PRP': 19, 'PRP$': 20, 'HYPH': 21, \"''\": 22, 'RB': 23, 'MD': 24, 'VBP': 25, 'JJS': 26, 'PDT': 27, '``': 28, 'WRB': 29, 'RBR': 30, 'WDT': 31, 'WP': 32, 'POS': 33, 'RBS': 34, 'NNPS': 35, 'RP': 36, 'UH': 37, 'EX': 38, 'JJR': 39, 'SYM': 40, 'NFP': 41, 'ADD': 42, 'FW': 43, 'LS': 44, '$': 45, 'WP$': 46, 'XX': 47}\n",
            "{'': 0, 'LANGUAGE': 1, 'DATE': 2, 'NORP': 3, 'ORG': 4, 'ORDINAL': 5, 'GPE': 6, 'PERSON': 7, 'CARDINAL': 8, 'TIME': 9, 'LOC': 10, 'PRODUCT': 11, 'WORK_OF_ART': 12, 'FAC': 13, 'QUANTITY': 14, 'LAW': 15, 'MONEY': 16, 'EVENT': 17, 'PERCENT': 18}\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# Map additional features to index\n",
        "import numpy as np \n",
        "\n",
        "# NOTE: All the properties are encoded as one-hot arrays.\n",
        "word_list = list(word_to_ix.keys())\n",
        "pos_embedding = np.eye(len(list(pos_to_ix.values())))\n",
        "dep_embedding = np.eye(len(list(dep_to_ix.values())))\n",
        "ent_embedding = np.eye(len(list(ent_to_ix.values())))\n",
        "\n",
        "print(pos_to_ix)\n",
        "print(ent_to_ix)\n",
        "print(ent_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwO6SbV8OTBZ"
      },
      "source": [
        "### 2.3 Domain Feature Embedding [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_0vViFe3PHD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEy2HutbW0sm"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## 3. NER Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJGoPrgXL37"
      },
      "source": [
        "### 3.1 The Bi-LSTM + Self Attention + CRF Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLAzTs9ON5QG"
      },
      "source": [
        "**NOTE: The model takes too long to train(nearly 30 mins), so you would better to load the models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xYgNXzOyyvl",
        "outputId": "79834883-319a-4466-aa06-4a27ba0e5510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "573\n",
            "[[0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 4, 5, 6, 7, 3, 8, 9, 3, 10, 9, 7, 0, 11, 12, 13, 6, 3, 9, 7, 10, 14, 6, 3, 15, 14, 6, 7, 3, 16], [6, 3, 17, 10, 9, 0, 15, 0, 3, 18, 6, 3, 3, 15, 3, 3, 16], [6, 3, 5, 7, 9, 9, 19, 5, 6, 7, 7, 3, 3, 9, 6, 0, 0, 0, 16], [6, 5, 20, 7, 3, 9, 0, 15, 6, 7, 3, 12, 13, 15, 13, 7, 10, 9, 6, 0, 15, 0, 9, 6, 3, 9, 10, 7, 9, 19, 6, 16], [6, 3, 5, 8, 9, 0, 0, 0, 0, 21, 0, 16, 22], [15, 23, 18, 9, 14, 18, 19, 24, 13, 6, 16, 22, 22, 22], [15, 3, 24, 13, 23, 16], [6, 10, 9, 6, 0, 0, 25, 8, 9, 6, 3, 9, 7, 0, 0, 0, 12, 13, 6, 7, 0, 0, 21, 0, 16], [0], [19, 25, 20, 26, 10, 9, 6, 10, 9, 6, 10, 15, 3, 9, 6, 7, 3, 9, 27, 6, 8, 16], [6, 3, 9, 0, 9, 3, 15, 7, 7, 7, 10, 5, 23, 11, 10, 23, 16], [28, 6, 3, 9, 14, 20, 3, 17, 23, 8, 9, 10, 17, 23, 10, 9, 3, 23, 16], [28, 6, 3, 22, 3, 3, 17, 29, 19, 25, 23, 23, 18, 22, 5, 0, 0, 0, 0, 18, 7, 0, 0, 0], [6, 3, 17, 9, 3, 16], [0, 0, 0, 0, 3, 9, 6, 0, 23, 14, 8, 3, 9, 7, 0, 16], [6, 3, 5, 6, 11, 10, 5, 6, 7, 10, 9, 6, 0, 3, 18, 14, 6, 3, 12, 13, 6, 7, 3, 9, 6, 7, 3, 15, 6, 0, 7, 3, 16], [0, 0, 0, 18, 0, 0, 18, 5, 2], [19, 5, 20, 7, 3, 24, 13, 6, 3, 9, 10, 15, 14, 3, 9, 7, 3, 16], [3, 21, 8, 3, 5, 9, 3, 3, 3, 15, 9, 3, 3, 12, 13, 10, 25, 23, 8, 9, 6, 3, 9, 8, 9, 0, 21, 0, 16], [6, 3, 9, 6, 3, 10, 9, 6, 3, 9, 7, 3, 16, 22], [19, 25, 8, 3, 15, 3, 9, 6, 7, 3, 9, 11, 3, 16], [19, 25, 14, 9, 6, 0, 9, 0, 12, 13, 6, 7, 3, 9, 6, 7, 3, 16], [9, 6, 7, 3, 23, 17, 19, 30, 9, 14, 10, 9, 6, 7, 10, 18, 19, 25, 6, 7, 3, 23, 18, 15, 25, 7, 12, 13, 9, 6, 7, 3, 31, 17, 6, 3, 6, 3, 16], [0, 9, 6, 0, 0, 18, 0, 0, 5, 2], [9, 11, 9, 20, 0, 10, 18, 0, 17, 19, 9, 23, 23, 9, 14, 10, 18, 10, 15, 10, 14, 9, 7, 15, 7, 10, 18, 7, 23, 23, 5, 9, 6, 3, 9, 19, 16], [6, 0, 0, 17, 8, 6, 7, 3, 3, 32, 5, 8, 9, 0, 9, 0, 0, 16], [6, 7, 3, 9, 20, 3, 8, 9, 6, 7, 3, 9, 7, 15, 7, 10, 31, 23, 25, 6, 3, 23, 8, 9, 20, 7, 18, 14, 3, 10, 15, 10, 14, 3, 9, 3, 9, 7, 10, 9, 10, 16], [15, 0, 17, 19, 17, 6, 3, 9, 6, 10, 9, 10, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [9, 6, 7, 11, 10, 18, 6, 0, 21, 8, 3, 22, 33, 11, 10, 5, 11, 10, 9, 0, 10, 18, 31, 25, 11, 7, 10, 18, 6, 3, 21, 15, 21, 3, 7, 15, 11, 3, 10, 9, 3, 9, 0, 10, 12, 13, 0, 18, 0, 5, 16], [9, 3, 9, 6, 10, 15, 10, 9, 0, 0, 0, 18, 19, 25, 20, 7, 3, 15, 20, 26, 10, 9, 20, 3, 18, 10, 15, 3, 3, 10, 16, 22], [19, 17, 3, 25, 12, 13, 9, 6, 7, 3, 23, 9, 6, 0, 0, 3, 16], [6, 3, 9, 0, 18, 29, 7, 18, 24, 13, 23, 7, 3, 14, 0, 15, 13, 6, 3, 9, 6, 3, 16], [19, 25, 7, 9, 6, 3, 9, 20, 7, 3, 31, 17, 6, 7, 3, 9, 0, 9, 0, 15, 0, 16, 22], [6, 7, 3, 17, 8, 9, 6, 0, 3, 17, 8, 7, 10, 9, 6, 7, 10, 9, 23, 34, 11, 10, 18, 15, 9, 0, 17, 8, 7, 10, 9, 26, 23, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [19, 24, 6, 13, 9, 14, 12, 13, 36, 20, 3, 18, 14, 20, 3, 9, 3, 15, 14, 6, 3, 9, 7, 10, 18, 9, 3, 9, 6, 7, 3, 19, 25, 14, 9, 6, 7, 3, 16], [0, 0, 0, 0, 0, 18, 3, 18, 0, 0, 0, 23, 5, 9, 0, 0, 0, 7, 10, 15, 5, 6, 3, 9, 6, 0, 3, 18, 6, 7, 3, 18, 15, 7, 3, 16], [6, 3, 3, 23, 5, 36, 6, 7, 7, 10, 9, 6, 3, 16, 1, 11, 4], [28, 28, 28, 19, 17, 23, 7, 12, 23, 13, 27, 6, 10, 19, 25, 14, 18, 23, 6, 14, 10, 24, 13, 8, 9, 7, 15, 23, 7, 18, 22, 22, 19, 5, 16, 22], [28, 6, 0, 21, 3, 3, 3, 18, 0, 0, 0, 0, 0, 0, 18, 5, 19, 7, 9, 20, 0, 7, 3, 3, 9, 19, 24, 13, 6, 3, 9, 19, 5, 6, 3, 12, 7, 7, 0, 10, 18, 22, 5, 0, 0, 0, 33, 0, 33, 0, 18, 6, 0, 21, 3], [0, 5, 0, 0, 0, 22, 33, 7, 10, 2], [9, 6, 23, 26, 3, 9, 6, 0, 7, 10, 18, 6, 0, 17, 8, 9, 11, 10, 18, 8, 9, 11, 3, 10, 9, 0, 10, 18, 15, 17, 23, 14, 6, 7, 11, 10, 12, 13, 9, 0, 21, 0, 3, 9, 6, 0, 0, 14, 19, 12, 13, 3, 23, 15, 13, 19, 9, 7, 10, 16], [19, 5, 8, 9, 7, 3, 0, 0, 0, 9, 6, 3, 9, 7, 3, 10, 9, 0, 11, 18, 15, 8, 9, 14, 0, 0, 0, 21, 0, 33, 3, 3, 31, 0, 5, 16, 1, 11, 4, 0, 5, 6, 3, 9, 19, 5, 28, 7, 22, 16], [6, 35, 23, 25, 20, 3, 17, 23, 15, 23, 23, 16], [28, 9, 6, 0, 18, 37, 13, 7, 9, 38, 25, 3, 10, 7, 12, 13, 36, 6, 3, 12, 13, 20, 3, 9, 0, 33, 33, 7, 15, 7, 3, 16, 22], [6, 3, 23, 17, 6, 3, 9, 0, 22, 0, 7, 3, 9, 0, 16], [15, 9, 7, 35, 32, 5, 9, 6, 11, 3, 5, 0, 0, 18, 0, 17, 19, 24, 13, 16], [9, 0, 18, 0, 11, 18, 3, 3, 9, 0, 21, 0, 5, 11, 0, 10, 15, 7, 3, 8, 9, 0, 18, 0, 16], [19, 5, 8, 19, 24, 13, 36, 10, 18, 13, 10, 9, 35, 15, 13, 8, 12, 13, 23, 18, 14, 36, 3, 16], [19, 5, 23, 8, 9, 6, 10, 9, 0, 22, 3, 3, 9, 0, 15, 8, 9, 3, 10, 18, 3, 15, 10, 9, 6, 0, 21, 0, 3, 16], [9, 0, 11, 6, 0, 0, 5, 3, 12, 13, 6, 0, 0, 0, 23, 9, 6, 3, 3, 16], [6, 0, 0, 24, 13, 6, 19, 24, 12, 13, 6, 0, 3, 12, 13, 10, 9, 3, 9, 0, 15, 13, 7, 3, 16], [6, 0, 0, 23, 17, 6, 7, 10, 3, 9, 6, 10, 9, 0, 9, 6, 7, 3, 9, 0, 18, 15, 6, 7, 3, 9, 0, 18, 9, 31, 0, 22, 0, 17, 8, 3, 15, 29, 39, 9, 11, 10, 5, 8, 15, 8, 9, 7, 10, 31, 5, 3, 10, 15, 6, 3, 16], [9, 0, 11, 18, 11, 18, 6, 3, 3, 8, 9, 6, 0, 0, 0, 5, 20, 3, 16], [9, 0, 18, 6, 17, 23, 6, 7, 3, 9, 0, 16], [9, 11, 18, 9, 14, 9, 6, 3, 9, 20, 7, 3, 18, 19, 5, 8, 15, 8, 9, 0, 21, 8, 3, 3, 16, 1, 11, 4], [6, 3, 5, 19, 5, 8, 8, 6, 3, 14, 9, 6, 3, 16], [6, 3, 3, 5, 8, 9, 11, 16], [0, 10, 5, 9, 6, 3, 5, 11, 10, 15, 5, 11, 39, 10, 18, 15, 8, 19, 7, 16], [6, 0, 9, 0, 5, 3, 6, 3, 9, 6, 3, 32, 5, 14, 0, 0, 0, 16], [6, 3, 17, 7, 9, 6, 0, 3, 17, 23, 8, 12, 13, 13, 13, 9, 6, 3, 16], [0, 7, 10, 3, 25, 8, 10, 9, 10, 14, 8, 9, 0, 0, 10, 9, 7, 3, 10, 7, 36, 9, 0, 16], [6, 3, 15, 20, 10, 18, 23, 0, 18, 25, 14, 6, 26, 3, 8, 23, 9, 0, 15, 7, 10, 15, 10, 9, 0, 18, 14, 7, 7, 3, 16], [14, 9, 10, 9, 0, 10, 25, 8, 6, 0, 0, 0, 0, 9, 0, 9, 0, 18, 0, 0, 0, 0, 0, 5, 2], [6, 0, 18, 9, 20, 0, 0, 9, 0, 0, 18, 24, 13, 23, 8, 9, 6, 7, 3, 12, 13, 6, 10, 9, 0, 0, 9, 7, 18, 7, 10, 23, 23, 9, 7, 16], [3, 2, 0, 0, 9, 0, 0, 0, 24, 13, 9, 0, 9, 3, 22, 33, 0, 0, 18, 8, 9, 6, 7, 3, 18, 12, 13, 10, 12, 13, 6, 3, 9, 0, 16], [28, 9, 3, 9, 6, 10, 18, 6, 3, 9, 6, 7, 11, 10, 8, 11, 10, 18, 14, 10, 9, 6, 7, 10, 15, 6, 3, 3, 31, 17, 0, 22, 33, 14, 7, 3, 16, 22], [0, 17, 23, 6, 9, 20, 11, 40, 11, 10, 9, 21, 3, 16], [28, 9, 11, 0, 18, 35, 9, 0, 0, 18, 7, 35, 15, 7, 3, 18, 7, 35, 5, 20, 3, 9, 3, 21, 8, 0, 18, 15, 5, 8, 6, 3, 11, 10, 23, 16], [28, 9, 19, 25, 22, 3, 25, 23, 18, 10, 18, 10, 18, 15, 10, 5, 22, 3, 13, 23, 9, 6, 3, 16], [6, 0, 0, 5, 6, 0, 0, 9, 0, 15, 7, 10, 9, 10, 9, 6, 7, 3, 9, 0, 15, 9, 0, 16], [0, 5, 6, 10, 15, 7, 7, 3, 9, 0, 0, 0, 9, 0, 18, 0, 18, 9, 11, 40, 11, 0], [2, 13, 6, 7, 3, 16], [6, 3, 5, 36, 7, 10, 9, 0, 18, 8, 20, 0, 21, 3, 3, 9, 6, 3, 15, 5, 19, 24, 13, 6, 3, 9, 6, 7, 3, 9, 6, 23, 21, 8, 0, 16], [13, 6, 3, 10, 9, 10, 14, 3, 21, 3, 18, 3, 18, 3, 15, 7, 3, 16], [6, 3, 17, 23, 6, 3, 9, 0, 22, 6, 28, 3, 22, 9, 0, 17, 6, 3, 15, 9, 19, 17, 19, 23, 16], [19, 25, 6, 3, 22, 33, 3, 24, 13, 23, 9, 6, 3, 16], [9, 10, 9, 3, 15, 3, 10, 18, 6, 11, 10, 5, 7, 12, 13, 6, 10, 9, 6, 3, 9, 10, 15, 10, 9, 10, 9, 14, 20, 3, 23, 5, 6, 7, 3, 9, 6, 10, 12, 13, 6, 7, 10, 16], [15, 18, 6, 0, 11, 0, 3, 5, 6, 3, 3, 19, 18, 19, 5, 16], [9, 0, 33, 33, 3, 18, 0, 5, 10, 9, 20, 3, 18, 0, 0, 0, 21, 0, 18, 15, 23, 15, 9, 6, 3, 16], [15, 9, 0, 18, 10, 9, 0, 25, 8, 12, 13, 8, 11, 7, 7, 3, 10, 19, 5, 8, 14, 9, 6, 3, 9, 0, 16], [16, 9, 0, 18, 11, 10, 5, 11, 0, 7, 10, 18, 5, 6, 3, 3, 18, 6, 7, 3, 3, 18, 11, 0, 21, 8, 10, 18, 11, 35, 18, 6, 0, 3, 18, 11, 10, 10, 18, 11, 7, 10, 18, 6, 3, 3, 18, 15, 6, 3, 18, 8, 11, 3, 10, 2, 15, 5, 11, 3, 10, 15, 6, 0, 7, 3, 16], [6, 0, 17, 12, 13, 3, 12, 13, 9, 6, 3, 15, 10, 9, 0, 9, 6, 3, 9, 3, 16], [6, 3, 5, 12, 13, 6, 7, 3, 10, 17, 8, 9, 28, 0, 22, 15, 0, 0, 0, 16], [0, 0, 24, 13, 9, 6, 3, 9, 6, 0, 0, 0, 15, 13, 8, 36, 12, 13, 20, 10, 9, 0, 0, 16, 0, 0, 32, 24, 23, 13, 9, 6, 3, 9, 6, 0], [8, 10, 9, 6, 3, 5, 7, 12, 13, 3, 9, 7, 10, 9, 6, 3, 3, 9, 7, 3, 10, 16], [6, 3, 19, 17, 9, 6, 7, 3, 18, 19, 17, 36, 9, 10, 23, 16], [8, 11, 10, 18, 14, 11, 10, 16], [19, 5, 8, 15, 8, 9, 6, 7, 7, 7, 3, 8, 9, 6, 0, 3, 9, 0, 11, 18, 11, 16, 1, 11, 4, 1, 11, 4], [23, 23, 38, 25, 7, 10, 23, 15, 6, 3, 17, 8, 6, 7, 3, 23, 9, 14, 6, 7, 3, 16], [0, 5, 6, 3, 28, 3, 3, 9, 6, 0, 21, 0, 3, 9, 0, 18, 6, 0, 21, 8, 3, 23, 11, 10, 1, 11, 10, 4, 23, 9, 0, 18, 5, 8, 8, 9, 0, 7, 10, 29, 19, 5, 8, 9, 6, 3], [19, 17, 8, 12, 13, 10, 15, 13, 10, 9, 7, 10, 15, 17, 9, 7, 3, 16], [19, 25, 23, 7, 15, 8, 9, 10, 9, 0, 17, 8, 11, 10, 9, 6, 0, 0, 0, 3, 9, 0, 3, 18, 0, 16], [19, 23, 17, 7, 18, 7, 18, 7, 15, 7, 3, 18, 3, 15, 3, 16], [6, 7, 3, 2, 14, 0, 15, 0, 2, 24, 23, 13, 12, 13, 6, 7, 3, 9, 35, 12, 13, 13, 6, 3, 9, 6, 7, 3, 8, 9, 7, 3, 16, 22], [6, 0, 0, 18, 0, 0, 18, 5, 2], [9, 20, 0, 3, 3, 24, 13, 7, 18, 35, 25, 7, 10, 14, 12, 13, 20, 10, 18, 15, 19, 25, 12, 13, 20, 7, 10, 12, 13, 15, 13, 6, 0, 13, 20, 7, 3, 16], [15, 11, 10, 9, 20, 3, 5, 9, 6, 14, 0, 3, 8, 36, 9, 6, 3, 16], [19, 25, 19, 13, 9, 3, 9, 0, 17, 8, 16], [28, 19, 22, 6, 7, 10, 25, 6, 3, 9, 7, 3, 18, 28, 0, 5, 16], [0, 0, 5, 8, 9, 6, 3, 9, 3, 10, 18, 10, 15, 7, 10, 9, 6, 0, 16], [9, 39, 3, 18, 10, 24, 13, 6, 0, 0, 0, 35, 0, 0, 35, 0, 9, 11, 21, 11, 21, 11, 16], [13, 41, 42, 9, 0, 15, 9, 0], [15, 3, 17, 23, 8, 19, 39, 16], [7, 10, 10, 25, 8, 9, 6, 3, 9, 7, 10, 24, 13, 9, 6, 7, 3, 12, 13, 0, 16], [19, 23, 5, 6, 7, 3, 9, 10, 9, 6, 0, 35, 9, 0, 18, 0, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 9, 6, 3, 9, 6, 3, 16], [28, 28, 28, 23, 6, 10, 25, 9, 3, 18, 22, 22, 19, 17, 16], [9, 20, 7, 3, 10, 15, 14, 9, 7, 7, 3, 9, 0, 15, 0, 18, 0, 17, 8, 10, 9, 10, 9, 7, 3, 18, 14, 19, 12, 13, 15, 13, 7, 10, 16], [3, 2, 24, 6, 0, 0, 25, 9, 6, 0, 3, 9, 0, 9, 0, 11, 18, 32, 24, 13, 40, 9, 31, 3, 18, 31, 7, 10, 24, 19, 13, 18, 43, 16, 16], [19, 25, 9, 6, 3, 16], [6, 7, 3, 5, 9, 3, 10, 9, 11, 10, 9, 3, 18, 14, 6, 3, 9, 0, 0, 0, 16], [9, 19, 25, 23, 8, 7, 18, 6, 0, 0, 17, 7, 3, 15, 3, 16], [6, 3, 5, 9, 0, 18, 15, 5, 36, 9, 6, 7, 3, 9, 6, 3, 9, 7, 17, 3, 15, 10, 10, 10, 16], [19, 5, 15, 5, 11, 10, 18, 14, 11, 3, 32, 5, 8, 16], [19, 5, 9, 0, 12, 13, 20, 3, 18, 6, 11, 21, 3, 21, 7, 3, 19, 5, 23, 9, 10, 5, 16], [6, 3, 24, 13, 6, 3, 9, 0, 10, 12, 13, 7, 10, 9, 0, 15, 0, 15, 12, 13, 10, 12, 23, 13, 6, 3, 9, 0, 22, 3, 9, 0, 15, 0, 15, 20, 3, 9, 6, 7, 10, 16], [19, 5, 19, 9, 14, 12, 13, 36, 7, 10, 9, 23, 16], [9, 6, 7, 21, 3, 0, 18, 6, 0, 0, 17, 7, 9, 3, 9, 0, 22, 19, 23, 8, 3, 15, 10, 2, 10, 8, 9, 0, 22, 33, 3, 16], [6, 3, 5, 6, 7, 3, 9, 3, 9, 7, 10, 9, 6, 7, 10, 9, 0, 3, 9, 0, 18, 0, 16], [19, 23, 17, 12, 13, 7, 3, 15, 3, 3, 2, 9, 6, 23, 17, 8, 30, 7, 9, 6, 3, 9, 6, 0, 0, 3, 15, 7, 7, 7, 10, 9, 0, 16], [6, 3, 5, 8, 9, 6, 3, 9, 6, 3, 18, 23, 9, 6, 3, 9, 6, 3, 16], [6, 0, 0, 23, 17, 6, 7, 7, 10, 9, 0, 3, 18, 14, 11, 9, 0, 5, 9, 0, 31, 23, 5, 10, 9, 6, 7, 3, 16], [28, 19, 25, 23, 8, 9, 6, 3, 9, 6, 0, 0, 9, 11, 9, 20, 7, 3, 10, 17, 8, 8, 9, 0, 9, 14, 19, 9, 6, 3, 9, 0, 18, 22, 0, 5, 9, 6, 0, 11, 3, 16], [16, 0, 0, 18, 11, 3, 5, 6, 0, 21, 8, 3, 16], [0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 11, 3, 4, 17, 6, 7, 7, 3, 3, 3, 31, 17, 9, 0, 18, 0, 18, 15, 0, 40, 0, 16], [0, 0, 0, 21, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 2, 5, 11, 0, 11, 4, 6, 3, 9, 0, 21, 0, 22, 19, 25, 18, 5, 6, 7, 1, 11, 4, 7, 7, 3, 15, 6, 3, 9, 6, 7, 7, 3, 9, 6, 0, 21, 0, 3, 9, 6, 0], [15, 10, 25, 8, 6, 7, 3, 9, 0, 9, 10, 8, 9, 8, 10, 2, 15, 10, 0, 10, 15, 10, 12, 13, 19, 16], [6, 7, 10, 25, 6, 7, 3, 9, 20, 7, 10, 9, 19, 2, 9, 3, 15, 3, 18, 7, 10, 18, 15, 7, 15, 7, 10, 16], [19, 17, 7, 9, 6, 7, 3, 9, 6, 3, 9, 3, 18, 7, 14, 9, 20, 3, 15, 9, 6, 35, 9, 0, 16, 1, 11, 4], [6, 17, 6, 7, 3, 2, 0, 17, 8, 8, 9, 20, 7, 15, 7, 3, 9, 6, 7, 3, 0, 16], [28, 38, 17, 23, 11, 3, 12, 13, 6, 3, 2, 6, 0, 21, 8, 7, 3, 8, 9, 6, 3, 23, 9, 6, 0, 3, 16], [28, 6, 7, 3, 23, 17, 7, 3, 9, 0, 18, 15, 6, 17, 6, 7, 3, 16], [9, 11, 0, 11, 18, 19, 5, 9, 19, 5, 8, 15, 5, 10, 9, 0, 15, 0, 18, 15, 5, 8, 11, 35, 9, 0, 11, 16, 1, 11, 4, 1, 11, 4], [6, 0, 3, 17, 6, 10, 9, 7, 10, 10, 23, 16], [0, 5, 6, 3, 9, 6, 3, 9, 0, 0, 33, 33, 3, 18, 15, 5, 3, 9, 6, 0, 21, 0, 0, 9, 11, 40, 11, 18, 15, 6, 3, 9, 0, 9, 11, 40, 11, 16], [10, 25, 6, 3, 9, 0, 5, 6, 3, 3, 9, 9, 6, 3, 9, 3, 15, 3, 9, 3, 16], [6, 3, 17, 6, 7, 3, 9, 6, 3, 9, 0, 12, 13, 39, 3, 9, 7, 3, 10, 16], [6, 7, 3, 24, 13, 8, 9, 6, 0, 35, 0, 3, 3, 8, 9, 6, 3, 9, 0, 9, 6, 0, 0, 0, 16], [19, 25, 23, 8, 39, 9, 11, 3, 9, 6, 3, 19, 23, 5, 23, 16], [28, 6, 3, 17, 8, 7, 10, 9, 6, 3, 18, 15, 19, 28, 13, 14, 12, 13, 7, 19, 25, 14, 6, 3, 12, 13, 6, 3, 9, 20, 10, 18, 22, 5, 0, 16, 0, 0, 0, 0, 18, 3, 18, 0, 0, 35, 0, 0, 0, 21, 0, 0, 0], [0, 9, 0, 0, 0], [19, 24, 13, 12, 13, 6, 7, 3, 12, 13, 15, 13, 0, 22, 3, 9, 9, 19, 24, 23, 23, 13, 6, 32, 25, 20, 8, 3, 16], [23, 17, 11, 18, 11, 16], [14, 6, 3, 9, 0, 6, 0, 0, 24, 13, 9, 6, 0, 18, 14, 0, 0, 9, 11, 0, 16], [6, 7, 10, 9, 7, 3, 25, 6, 7, 3, 9, 6, 3, 12, 13, 3, 10, 9, 0, 22, 3, 15, 12, 13, 6, 3, 6, 7, 3, 17, 16], [6, 3, 9, 0, 17, 6, 26, 3, 8, 9, 0, 9, 0, 16], [0, 9, 6, 0, 0, 0, 0, 10, 9, 10, 9, 10, 9, 6, 3, 9, 0, 8, 9, 0, 15, 0, 9, 11, 0, 16], [0, 9, 6, 0, 0, 0, 0, 10, 9, 6, 3, 9, 6, 7, 3, 12, 13, 6, 3, 9, 0, 9, 0, 16], [6, 3, 5, 9, 6, 3, 9, 6, 3, 18, 23, 11, 10, 1, 11, 10, 4, 9, 6, 7, 3], [7, 3, 10, 15, 8, 0, 7, 7, 10, 25, 8, 8, 9, 14, 7, 10, 9, 7, 10, 9, 0, 16], [16, 0, 0, 18, 11, 3, 5, 6, 0, 7, 3, 16], [9, 3, 9, 6, 0, 0, 35, 18, 0, 3, 5, 6, 3, 3, 9, 6, 3, 12, 13, 3, 10, 14, 9, 7, 10, 16], [0, 5, 3, 3, 15, 10, 9, 6, 3, 18, 31, 19, 5, 5, 23, 13, 6, 7, 10, 15, 10, 9, 6, 3, 16], [19, 25, 14, 10, 18, 10, 15, 10, 9, 7, 10, 18, 9, 9, 10, 15, 10, 14, 6, 7, 7, 10, 16], [6, 10, 5, 8, 9, 3, 9, 0, 0, 0, 18, 6, 3, 12, 13, 6, 0, 3, 3, 15, 6, 3, 19, 25, 9, 0, 18, 0, 18, 15, 6, 39, 7, 3, 16], [19, 23, 17, 9, 6, 10, 9, 10, 12, 13, 9, 19, 17, 9, 18, 9, 6, 3, 6, 3, 17, 36, 18, 6, 3, 39, 9, 6, 3, 9, 3, 9, 6, 3, 17, 16], [6, 17, 19, 24, 13, 6, 3, 9, 7, 3, 12, 13, 18, 15, 13, 9, 18, 0, 3, 9, 6, 7, 3, 16], [9, 6, 7, 11, 10, 18, 7, 10, 5, 20, 7, 10, 9, 0, 0, 9, 0, 15, 6, 7, 10, 9, 11, 10, 9, 0, 18, 6, 3, 22, 42, 23, 21, 26, 3, 18, 0, 3, 10, 3, 0, 0, 16, 0, 0, 5, 3], [0], [23, 23, 6, 10, 18, 9, 6, 3, 19, 25, 7, 2, 6, 0, 0, 17, 7, 9, 14, 6, 0, 0, 0, 9, 35, 9, 6, 3, 9, 6, 0, 0, 10, 9, 6, 7, 15, 7, 3, 9, 9, 0, 16], [3, 10, 25, 7, 10, 18, 14, 7, 18, 7, 18, 7, 15, 3, 16], [10, 5, 16], [16, 0, 0, 0, 0, 18, 11, 3, 5, 6, 3, 3, 3, 16], [0, 17, 14, 8, 23, 9, 6, 7, 3, 18, 6, 3, 9, 7, 10, 18, 15, 6, 3, 15, 3, 9, 20, 10, 16], [28, 28, 28, 19, 25, 12, 13, 9, 8, 21, 36, 10, 12, 13, 19, 9, 10, 16], [16, 0, 0, 18, 11, 10, 5, 6, 0, 7, 3, 15, 5, 11, 7, 10, 16], [6, 0, 0, 17, 7, 9, 6, 3, 16], [23, 34, 11, 0, 35, 15, 6, 7, 3, 9, 7, 10, 5, 16, 1, 11, 4, 1, 11, 4, 1, 11, 4, 6, 7, 3, 9, 0, 18, 0, 0, 0, 18, 5, 9, 7, 10, 5, 8, 11, 10, 8, 9, 6, 7, 3, 9, 6, 3, 16, 1, 11, 4], [6, 0, 17, 14, 9, 20, 10, 12, 13, 6, 7, 0, 0, 12, 13, 6, 7, 10, 9, 0, 22, 33, 3, 15, 0, 22, 33, 3, 16], [23, 23, 19, 17, 7, 9, 20, 3, 9, 6, 3, 9, 11, 3, 15, 6, 3, 9, 6, 7, 16], [28, 6, 7, 3, 21, 7, 3, 17, 20, 10, 9, 6, 3, 28, 33, 3, 18, 10, 15, 10, 18, 22, 0, 0, 0, 0, 0, 0, 0, 18, 3, 9, 0, 0, 0, 0, 21, 0, 0, 0, 18, 5, 9, 6, 3], [6, 3, 9, 7, 10, 12, 13, 6, 3, 9, 0, 17, 6, 3, 23, 9, 14, 0, 9, 0, 16], [0, 0, 17, 7, 0, 10, 10, 31, 25, 7, 3, 9, 0, 22, 17, 7, 3, 16], [7, 10, 5, 6, 0, 3, 5, 8, 10, 9, 6, 3, 3, 12, 13, 7, 10, 16], [19, 5, 8, 9, 0, 0, 21, 0, 9, 11, 18, 15, 5, 23, 8, 9, 20, 3, 0, 0, 21, 0, 18, 1, 11, 4, 32, 24, 23, 13, 6, 0, 9, 0, 16], [6, 17, 39, 9, 23, 6, 26, 3, 3, 9, 6, 7, 3, 16], [8, 9, 6, 3, 21, 8, 3, 18, 19, 28, 17, 23, 9, 6, 7, 3, 31, 17, 3, 18, 3, 15, 3, 9, 7, 9, 6, 8, 0, 3, 9, 0, 16], [28, 28, 6, 3, 9, 6, 0, 9, 0, 17, 23, 18, 8, 20, 7, 18, 7, 15, 7, 3, 16, 22, 22, 22], [0, 0, 0, 0, 9, 0, 17, 11, 9, 6, 26, 3, 10, 9, 0, 16], [6, 3, 18, 9, 8, 9, 6, 0, 10, 18, 17, 11, 15, 39, 7, 10, 31, 25, 9, 23, 6, 7, 7, 3, 12, 13, 6, 7, 18, 23, 7, 3, 9, 6, 3, 16], [23, 9, 0, 18, 20, 7, 10, 15, 6, 10, 9, 0, 18, 19, 24, 23, 13, 7, 3, 9, 3, 16], [19, 17, 23, 35, 7, 16, 1, 11, 4], [0, 0, 17, 3, 9, 0, 15, 0, 0, 10, 9, 3, 10, 15, 3, 3, 16], [28, 28, 7, 10, 24, 13, 9, 6, 5, 6, 7, 7, 3, 31, 5, 10, 15, 13, 20, 10, 16], [11, 3, 19, 5, 9, 5, 9, 9, 23, 11, 7, 10, 9, 3, 0, 24, 13, 9, 10, 16], [28, 0, 22, 17, 7, 15, 7, 3, 5, 9, 3, 9, 6, 26, 10, 9, 7, 3, 15, 23, 24, 23, 13, 8, 9, 19, 25, 12, 13, 3, 9, 15, 23, 13, 6, 10, 8, 9, 0, 16], [38, 24, 13, 6, 7, 3, 9, 0, 16], [0, 0, 1, 0, 2, 3, 0, 3, 3, 3, 0, 0, 0, 11, 3, 4, 18, 8, 0, 0, 0, 0, 21, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 3, 4, 23, 9, 11, 40, 11, 9, 0, 18, 5, 6, 7, 7, 7, 9, 0, 21, 0, 9, 0, 1, 0, 4, 16], [22], [16, 9, 0, 18, 11, 10, 5, 6, 0, 7, 3, 15, 5, 11, 0, 21, 8, 3, 15, 6, 3, 16], [28, 28, 28, 6, 19, 25, 17, 19, 28, 3, 23, 14, 8, 9, 18, 22, 22, 19, 17, 16, 22], [6, 3, 9, 6, 10, 25, 9, 6, 7, 3, 18, 23, 14, 19, 17, 23, 7, 18, 22, 22, 19, 17, 16, 22], [0, 21, 0, 3], [19, 23, 17, 6, 7, 10, 9, 31, 0, 22, 17, 3, 9, 6, 18, 15, 7, 18, 8, 10, 24, 13, 30, 8, 16], [6, 3, 17, 19, 6, 3, 15, 6, 7, 3, 23, 16], [6, 3, 17, 12, 13, 11, 3, 10, 9, 3, 9, 14, 10, 12, 13, 7, 3, 9, 6, 7, 3, 15, 12, 13, 10, 15, 10, 16], [9, 6, 3, 18, 10, 5, 6, 14, 10, 2], [23, 11, 18, 11, 9, 0, 33, 33, 3, 9, 11, 18, 11, 25, 8, 16], [19, 24, 13, 9, 0, 9, 7, 0, 6, 3, 9, 7, 10, 12, 13, 7, 3, 9, 6, 3, 3, 9, 0, 18, 15, 9, 3, 15, 3, 18, 9, 7, 3, 15, 3, 9, 6, 10, 31, 24, 13, 8, 9, 0, 9, 0, 16, 9, 3, 18, 6, 0, 24, 13, 14, 6, 3, 9, 0, 18, 3, 21, 3, 9, 7, 3, 22, 33, 0, 3, 9, 0, 18, 9, 3, 16], [9, 6, 3, 18, 6, 3, 9, 6, 3, 9, 3, 9, 10, 9, 6, 3, 9, 6, 0, 0, 17, 7, 16], [0, 0], [0, 0, 0, 5, 14, 15, 14, 20, 7, 10, 18, 9, 3, 9, 0, 0, 0, 16, 22], [19, 28, 3, 23, 9, 20, 3, 18, 19, 25, 10, 16, 22, 22, 22], [9, 9, 20, 10, 9, 0, 3, 3, 10, 15, 0, 0, 10, 18, 20, 3, 24, 13, 0, 28, 33, 3, 12, 13, 0, 15, 13, 7, 10, 9, 6, 0, 16], [23, 18, 9, 0, 18, 19, 24, 13, 8, 23, 6, 3, 3, 16], [19, 23, 25, 6, 3, 9, 0, 12, 13, 10, 15, 25, 23, 23, 8, 35, 18, 6, 19, 25, 12, 13, 14, 16], [0, 14, 18, 10, 9, 6, 0, 24, 23, 13, 7, 12, 13, 10, 16], [28, 9, 11, 6, 7, 21, 3, 3, 18, 31, 5, 6, 3, 15, 3, 9, 6, 0, 0, 18, 5, 8, 9, 6, 3, 16], [0, 0, 2, 19, 23, 5, 9, 6, 23, 7, 3, 9, 0, 15, 19, 5, 6, 10, 9, 19, 18, 15, 19, 25, 12, 13, 9, 6, 3, 12, 13, 29, 19, 25, 9, 19, 16], [28, 9, 0, 11, 18, 11, 10, 8, 9, 35, 18, 23, 8, 9, 10, 9, 0, 17, 8, 9, 6, 0, 0, 0, 18, 1, 11, 4, 5, 8, 12, 13, 8, 8, 9, 6, 0, 3, 3, 3, 0, 35, 16, 1, 44, 4, 6, 3, 10, 5, 8, 9, 28, 28, 0, 0, 22, 22, 18, 8, 12, 13, 6, 3, 9, 6, 0, 7, 3, 22, 0, 0, 9, 0, 35, 0, 16, 1, 11, 4, 22], [9, 19, 24, 13, 9, 6, 23, 16], [6, 3, 17, 8, 12, 13, 10, 18, 9, 23, 10, 18, 14, 9, 29, 7, 3, 19, 25, 9, 6, 11, 18, 11, 12, 11, 18, 11, 10, 8, 12, 13, 9, 0, 16], [9, 0, 18, 10, 5, 7, 11, 10, 15, 11, 10, 9, 0, 18, 23, 23, 9, 0, 18, 9, 19, 5, 14, 9, 6, 3, 9, 7, 10, 9, 6, 7, 3, 32, 5, 14, 8, 9, 17, 12, 13, 9, 6, 3, 16], [19, 24, 13, 14, 9, 6, 3, 9, 6, 7, 3, 9, 6, 3, 7, 9, 6, 10, 12, 13, 9, 6, 3, 9, 27, 6, 3, 16], [9, 14, 8, 9, 7, 10, 18, 0, 5, 23, 8, 9, 0, 9, 20, 3, 9, 3, 9, 6, 3, 16, 1, 11, 4, 9, 6, 3, 18, 0, 5, 8, 9, 0, 18, 15, 5, 8, 9, 7, 10, 9, 19, 5, 12, 13, 6, 11, 7, 3, 16, 1, 11, 4], [2, 13, 10, 9, 10, 16], [19, 25, 6, 8, 9, 20, 3, 12, 13, 6, 23, 21, 14, 3, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [28, 6, 0, 0, 5, 6, 0, 22, 17, 3, 9, 14, 6, 3, 9, 0, 12, 13, 3, 15, 12, 13, 6, 35, 7, 3, 9, 6, 3, 16], [0, 0, 1, 0, 2, 11, 3, 0, 0, 3, 3, 3, 0, 0, 0, 4, 18, 23, 8, 9, 0, 0, 18, 1, 11, 4, 17, 6, 0, 0, 16], [3, 10, 25, 8, 9, 7, 10, 16], [0, 0, 2, 10, 18, 3, 16], [19, 17, 17, 6, 7, 3, 9, 14, 10, 23, 23, 9, 10, 16, 22, 22, 22], [19, 5, 6, 7, 3, 9, 6, 0, 22, 0, 0, 0, 15, 5, 8, 9, 20, 3, 9, 0, 18, 0, 18, 9, 14, 8, 9, 10, 12, 13, 12, 13, 0, 22, 3, 10, 9, 6, 7, 0, 35, 16], [6, 0, 24, 13, 12, 13, 9, 6, 7, 3, 12, 13, 6, 3, 9, 6, 3, 22, 0, 0, 3, 3, 18, 7, 15, 7, 7, 3, 18, 6, 7, 15, 7, 3, 18, 15, 6, 3, 9, 6, 3, 3, 16], [6, 0, 0, 3, 5, 2], [20, 10, 25, 9, 6, 3, 15, 10, 9, 6, 32, 25, 8, 8, 15, 8, 9, 27, 6, 7, 3, 16], [6, 3, 17, 6, 3, 9, 6, 7, 3, 9, 10, 9, 6, 3, 9, 6, 0, 0, 22, 0, 0, 0, 9, 6, 3, 9, 0, 18, 0, 0, 0, 0, 22, 0, 3, 12, 13, 7, 7, 7, 10, 9, 6, 3, 16], [28, 28, 9, 19, 25, 22, 3, 3, 23, 23, 16], [15, 23, 38, 17, 17, 6, 7, 3, 32, 25, 19, 25, 6, 3, 9, 12, 13, 36, 18, 23, 23, 32, 16], [9, 3, 18, 6, 10, 5, 3, 9, 6, 3, 3, 9, 0, 9, 6, 3, 15, 0, 9, 6, 3, 16], [6, 7, 0, 35, 5, 9, 19, 5, 20, 3, 12, 13, 0, 18, 0, 18, 0, 11, 18, 11, 16], [6, 7, 3, 9, 6, 3, 17, 12, 13, 3, 9, 6, 0, 0, 9, 0, 0, 9, 6, 0, 3, 9, 9, 3, 15, 6, 11, 0, 10, 21, 3, 3, 16], [23, 18, 6, 3, 9, 0, 24, 13, 35, 12, 13, 9, 6, 3, 7, 9, 0, 0, 9, 0, 15, 7, 10, 15, 13, 0, 6, 7, 3, 12, 13, 7, 10, 9, 6, 0, 16], [19, 25, 20, 7, 10, 9, 6, 10, 15, 10, 9, 6, 10, 18, 15, 25, 6, 8, 9, 6, 3, 25, 23, 16], [9, 14, 8, 9, 0, 18, 19, 5, 23, 8, 9, 0, 0, 9, 20, 3, 16], [6, 0, 0, 17, 12, 13, 3, 12, 13, 9, 6, 7, 10, 9, 19, 25, 0, 15, 6, 3, 19, 17, 16], [14, 6, 3, 6, 0, 0, 5, 2], [9, 0, 11, 18, 0, 7, 10, 5, 11, 10, 9, 0, 10, 9, 0, 15, 0, 16], [6, 0, 22, 33, 3, 12, 13, 0, 17, 14, 16], [9, 0, 11, 6, 0, 0, 5, 8, 9, 6, 0, 0, 9, 7, 10, 9, 3, 31, 5, 11, 7, 10, 15, 6, 7, 7, 3, 8, 16], [6, 0, 0, 23, 17, 3, 22, 17, 7, 18, 7, 7, 3, 9, 0, 0, 31, 5, 15, 5, 3, 10, 16], [28, 28, 22, 23, 18, 9, 3, 18, 29, 19, 25, 8, 14, 6, 3, 19, 25, 23, 9, 11, 9, 6, 10, 9, 6, 7, 3, 15, 13, 19, 9, 11, 19, 17, 23, 8, 16], [6, 3, 17, 23, 8, 12, 13, 10, 9, 14, 3, 15, 10, 10, 23, 15, 9, 6, 3, 15, 29, 14, 9, 3, 3, 16], [6, 0, 0, 0, 17, 7, 9, 6, 3, 9, 3, 9, 10, 9, 6, 3, 9, 3, 18, 14, 6, 3, 9, 7, 10, 18, 17, 7, 16], [19, 25, 7, 9, 6, 7, 3, 17, 8, 6, 0, 0, 35, 9, 20, 10, 12, 13, 0, 15, 9, 20, 3, 3, 18, 0, 25, 14, 6, 3, 16], [0, 16, 0, 0, 18, 3, 9, 0, 0, 0, 5, 18, 28, 19, 22, 0, 8, 23, 7, 9, 6, 3, 9, 6, 3, 6, 12, 13, 0, 19, 24, 13, 12, 13, 9, 7, 7, 10, 14, 9, 6, 3, 9, 0, 15, 0, 16], [6, 10, 9, 0, 18, 0, 15, 35, 25, 6, 7, 3, 9, 6, 0, 3, 9, 6, 3, 16], [0], [19, 25, 19, 22, 28, 6, 0, 22, 22, 16, 22], [9, 6, 11, 10, 9, 6, 3, 18, 6, 10, 9, 0, 25, 8, 9, 6, 7, 3, 18, 9, 3, 9, 6, 0, 0, 18, 31, 17, 6, 10, 9, 0, 9, 6, 7, 3, 16], [19, 24, 23, 13, 9, 7, 0, 7, 10, 12, 13, 20, 7, 3, 9, 6, 7, 0, 35, 18, 0, 0, 35, 18, 15, 10, 9, 3, 9, 6, 3, 3, 9, 0, 16], [9, 6, 7, 3, 18, 19, 28, 17, 23, 8, 7, 9, 10, 12, 13, 0, 18, 15, 19, 25, 8, 14, 10, 9, 0, 21, 20, 10, 23, 8, 9, 14, 10, 18, 23, 7, 9, 19, 25, 6, 3, 22, 3, 10, 16], [0, 17, 8, 23, 7, 9, 7, 10, 15, 7, 3, 3, 18, 15, 17, 8, 3, 9, 6, 7, 3, 14, 7, 10, 9, 6, 7, 10, 16], [3, 2, 0, 0, 18, 5, 19, 13, 0, 16], [0, 17, 8, 3, 9, 0, 9, 11, 10, 18, 15, 6, 35, 25, 8, 11, 12, 11, 3, 9, 6, 3, 9, 3, 10, 9, 6, 3, 9, 6, 3, 5, 0, 11, 18, 0, 5, 16], [28, 19, 25, 6, 3, 9, 11, 9, 20, 7, 18, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 18, 32, 5, 8, 3, 9, 7, 0, 16], [9, 6, 3, 12, 13, 6, 7, 3, 9, 0, 18, 7, 3, 10, 25, 8, 3, 9, 14, 10, 15, 9, 3, 10, 23, 8, 18, 19, 5, 16], [6, 10, 5, 20, 10, 14, 9, 6, 3, 33, 33, 3, 3, 18, 23, 9, 20, 10, 15, 10, 16], [19, 25, 6, 7, 10, 31, 25, 8, 8, 9, 0, 0, 0, 15, 20, 3, 9, 7, 10, 9, 3, 9, 6, 18, 14, 3, 22, 33, 3, 16], [28, 19, 25, 7, 9, 0, 0, 24, 13, 14, 9, 3, 9, 0, 11, 15, 14, 23, 9, 6, 3, 19, 24, 13, 9, 7, 0, 18, 0, 15, 7, 3, 3, 10, 9, 6, 0, 18, 22, 5, 0, 16], [28, 38, 17, 11, 3, 32, 17, 6, 3, 6, 3, 18, 23, 18, 9, 19, 17, 7, 3, 16], [2, 3, 15, 3, 3, 3, 16], [16, 9, 0, 0, 18, 11, 10, 5, 11, 10, 16], [19, 5, 8, 9, 11, 9, 6, 7, 3, 0, 16], [0, 9, 6, 0, 0, 18, 0, 0, 18, 17, 9, 6, 3, 9, 6, 0, 3, 9, 0], [9, 6, 3, 19, 5, 12, 13, 19, 12, 13, 16, 22, 22, 22], [19, 17, 3, 23, 5, 36, 6, 3, 3, 9, 7, 10, 18, 10, 18, 15, 18, 9, 3, 18, 10, 16], [6, 7, 3, 17, 23, 15, 8, 15, 5, 9, 19, 17, 36, 10, 9, 0, 16], [9, 7, 15, 7, 18, 6, 5, 6, 7, 3, 18, 9, 23, 26, 11, 10, 8, 15, 7, 3, 9, 7, 10, 8, 16], [28, 19, 25, 30, 8, 6, 7, 10, 9, 3, 9, 0, 18, 31, 9, 19, 22, 0, 5, 23, 17, 6, 0, 22, 33, 3, 39, 15, 0, 22, 19, 3, 30, 7, 18, 22, 0, 5, 16], [23, 9, 0, 18, 0, 7, 10, 5, 11, 10, 8, 9, 15, 9, 3, 9, 6, 3, 9, 0, 14, 3, 18, 3, 18, 3, 18, 15, 23, 8, 3, 23, 23, 9, 3, 3, 9, 0, 10, 16], [19, 25, 8, 7, 18, 14, 0, 24, 13, 3, 15, 3, 15, 19, 17, 6, 3, 19, 24, 13, 16], [22], [19, 25, 8, 9, 10, 9, 6, 3, 9, 10, 9, 10, 18, 6, 7, 3, 9, 10, 15, 6, 3, 9, 10, 23, 7, 9, 11, 9, 10, 18, 10, 15, 3, 10, 16], [19, 17, 8, 12, 13, 8, 10, 10, 8, 9, 0, 33, 33, 0, 3, 7, 10, 9, 0, 33, 33, 7, 3, 5, 9, 11, 16], [19, 17, 22, 3, 25, 6, 30, 7, 9, 6, 16, 22], [0, 0, 0, 0, 17, 7, 3, 3, 9, 7, 3, 9, 14, 6, 3, 9, 6, 3, 9, 7, 3, 10], [14, 23, 11, 10, 9, 6, 3, 9, 0, 12, 0, 10, 18, 0, 0, 5, 2], [19, 17, 7, 9, 6, 10, 9, 6, 3, 25, 20, 3, 12, 13, 19, 17, 8, 9, 18, 15, 9, 6, 10, 25, 39, 10, 12, 13, 7, 3, 17, 27, 6, 9, 3, 9, 0, 16], [0, 0, 0, 5, 2], [6, 3, 9, 0, 17, 7, 9, 7, 10, 9, 3, 15, 6, 7, 3, 9, 7, 3, 14, 8, 9, 6, 10, 9, 6, 7, 3, 16], [28, 23, 23, 18, 6, 7, 3, 17, 12, 13, 7, 10, 9, 3, 10, 2, 2, 11, 9, 6, 7, 11, 10, 9, 3, 9, 6, 0, 10, 18, 2, 23, 23, 9, 3, 9, 6, 3, 16, 22], [19, 5, 7, 12, 13, 6, 3, 9, 10, 12, 13, 9, 6, 7, 3, 19, 25, 16], [9, 7, 7, 10, 15, 3, 10, 25, 8, 8, 18, 0, 24, 13, 7, 12, 13, 9, 6, 0, 7, 3, 3, 16], [39, 9, 11, 11, 10, 9, 0, 23, 25, 9, 3, 9, 6, 7, 3, 3, 16], [20, 8, 3, 17, 23, 8, 20, 3, 23, 18, 22, 22, 19, 17, 16, 22], [19, 24, 13, 6, 7, 3, 9, 6, 10, 8, 12, 13, 6, 3, 9, 6, 3, 9, 20, 7, 10, 9, 3, 18, 14, 7, 18, 7, 7, 10, 18, 3, 3, 18, 3, 3, 3, 15, 3, 9, 7, 10, 16], [0, 1, 0, 2, 3, 3, 3, 0, 0, 0, 0, 0, 11, 11, 3, 18, 0, 0, 4, 17, 6, 3, 9, 7, 0, 18, 8, 11, 10, 1, 11, 16, 41, 0, 4, 3, 9, 0, 16], [6, 3, 23, 23, 17, 6, 3, 9, 7, 10, 31, 0, 17, 8, 9, 10, 9, 10, 9, 7, 10, 9, 0, 15, 0, 15, 17, 6, 3, 9, 7, 3, 12, 13, 6, 9, 6, 3, 14, 0, 16], [7, 3, 18, 6, 7, 3, 5, 8, 12, 13, 6, 23, 21, 8, 10, 9, 3, 9, 7, 10, 14, 9, 6, 0, 0, 16], [6, 10, 9, 6, 0, 0, 25, 20, 3, 9, 0, 22, 0, 7, 3, 15, 20, 3, 9, 6, 0, 10, 9, 20, 3, 9, 3, 18, 9, 7, 9, 0, 16], [28, 28, 28, 19, 5, 9, 19, 5, 7, 12, 13, 6, 7, 3, 9, 9, 19, 24, 13, 20, 3, 16], [28, 19, 17, 6, 7, 3, 9, 6, 10, 20, 10, 15, 10, 9, 7, 3, 9, 6, 3, 12, 13, 19, 7, 16, 22], [0, 0, 16, 0, 23, 18, 6, 3, 3, 3, 9, 0, 0, 18, 0, 0, 18, 5, 28, 19, 5, 7, 12, 13, 10, 9, 6, 7, 10, 19, 25, 9, 6, 10, 16, 22, 6, 3, 5, 6, 14, 3, 9, 6, 11, 10], [19, 25, 6, 3, 9, 6, 0, 9, 0, 17, 8, 6, 7, 3, 23, 9, 0, 16], [8, 9, 6, 3, 9, 7, 10, 9, 11, 18, 19, 17, 11, 10, 9, 0, 18, 0, 18, 6, 0, 0, 0, 15, 0, 16], [19, 24, 13, 12, 13, 10, 12, 13, 6, 7, 3, 9, 0, 21, 0, 9, 0, 16], [19, 25, 23, 7, 9, 14, 23, 9, 0, 9, 0, 15, 6, 3, 7, 0, 3, 23, 23, 9, 23, 12, 13, 7, 10, 9, 3, 15, 3, 16, 22], [28, 0, 0, 23, 5, 6, 3, 9, 0, 18, 29, 38, 25, 8, 10, 9, 6, 7, 10, 18, 15, 5, 6, 3, 9, 6, 7, 3, 9, 0, 0, 16], [0, 0, 0, 0, 0, 5, 2], [0, 17, 7, 9, 23, 11, 16, 11, 11, 7, 10, 9, 3, 14, 6, 0, 0, 18, 0, 9, 0, 18, 0, 0, 0, 18, 0, 9, 0, 15, 6, 0, 0, 16], [9, 0, 18, 0, 0, 0, 24, 13, 10, 9, 6, 0, 22, 33, 25, 12, 13, 7, 10, 9, 6, 0, 21, 11, 0, 0, 16], [7, 10, 5, 12, 13, 3, 10, 9, 3, 16], [17, 14, 6, 7, 0, 3, 25, 6, 10, 3, 16], [6, 3, 28, 3, 3, 3, 17, 11, 18, 11, 18, 11, 3, 1, 11, 18, 11, 3, 41, 3, 4, 15, 7, 9, 6, 17, 8, 9, 3, 16, 1, 11, 4, 1, 11, 4], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [28, 28, 28, 9, 7, 10, 19, 25, 10, 9, 3, 15, 3, 21, 8, 10, 16], [7, 3, 11, 0, 0, 11, 16], [23, 6, 10, 25, 14, 9, 6, 3, 2, 9, 6, 3, 9, 23, 7, 35, 24, 13, 6, 7, 3, 16], [9, 6, 3, 21, 8, 3, 23, 9, 6, 7, 3, 9, 0, 18, 6, 7, 3, 9, 7, 10, 15, 10, 25, 20, 3, 9, 15, 9, 10, 9, 7, 10, 16], [0, 22, 33, 3, 9, 6, 7, 3, 9, 0, 17, 6, 3, 9, 6, 35, 15, 20, 3, 24, 13, 6, 7, 3, 9, 3, 16], [0, 5, 20, 3, 9, 6, 3, 17, 15, 17, 6, 23, 26, 10, 12, 13, 6, 7, 10, 15, 9, 19, 25, 23, 8, 12, 13, 15, 13, 9, 6, 3, 16], [20, 10, 25, 8, 12, 13, 7, 10, 32, 25, 14, 20, 7, 10, 9, 3, 18, 3, 15, 3, 18, 9, 14, 10, 9, 10, 15, 10, 9, 7, 10, 24, 13, 9, 20, 10, 16], [0, 0, 0, 0, 5, 2], [0, 17, 6, 7, 3, 9, 6, 3, 14, 9, 6, 7, 3, 16], [20, 10, 25, 36, 9, 6, 10, 9, 6, 10, 18, 15, 19, 25, 9, 6, 7, 3, 9, 6, 8, 16], [0, 0, 35, 0, 0, 24, 13, 6, 10, 9, 6, 0, 11, 0, 0, 0, 18, 0, 3, 3, 9, 6, 3, 18, 9, 21, 6, 21, 3, 3, 3, 36, 9, 10, 18, 14, 9, 11, 3, 0, 11, 18, 11, 16], [0, 17, 6, 7, 3, 9, 6, 3, 12, 13, 6, 3, 12, 13, 20, 7, 10, 15, 10, 31, 25, 10, 18, 23, 13, 3, 10, 18, 9, 8, 23, 9, 6, 0, 18, 12, 13, 6, 9, 3, 18, 15, 12, 13, 23, 9, 6, 0, 9, 0, 18, 9, 9, 7, 10, 8, 9, 14, 6, 3, 13, 6, 3, 12, 13, 16], [0, 22, 17, 7, 3, 24, 23, 13, 0, 15, 13, 3, 16], [6, 3, 9, 3, 0, 10, 9, 0, 21, 0, 17, 6, 3, 9, 7, 0, 22, 17, 7, 3, 5, 39, 9, 11, 10, 15, 5, 11, 10, 15, 11, 10, 18, 0, 0, 0, 0, 0, 5, 10, 3, 16], [7, 10, 25, 8, 7, 3, 9, 0, 0, 9, 20, 3, 9, 23, 21, 8, 0, 0, 18, 7, 3, 3, 5, 16], [9, 6, 3, 18, 0, 16, 0, 0, 33, 0, 22, 0, 18, 6, 3, 14, 3, 18, 24, 13, 6, 10, 15, 10], [6, 3, 5, 12, 13, 6, 7, 3, 10, 17, 8, 9, 28, 0, 22, 15, 0, 0, 0, 16], [6, 10, 5, 8, 9, 3, 3, 10, 9, 0, 9, 6, 3, 9, 14, 6, 3, 9, 17, 12, 13, 15, 13, 20, 10, 9, 6, 3, 16], [8, 9, 0, 35, 18, 22, 28, 0, 0, 0, 22, 22, 18, 5, 6, 7, 3, 15, 7, 3, 23, 22, 28, 3, 3, 22, 22, 9, 11, 9, 6, 10, 5, 6, 0, 0, 3, 16, 22], [3, 21, 8, 7, 3, 3, 0, 5, 10, 9, 6, 7, 7, 3, 15, 5, 10, 5, 14, 9, 6, 3, 16], [9, 6, 7, 3, 6, 0, 0, 17, 8, 23, 11, 18, 11, 10, 9, 0, 9, 0, 15, 0, 15, 25, 8, 23, 11, 3, 9, 3, 0, 23, 8, 9, 0, 16], [6, 10, 5, 24, 13, 3, 3, 18, 3, 15, 3, 18, 3, 3, 15, 3, 3, 16], [7, 9, 20, 10, 23, 25, 20, 7, 7, 7, 3, 10, 18, 19, 17, 18, 14, 9, 9, 23, 23, 6, 3, 3, 18, 31, 25, 8, 9, 11, 10, 23, 18, 19, 24, 13, 6, 3, 9, 8, 10], [7, 10, 5, 8, 6, 3, 9, 0, 9, 3, 9, 6, 7, 3, 9, 7, 3, 16], [9, 23, 23, 18, 0, 17, 6, 3, 17, 14, 12, 13, 6, 3, 28, 3, 10, 23, 23, 9, 20, 3, 16], [19, 17, 26, 9, 6, 12, 13, 18, 9, 3, 9, 10, 18, 9, 6, 8, 3, 9, 6, 7, 15, 7, 0, 18, 15, 12, 13, 6, 7, 10, 12, 13, 20, 3, 9, 6, 7, 10, 25, 9, 3, 16], [28, 9, 3, 9, 6, 0, 9, 0, 18, 19, 25, 12, 13, 20, 26, 10, 9, 6, 10, 9, 6, 10, 9, 0, 33, 3, 23, 7, 3, 9, 0, 18, 22, 6, 3, 5, 9, 6, 3, 16], [6, 3, 5, 18, 28, 37, 13, 6, 3, 3, 22, 33, 3, 18, 10, 15, 10, 9, 20, 10, 15, 10, 18, 15, 6, 0, 19, 25, 19, 24, 13, 19, 9, 14, 10, 9, 6, 9, 20, 7, 10, 32, 23, 25, 19, 6, 3, 16, 22], [0, 0, 17, 8, 19, 17, 6, 3, 21, 3, 3, 16], [6, 34, 7, 10, 25, 6, 7, 3, 9, 10, 9, 3, 9, 6, 0, 9, 0, 11, 9, 11, 18, 19, 5, 16], [0, 0, 0, 17, 8, 20, 10, 9, 0, 15, 17, 14, 9, 0, 16], [6, 0, 0, 18, 23, 6, 0, 0, 9, 0, 17, 6, 3, 8, 9, 0, 18, 0, 0, 18, 23, 9, 0, 18, 0, 16], [9, 0, 3, 18, 7, 9, 6, 3, 17, 8, 8, 15, 8, 18, 3, 3, 3, 17, 8, 8, 18, 15, 5, 6, 3, 3, 3, 16], [0, 3, 17, 14, 13, 10, 7, 9, 7, 10, 18, 14, 3, 9, 3, 15, 3, 3, 18, 14, 7, 10, 15, 7, 3, 12, 13, 20, 7, 10, 7, 18, 15, 14, 6, 3, 9, 6, 7, 3, 12, 13, 6, 3, 9, 6, 7, 0, 31, 17, 7, 10, 15, 6, 3, 9, 3, 16], [6, 0, 35, 0, 0, 17, 8, 6, 3, 9, 6, 7, 3, 9, 14, 12, 13, 0, 16], [19, 22, 17, 7, 9, 6, 3, 15, 20, 10, 25, 6, 0, 0, 31, 3, 9, 7, 3, 16], [6, 17, 23, 7, 3, 9, 6, 7, 15, 13, 10, 6, 0, 3, 17, 14, 12, 13, 15, 13, 7, 10, 18, 14, 10, 16], [7, 9, 0, 33, 33, 0, 0, 0, 25, 23, 8, 16], [28, 6, 3, 23, 17, 20, 3, 12, 13, 3, 21, 9, 21, 3, 9, 20, 7, 10, 9, 14, 6, 10, 15, 23, 14, 3, 16], [9, 6, 7, 3, 18, 19, 25, 23, 7, 9, 6, 7, 3, 8, 9, 20, 0, 10, 18, 23, 6, 35, 16], [9, 3, 9, 14, 10, 9, 6, 7, 3, 9, 0, 18, 0, 22, 0, 3, 21, 8, 3, 18, 6, 3, 24, 23, 13, 36, 10, 9, 0, 9, 0, 18, 0, 16], [7, 3, 22, 3, 3, 9, 10, 9, 10, 9, 0, 23, 5, 9, 10, 9, 3, 3, 15, 3, 10, 16], [9, 0, 18, 0, 11, 18, 0, 0, 0, 21, 0, 18, 6, 0, 3, 3, 9, 0, 5, 8, 15, 8, 9, 6, 3, 3, 9, 35, 0, 18, 0, 16], [28, 3, 3, 17, 20, 3, 11, 3, 23, 16, 22], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 0, 0, 18, 0, 18, 0, 0, 35, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [6, 7, 3, 17, 8, 19, 17, 14, 9, 6, 3, 16], [32, 24, 13, 29, 20, 10, 24, 13, 23, 30, 15, 32, 0, 24, 13, 9, 23, 16], [15, 7, 10, 9, 6, 3, 24, 13, 36, 7, 0, 18, 31, 17, 23, 9, 0, 3, 18, 9, 7, 10, 14, 12, 13, 16], [10, 23, 25, 12, 13, 3, 9, 10, 16], [6, 0, 9, 0, 17, 8, 6, 3, 9, 9, 12, 45, 11, 11, 9, 3, 14, 9, 6, 7, 3, 9, 6, 3, 15, 40, 15, 3, 9, 3, 15, 10, 9, 18, 9, 18, 9, 3, 9, 18, 15, 12, 13, 0, 18, 23, 8, 9, 20, 7, 3, 9, 0, 16], [0, 0, 17, 0, 22, 33, 3, 9, 10, 9, 0, 15, 17, 6, 0, 22, 17, 3, 9, 14, 6, 7, 3, 16], [19, 5, 23, 23, 8, 9, 10, 15, 3, 3, 16], [6, 0, 18, 6, 39, 0, 0, 15, 6, 0, 9, 0, 25, 8, 9, 14, 6, 10, 15, 25, 23, 8, 6, 3, 3, 3, 9, 7, 3, 10, 9, 9, 11, 3, 9, 7, 3, 18, 14, 0, 16], [28, 28, 28, 19, 5, 12, 13, 35, 9, 11, 7, 10, 1, 45, 11, 16, 11, 2, 45, 11, 16, 11, 4, 6, 3, 16], [19, 25, 8, 9, 3, 9, 7, 7, 10, 12, 13, 9, 6, 0, 9, 0, 17, 6, 7, 7, 15, 3, 3, 9, 6, 10, 16], [19, 23, 5, 7, 18, 7, 15, 3, 3, 9, 7, 10, 12, 13, 6, 3, 9, 0, 16], [9, 0, 18, 0, 0, 0, 24, 13, 9, 7, 15, 7, 10, 12, 13, 7, 10, 9, 6, 7, 7, 10, 15, 12, 13, 6, 3, 9, 7, 7, 3, 3, 10, 14, 9, 0, 9, 0, 15, 0, 16], [6, 0, 0, 5, 2], [6, 0, 23, 5, 3, 11, 7, 0, 18, 14, 19, 9, 6, 3, 21, 8, 3, 3, 0, 0, 9, 0, 16], [6, 10, 9, 6, 0, 0, 0, 5, 9, 6, 7, 3, 9, 10, 9, 6, 0, 0, 0, 16, 1, 11, 4], [6, 0, 0, 15, 0, 0, 5, 0, 21, 0, 3, 3, 3, 9, 3, 3, 3, 18, 3, 18, 3, 15, 7, 3, 16], [28, 14, 9, 19, 5, 7, 2, 6, 35, 25, 23, 7, 9, 20, 10, 16, 22], [6, 10, 25, 6, 3, 9, 6, 0, 9, 0, 18, 0, 0, 0, 0, 11, 18, 15, 10, 12, 13, 6, 7, 3, 9, 6, 3, 18, 23, 23, 9, 20, 7, 10, 12, 13, 7, 3, 9, 7, 3, 10, 16], [9, 3, 9, 11, 3, 10, 9, 6, 7, 11, 10, 18, 6, 0, 17, 14, 9, 11, 10, 31, 25, 11, 11, 35, 9, 6, 10, 9, 3, 18, 3, 3, 18, 3, 18, 3, 18, 3, 18, 15, 13, 3, 9, 10, 9, 7, 3, 3, 9, 0, 16], [15, 18, 19, 25, 19, 18, 9, 6, 7, 3, 31, 17, 23, 5, 3, 9, 11, 3, 10, 9, 23, 11, 10, 18, 0, 33, 3, 19, 25, 39, 3, 9, 6, 3, 10, 12, 13, 20, 3, 14, 9, 3, 23, 9, 10, 16], [6, 7, 7, 7, 7, 7, 0, 0, 5, 0, 0, 35, 0, 0, 22, 3, 1, 0, 4, 0, 0, 1, 0, 4, 11, 9, 0, 11, 16], [0, 25, 7, 10, 12, 13, 10, 18, 10, 18, 10, 18, 10, 18, 3, 18, 15, 10, 10, 16], [6, 0, 17, 6, 17, 17, 31, 17, 24, 13, 10, 9, 7, 10, 16], [39, 9, 11, 18, 11, 35, 25, 8, 9, 10, 9, 0, 16], [19, 25, 29, 9, 6, 7, 3, 5, 20, 10, 9, 6, 7, 3, 18, 15, 19, 23, 5, 19, 16], [19, 17, 30, 8, 7, 10, 12, 13, 8, 9, 7, 10, 31, 25, 8, 9, 0, 21, 8, 18, 9, 19, 25, 3, 21, 7, 3, 9, 0, 10, 16], [38, 17, 3, 6, 3, 16, 22, 22, 22], [6, 0, 3, 9, 0, 11, 5, 11, 0, 21, 0, 10, 31, 5, 8, 6, 7, 0, 21, 0, 3, 9, 0, 18, 0, 18, 0, 5, 16], [9, 3, 9, 14, 3, 9, 7, 3, 9, 0, 17, 7, 7, 10, 18, 6, 3, 3, 17, 7, 7, 15, 7, 10, 8, 9, 14, 6, 3, 9, 3, 15, 3, 16], [9, 6, 3, 9, 7, 10, 18, 23, 23, 9, 19, 28, 17, 7, 12, 13, 9, 6, 14, 3, 18, 3, 15, 10, 33, 10, 23, 18, 17, 6, 7, 3, 8, 3, 16], [3, 10, 31, 25, 8, 10, 9, 0, 25, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 0, 18, 6, 0, 18, 6, 0, 0, 18, 15, 6, 0, 0, 16], [6, 10, 25, 9, 0, 17, 8, 9, 20, 3, 9, 0, 22, 33, 7, 10, 16], [16, 9, 0, 0, 18, 11, 10, 5, 6, 0, 21, 8, 3, 15, 11, 7, 3, 10, 16], [6, 5, 11, 9, 6, 34, 7, 10, 9, 6, 3, 23, 23, 16], [9, 6, 3, 8, 9, 15, 0, 15, 23, 9, 0, 18, 6, 0, 0, 0, 0, 17, 12, 13, 6, 3, 9, 7, 10, 9, 0, 9, 0, 10, 18, 14, 6, 7, 3, 9, 3, 9, 6, 0, 0, 9, 11, 16], [15, 9, 8, 23, 18, 6, 3, 24, 13, 6, 3, 9, 7, 15, 13, 7, 3, 9, 6, 9, 3, 16, 15, 18, 26, 9, 6, 18, 9, 7, 19, 24, 13, 6, 3, 9, 6, 3, 9, 7, 7, 7, 10, 9, 0, 10, 18, 9, 6, 3, 8, 9, 6, 0, 0, 0, 16, 9, 6, 3, 18, 19, 24, 13, 6, 7, 3, 8, 9, 7, 7, 10, 18, 7, 9, 6, 8, 3, 9, 0, 18, 9, 23, 7, 15, 8, 9, 14, 6, 7, 15, 7, 3, 9, 7, 7, 7, 10, 9, 0, 9, 0, 10, 16, 9, 6, 3, 18, 19, 25, 8, 3, 9, 7, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 3, 15, 9, 6, 0, 0, 0, 0, 0, 0, 18, 12, 13, 20, 3, 9, 0, 16, 6, 0, 0, 17, 14, 20, 3, 9, 6, 7, 10, 16], [7, 7, 10, 25, 19, 23, 25, 6, 7, 3, 9, 6, 3, 33, 3, 0, 3, 16], [9, 3, 9, 20, 7, 3, 12, 13, 6, 7, 3, 18, 6, 3, 24, 13, 7, 15, 7, 10, 18, 7, 3, 10, 18, 3, 7, 10, 18, 15, 10, 12, 13, 6, 10, 9, 6, 7, 10, 16], [9, 6, 7, 3, 39, 9, 11, 18, 11, 10, 2, 7, 9, 19, 10, 18, 25, 8, 8, 16], [0, 0, 18, 6, 0, 0, 9, 0, 0, 9, 0, 10, 9, 6, 0, 0, 15, 0, 15, 0, 0, 18, 24, 13, 9, 0, 0, 11, 40, 11, 15, 9, 0, 0, 11, 40, 11, 16], [19, 17, 7, 9, 6, 10, 12, 13, 23, 9, 6, 14, 10, 15, 10, 12, 13, 3, 9, 3, 9, 14, 0, 15, 12, 13, 9, 6, 14, 3, 9, 6, 7, 10, 16], [9, 6, 3, 9, 6, 3, 12, 13, 0, 18, 6, 0, 24, 23, 13, 9, 10, 15, 10, 9, 19, 25, 9, 6, 3, 22, 17, 7, 3, 15, 3, 9, 0, 17, 8, 16], [23, 19, 25, 6, 7, 3, 16, 22, 22, 22], [28, 6, 0, 0, 5, 6, 7, 0, 0, 18, 0, 0, 21, 0, 18, 12, 13, 6, 3, 9, 10, 14, 6, 7, 3, 15, 6, 3, 14, 8, 9, 0, 9, 0, 16], [23, 11, 18, 11, 7, 3, 3, 10, 18, 7, 10, 18, 0, 7, 10, 15, 0, 10, 18, 8, 9, 0, 21, 8, 3, 3, 10, 18, 5, 6, 23, 21, 8, 3, 12, 13, 0, 11, 10, 23, 16], [0], [19, 23, 23, 25, 6, 3, 9, 11, 0, 9, 6, 0, 0, 9, 0, 18, 8, 9, 0, 0, 0, 0, 0, 18, 15, 6, 0, 0, 0, 18, 8, 9, 0, 0, 0, 0, 18, 9, 7, 10, 9, 3, 10, 15, 3, 3, 16], [19, 25, 3, 22, 33, 3, 9, 7, 3, 10, 5, 6, 7, 3, 9, 0, 9, 0, 3, 18, 8, 9, 7, 3, 9, 20, 3, 0, 15, 20, 7, 3, 16], [23, 9, 0, 18, 0, 7, 10, 5, 11, 10, 8, 9, 15, 9, 3, 9, 6, 3, 9, 0, 14, 3, 18, 3, 18, 3, 18, 15, 23, 8, 3, 23, 23, 9, 3, 3, 9, 0, 10, 16], [6, 3, 9, 7, 10, 17, 7, 18, 15, 6, 3, 9, 7, 3, 15, 0, 0, 0, 10, 16], [6, 3, 3, 9, 6, 39, 0, 10, 18, 1, 11, 4, 6, 0, 10, 25, 9, 6, 0, 0, 14, 0, 18, 23, 9, 6, 0, 3, 18, 14, 7, 0, 3, 0, 0, 15, 7, 0, 0, 16], [23, 3, 18, 0, 5, 9, 3, 22, 3, 7, 10, 9, 0, 18, 0, 16], [9, 0, 18, 7, 3, 10, 5, 6, 7, 3, 3, 5, 8, 6, 3, 9, 0, 18, 14, 10, 15, 6, 10, 16], [14, 0, 3, 9, 6, 7, 3, 17, 6, 3, 9, 0, 21, 0, 3, 9, 0, 9, 0, 11, 16], [7, 7, 3, 9, 0, 15, 0, 9, 0, 3], [0, 0, 18, 14, 3, 9, 6, 0, 0, 18, 5, 9, 0, 12, 13, 10, 15, 7, 10, 16], [23, 8, 9, 6, 7, 3, 9, 7, 7, 7, 10, 18, 0, 21, 0, 18, 5, 19, 24, 13, 36, 9, 0, 9, 6, 28, 28, 3, 3, 22, 22, 16], [6, 24, 13, 6, 26, 3, 31, 0, 22, 0, 0, 35, 25, 23, 8, 2, 19, 25, 9, 9, 6, 3, 16], [19, 5, 9, 6, 3, 16], [0, 16], [6, 0, 0, 5, 11, 10, 9, 3, 21, 14, 7, 3, 16], [19, 23, 17, 6, 3, 9, 0, 10, 12, 13, 7, 3, 10, 9, 20, 3, 12, 13, 6, 7, 3, 16], [9, 0, 11, 0, 6, 0, 0, 0, 5, 0, 11, 16], [19, 24, 13, 12, 13, 20, 10, 7, 9, 6, 3, 9, 0, 16], [9, 6, 7, 11, 10, 7, 3, 10, 15, 20, 0, 10, 25, 8, 9, 27, 6, 3, 9, 6, 3, 15, 23, 6, 7, 7, 10, 16], [19, 17, 6, 3, 3, 3, 15, 6, 7, 3, 16], [6, 3, 9, 6, 0, 0, 17, 6, 34, 7, 7, 3, 9, 6, 0, 9, 0, 3, 2, 6, 3, 9, 11, 9, 6, 10, 9, 0, 15, 6, 0, 0, 15, 0, 12, 13, 7, 10, 12, 13, 18, 3, 18, 15, 13, 6, 7, 3, 8, 23, 9, 0, 16], [0, 0, 18, 0, 9, 6, 0, 0, 17, 6, 3, 9, 0, 15, 17, 25, 9, 6, 0, 9, 0, 9, 6, 3, 9, 0], [9, 6, 3, 18, 19, 25, 6, 7, 10, 9, 0, 24, 13, 23, 9, 6, 3, 9, 15, 9, 3, 9, 6, 0, 9, 0, 18, 9, 6, 3, 9, 6, 0, 16], [9, 6, 17, 6, 7, 3, 18, 19, 17, 6, 7, 15, 7, 7, 3, 31, 17, 9, 6, 10, 25, 7, 15, 7, 12, 13, 23, 15, 9, 7, 3, 16], [6, 3, 9, 0, 0, 18, 8, 36, 9, 6, 3, 9, 10, 3, 15, 3, 3, 10, 17, 11, 9, 6, 26, 10, 12, 13, 9, 0, 6, 3, 12, 13, 7, 10, 9, 6, 0, 0, 9, 0, 15, 6, 0, 16], [19, 17, 19, 5, 19, 12, 13, 10, 30, 7, 9, 19, 18, 10, 32, 19, 24, 13, 10, 9, 15, 13, 10, 9, 16], [7, 3, 18, 19, 23, 5, 9, 19, 24, 13, 20, 7, 3, 9, 0, 9, 45, 11, 11, 18, 14, 20, 7, 3, 9, 45, 11, 16, 11, 11, 18, 14, 10, 9, 0, 32, 25, 8, 8, 12, 13, 9, 0, 22, 33, 3, 16], [6, 10, 9, 6, 0, 0, 25, 6, 0, 0, 21, 0, 18, 0, 0, 21, 0, 18, 12, 13, 23, 12, 13, 27, 6, 0, 23, 23, 9, 7, 15, 9, 6, 7, 3, 21, 3, 16], [6, 3, 9, 0, 0, 17, 6, 7, 3, 6, 7, 3, 17, 8, 9, 6, 3, 3, 9, 14, 8, 9, 6, 0, 0, 16], [28, 28, 28, 0, 22, 33, 3, 9, 3, 17, 23, 7, 16], [6, 0, 9, 6, 0, 0, 5, 8, 9, 11, 9, 20, 7, 10, 18, 9, 9, 0, 21, 0, 9, 0, 18, 0, 35, 35, 18, 0, 0, 21, 0, 0, 18, 0, 21, 0, 35, 18, 15, 0, 21, 0, 35, 16], [6, 0, 17, 20, 3, 12, 13, 6, 3, 15, 10, 9, 0, 9, 6, 3, 18, 15, 17, 7, 9, 20, 3, 9, 6, 0, 0, 10, 12, 13, 15, 13, 0, 22, 3, 16], [28, 3, 11, 17, 6, 3, 9, 10, 18, 15, 17, 6, 3, 14, 6, 28, 28, 3, 22, 22, 9, 7, 10, 23, 7, 9, 6, 9, 6, 35, 16, 22], [0, 0, 5, 2], [9, 6, 7, 3, 5, 18, 6, 0, 0, 17, 8, 9, 9, 45, 11, 11, 9, 7, 3, 16], [9, 23, 0, 15, 7, 10, 18, 7, 9, 19, 23, 7, 10, 46, 10, 5, 8, 9, 6, 3, 9, 6, 3, 18, 25, 8, 39, 9, 11, 18, 11, 10, 9, 23, 23, 6, 7, 7, 16], [19, 5, 39, 10, 15, 19, 5, 12, 13, 39, 3, 16], [6, 0, 24, 13, 8, 12, 13, 14, 3, 9, 9, 11, 3, 0, 15, 7, 10, 9, 7, 10, 10, 15, 7, 3, 10, 24, 13, 8, 12, 13, 16], [9, 3, 18, 23, 11, 7, 15, 7, 10, 25, 8, 23, 12, 13, 0, 22, 33, 3, 9, 0, 15, 0, 28, 0, 17, 12, 13, 6, 3, 9, 6, 7, 3, 18, 15, 6, 35, 15, 35, 9, 6, 7, 0, 18, 23, 9, 20, 0, 15, 0, 10, 18, 25, 14, 23, 12, 13, 15, 13, 0, 22, 3, 18, 22, 10, 5, 16], [16, 9, 0, 0, 18, 11, 3, 5, 6, 0, 3, 3, 16], [29, 6, 35, 5, 19, 8, 10, 16], [3], [13, 9, 6, 3, 18, 19, 25, 7, 9, 14, 6, 7, 3, 9, 0, 31, 17, 6, 10, 9, 10, 9, 6, 10, 9, 6, 0, 15, 0, 16], [19, 25, 6, 0, 9, 0, 22, 0, 3, 18, 31, 5, 9, 6, 3, 3, 5, 8, 9, 6, 3, 18, 14, 19, 7, 9, 6, 3, 5, 8, 36, 9, 10, 9, 15, 6, 0, 3, 15, 0, 16], [10, 5, 6, 0, 3, 15, 3, 9, 0], [6, 17, 29, 6, 0, 17, 14, 9, 3, 9, 6, 0, 0, 12, 13, 15, 13, 0, 9, 0, 15, 0, 15, 18, 29, 7, 18, 14, 9, 6, 32, 25, 6, 3, 9, 6, 0, 18, 15, 20, 10, 15, 10, 16], [6, 0, 17, 8, 6, 10, 9, 3, 9, 16], [28, 28, 28, 9, 6, 3, 6, 3, 5, 19, 23, 20, 3, 16], [15, 6, 7, 3, 19, 5, 9, 6, 3, 10, 23, 23, 17, 19, 17, 12, 13, 16], [6, 7, 10, 9, 0, 25, 8, 6, 3, 9, 6, 7, 3, 9, 20, 3, 12, 13, 0, 18, 46, 10, 9, 3, 9, 0, 15, 6, 0, 0, 25, 12, 13, 20, 7, 10, 15, 3, 9, 3, 16], [19, 25, 19, 9, 6, 3, 12, 13, 13, 19, 14, 8, 9, 10, 15, 10, 9, 23, 7, 7, 10, 23, 18, 22, 22, 17, 0, 0, 18, 6, 7, 7, 3, 3, 32, 5, 11, 9, 6, 10, 16, 22], [6, 0, 9, 0, 17, 11, 10, 9, 3, 9, 14, 3, 16], [12, 13, 7, 19, 5, 23, 7, 9, 0, 9, 19, 5, 14, 19, 9, 3, 16], [6, 3, 9, 0, 10, 9, 0, 15, 0, 23, 17, 6, 3, 28, 33, 3, 12, 13, 3, 15, 13, 10, 16], [19, 24, 13, 12, 13, 27, 6, 8, 9, 6, 7, 10, 9, 3, 9, 6, 11, 3, 3, 8, 12, 13, 0, 22, 33, 7, 3, 16], [0, 0, 22, 17, 5, 3, 3, 18, 15, 0, 0, 0, 0, 0, 18, 0, 0, 3, 24, 13], [3, 17, 8, 12, 13, 8, 9, 11, 16], [0, 17, 6, 3, 9, 6, 35, 15, 19, 17, 7, 9, 0, 22, 33, 10, 25, 9, 6, 3, 9, 6, 7, 3, 16], [19, 25, 8, 9, 6, 26, 7, 3, 9, 0, 22, 3, 3, 18, 23, 9, 6, 3, 9, 3, 3, 5, 14, 7, 3, 5, 0, 3, 10, 9, 9, 10, 16], [23, 23, 18, 19, 5, 12, 13, 6, 0, 21, 8, 3, 16], [6, 3, 9, 6, 0, 0, 0, 9, 0, 35, 18, 0, 0, 18, 5, 6, 3, 3, 9, 0, 9, 0, 9, 19, 5, 14, 12, 13, 10, 9, 10, 9, 10, 15, 10, 18, 23, 23, 9, 7, 10, 16], [19, 23, 5, 12, 13, 23, 9, 7, 10, 15, 9, 6, 0, 9, 6, 7, 3, 12, 13, 0, 9, 3, 16, 22], [28, 0, 17, 8, 9, 14, 23, 9, 6, 0, 15, 0, 0, 18, 22, 5, 0, 16, 0, 0, 18, 3, 9, 0], [28, 28, 28, 19, 22, 24, 13, 14, 9, 19, 18, 14, 6, 3, 14, 8, 15, 19, 22, 3, 3, 18, 28, 29, 25, 19, 14, 6, 3, 16, 22, 23, 19, 25, 9, 3, 17, 8, 9, 9, 20, 10, 15, 19, 25, 19, 18, 28, 29, 5, 19, 13, 18, 32, 5, 19, 13, 16, 22, 19, 25, 22, 3, 25, 16, 22, 22, 22], [23, 9, 11, 0, 6, 3, 10, 5, 12, 13, 6, 3, 9, 0, 15, 13, 6, 10, 32, 5, 0, 16], [28, 0, 0, 21, 0, 22, 0, 0, 21, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 3, 11, 0, 6, 0, 21, 0, 0, 22, 0, 0, 21, 0, 0, 3, 3, 18, 22, 28, 0, 9, 6, 0, 0, 22, 22, 4, 17, 6, 7, 7, 3, 8, 12, 13, 3, 9, 6, 7, 3, 16], [28, 6, 10, 5, 6, 3, 9, 0, 15, 6, 3, 9, 3, 9, 7, 10, 18, 14, 6, 7, 7, 3, 9, 6, 3, 3, 16], [9, 6, 3, 9, 0, 22, 33, 0, 0, 0, 18, 0, 17, 8, 6, 3, 14, 6, 3, 9, 6, 3, 16], [19, 25, 8, 6, 3, 9, 7, 10, 9, 7, 0, 14, 0, 15, 0, 18, 5, 6, 3, 9, 6, 0, 9, 0, 0, 18, 15, 5, 0, 9, 9, 0, 16], [23, 11, 18, 11, 7, 10, 18, 14, 10, 9, 7, 10, 15, 20, 3, 18, 3, 15, 3, 10, 18, 24, 13, 6, 3, 16], [6, 17, 23, 6, 7, 3, 12, 13, 6, 7, 3, 9, 0, 9, 0, 16], [28, 28, 28, 38, 22, 33, 7, 3, 23, 9, 6, 32, 25, 18, 22, 28, 6, 3, 17, 16], [6, 0, 10, 9, 6, 0, 9, 0, 0, 15, 17, 6, 7, 10, 9, 6, 3, 15, 3, 9, 10, 12, 13, 6, 3, 9, 3, 9, 10, 10, 10, 16], [9, 7, 7, 18, 6, 10, 24, 13, 13, 6, 3, 9, 11, 18, 11, 10, 5, 9, 6, 3, 16], [14, 6, 3, 18, 6, 0, 0, 0, 0, 5, 2], [28, 28, 28, 23, 19, 25, 6, 3, 17, 6, 7, 3, 9, 27, 20, 10, 16], [9, 3, 9, 6, 11, 10, 18, 11, 3, 10, 18, 11, 0, 21, 8, 10, 18, 11, 7, 10, 18, 15, 6, 3, 3, 3, 3, 5, 23, 8, 9, 6, 3, 16], [23, 20, 3, 17, 6, 3, 12, 13, 0, 6, 7, 3, 7, 3, 9, 0, 16], [15, 19, 17, 23, 28, 3, 23, 7, 3, 9, 10, 31, 25, 30, 7, 9, 6, 0, 16], [3, 2, 11, 0, 0, 2, 0, 0, 3, 2, 11, 16, 11, 10, 40, 3, 0, 2, 11, 40, 11, 0, 11, 40, 11, 0, 0, 0, 2, 11, 0, 2, 11, 16, 11, 10, 3, 2, 0, 0, 0, 2, 11, 16, 11, 0, 0, 0, 2, 44, 40, 11], [28, 28, 28, 19, 25, 6, 3, 18, 22, 22, 17, 0, 16], [9, 6, 3, 9, 6, 3, 9, 0, 24, 13, 6, 3, 9, 6, 7, 3, 19, 24, 23, 13, 6, 10, 9, 6, 3, 16], [9, 0, 0, 9, 0, 0, 0, 5, 3, 9, 6, 0, 0, 18, 6, 0, 0, 24, 13, 23, 45, 11, 11, 9, 7, 3, 9, 6, 7, 3, 18, 14, 6, 7, 3, 19, 25, 8, 15, 25, 14, 9, 6, 7, 3, 9, 11, 9, 23, 45, 11, 11, 16], [19, 28, 9, 6, 11, 10, 9, 0, 19, 25, 23, 19, 25, 16], [6, 3, 9, 0, 0, 0, 0, 5, 9, 3, 9, 0, 23, 23, 9, 7, 3, 15, 3, 16], [6, 3, 9, 20, 34, 7, 10, 25, 9, 23, 21, 8, 7, 10, 7, 9, 6, 3, 15, 3, 0, 0, 18, 8, 9, 6, 0, 9, 35, 18, 15, 7, 3, 0, 21, 0, 18, 32, 5, 10, 9, 6, 7, 3, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [9, 20, 3, 9, 0, 18, 6, 0, 17, 23, 8, 10, 9, 10, 15, 39, 9, 11, 7, 10, 9, 0, 21, 8, 3, 16], [0, 0, 0, 11, 24, 13, 9, 6, 3, 21, 7, 3, 9, 7, 10, 12, 13, 6, 7, 7, 7, 3, 9, 7, 15, 7, 10, 16], [6, 0, 17, 6, 0, 9, 3, 18, 13, 15, 13, 10, 18, 10, 15, 3, 3, 9, 6, 0, 16], [6, 3, 17, 15, 6, 3, 9, 3, 3, 8, 15, 6, 9, 0, 22, 7, 7, 10, 16], [29, 6, 3, 17, 8, 8, 9, 10, 15, 3, 10, 6, 10, 18, 19, 24, 13, 8, 10, 24, 13, 7, 3, 9, 10, 16], [19, 24, 13, 9, 23, 9, 6, 3, 17, 3, 18, 6, 3, 17, 10, 16, 22, 22, 22], [6, 3, 9, 6, 3, 17, 7, 9, 0, 15, 7, 10, 25, 19, 24, 13, 8, 9, 0, 33, 3, 10, 9, 19, 5, 29, 19, 5, 16], [23, 11, 35, 25, 9, 0, 21, 0, 0, 16], [9, 19, 5, 9, 0, 0, 0, 21, 0, 3, 18, 19, 25, 0, 0, 22, 33, 3, 9, 14, 6, 7, 15, 7, 3, 9, 11, 10, 18, 32, 5, 3, 9, 6, 3, 9, 20, 14, 3, 12, 13, 19, 9, 6, 7, 3, 16], [0, 5, 23, 8, 9, 6, 3, 9, 0, 0, 9, 7, 0, 18, 14, 9, 6, 7, 0, 0, 0, 21, 0, 0, 0, 3, 3, 16], [19, 17, 8, 9, 6, 7, 3, 9, 6, 0, 9, 0, 17, 8, 9, 6, 0, 3, 9, 0, 33, 0, 15, 6, 7, 3, 9, 6, 0, 0, 0, 17, 8, 9, 6, 3, 9, 0, 33, 11, 9, 21, 3, 16, 1, 11, 4], [12, 13, 2, 0, 10, 25, 14, 15, 14, 7, 0, 35, 1, 0, 4, 18, 23, 7, 7, 10, 14, 9, 6, 7, 3, 9, 6, 3, 32, 25, 14, 10, 9, 6, 3, 12, 13, 0, 16], [28, 28, 28, 19, 23, 23, 25, 9, 6, 24, 23, 13, 6, 7, 7, 10, 19, 25, 9, 7, 7, 10, 9, 1, 0, 4, 18, 15, 13, 20, 3, 9, 3, 10, 15, 20, 10, 12, 13, 20, 10, 25, 23, 13, 3, 9, 6, 9, 6, 10, 32, 25, 9, 10, 9, 0, 3, 18, 15, 13, 6, 8, 17, 10, 19, 25, 9, 3, 9, 7, 7, 3, 18, 22, 22, 5, 0, 0, 16, 22], [19, 24, 23, 13, 0, 21, 0, 12, 13, 20, 3, 12, 13, 6, 0, 0, 15, 20, 10, 9, 6, 3, 18, 15, 6, 10, 25, 6, 3, 16, 22], [3, 17, 9, 21, 6, 21, 3, 9, 0, 0, 0, 16], [6, 0, 0, 23, 17, 0, 28, 7, 7, 3, 10, 9, 6, 3, 9, 0, 0, 9, 0, 0, 18, 31, 5, 15, 5, 10, 9, 7, 10, 18, 14, 9, 6, 3, 15, 3, 16], [28, 28, 28, 19, 25, 22, 3, 25, 3, 15, 19, 5, 6, 20, 10, 9, 3, 3, 9, 20, 3, 18, 22, 28, 19, 17, 36, 16], [9, 0, 18, 0, 7, 10, 5, 11, 10, 14, 3, 18, 3, 18, 3, 15, 23, 5, 3, 9, 0, 10, 16], [6, 0, 0, 15, 0, 0, 5, 9, 11, 0, 9, 6, 11, 0, 21, 0, 0, 9, 35, 9, 0, 16], [20, 3, 17, 19, 9, 14, 6, 7, 18, 7, 21, 7, 3, 16], [0, 15, 0, 5, 23, 6, 3, 9, 6, 7, 3, 16], [19, 17, 3, 23, 6, 3, 9, 0, 16], [6, 0, 0, 17, 8, 9, 6, 14, 9, 6, 7, 3, 9, 10, 9, 0, 23, 9, 6, 7, 3, 5, 36, 9, 0, 16], [7, 7, 10, 32, 25, 8, 8, 9, 10, 9, 0, 25, 14, 12, 13, 6, 0, 22, 19, 3, 9, 0, 18, 0, 5, 16], [16, 9, 0, 0, 18, 11, 10, 5, 11, 0, 7, 10, 15, 5, 6, 7, 21, 3, 3, 18, 6, 0, 18, 6, 3, 3, 18, 15, 6, 3, 3, 16], [29, 8, 18, 25, 0, 11, 16], [6, 0, 17, 9, 6, 3, 9, 7, 10, 12, 13, 7, 3, 14, 8, 9, 45, 11, 16, 3, 12, 7, 15, 7, 10, 9, 0, 15, 9, 10, 9, 3, 10, 16], [27, 6, 3, 24, 13, 6, 10, 12, 13, 9, 6, 8, 3, 8, 9, 0, 22, 3, 15, 7, 10, 16], [23, 9, 0, 18, 6, 7, 7, 3, 5, 6, 0, 21, 8, 3, 9, 14, 39, 9, 11, 10, 15, 14, 11, 10, 9, 3, 10, 9, 15, 9, 0, 9, 6, 7, 11, 10, 16], [3, 18, 7, 10, 18, 23, 8, 9, 0, 22, 0, 18, 25, 8, 9, 26, 11, 30, 7, 10, 15, 7, 10, 9, 10, 16], [6, 0, 0, 3, 5, 2, 28, 6, 0, 0, 5, 7, 0, 0, 0, 0, 30, 3, 9, 6, 10, 9, 6, 0, 3, 9, 0, 0, 18, 20, 7, 3, 21, 9, 21, 3, 3, 9, 0, 0, 5, 8, 16], [19, 25, 10, 16], [9, 0, 18, 0, 22, 0, 5, 7, 10, 9, 6, 7, 3, 9, 6, 0, 21, 0, 3, 9, 0, 31, 5, 23, 34, 11, 10, 16], [9, 11, 10, 9, 7, 3, 18, 6, 10, 9, 0, 24, 13, 12, 13, 3, 9, 6, 39, 3, 16], [28, 28, 19, 25, 6, 3, 3, 15, 19, 25, 22, 3, 3, 15, 6, 3, 19, 25, 22, 3, 3, 18, 19, 25, 22, 3, 3, 16, 22, 22, 22], [28, 20, 3, 24, 13, 20, 23, 7, 10, 15, 13, 6, 3, 9, 3, 8, 9, 3, 16, 22], [10, 2, 11, 3, 11, 40, 11, 3, 11, 11, 10, 11, 40, 11, 3, 10, 40, 11, 16, 11, 18, 0, 11, 16, 11, 0, 0, 40, 11, 16, 11, 2, 11, 16, 11], [0, 22, 33, 7, 3, 17, 8, 36, 23, 16], [9, 6, 7, 18, 19, 5, 6, 3, 9, 6, 0, 3, 9, 0, 18, 15, 5, 8, 3, 9, 0, 35, 16], [0, 0, 5, 6, 7, 0, 0, 18, 0, 0, 21, 0, 9, 0, 3, 16], [19, 5, 6, 3, 9, 6, 3, 9, 14, 9, 6, 3, 16], [14, 3, 18, 0, 0, 5, 2], [6, 10, 9, 6, 0, 0, 25, 6, 7, 10, 15, 20, 10, 12, 13, 7, 15, 13, 6, 7, 3, 8, 9, 6, 0, 16], [6, 0, 25, 8, 23, 11, 7, 10, 9, 6, 3, 9, 0, 5, 23, 7, 3, 18, 0, 5, 16], [28, 28, 6, 10, 0, 33, 3, 23, 15, 6, 0, 0, 0, 28, 3, 14, 23, 16], [6, 3, 46, 3, 5, 8, 5, 6, 3, 3, 31, 19, 5, 8, 6, 3, 14, 9, 20, 3, 18, 11, 11, 1, 11, 3, 4, 9, 6, 3, 18, 23, 9, 6], [6, 3, 18, 9, 8, 9, 6, 0, 10, 18, 17, 11, 15, 39, 7, 10, 31, 25, 9, 23, 6, 7, 7, 3, 12, 13, 6, 7, 18, 23, 7, 3, 9, 6, 3, 16], [20, 3, 17, 23, 7, 9, 23, 2, 19, 24, 13, 12, 13, 9, 6, 7, 3, 9, 6, 3, 9, 31, 0, 17, 6, 3, 16], [9, 14, 7, 3, 9, 7, 7, 7, 10, 18, 6, 3, 17, 14, 6, 3, 9, 3, 15, 3, 23, 6, 7, 7, 3, 17, 23, 9, 3, 16], [5, 38, 6, 10, 16], [19, 17, 7, 9, 6, 8, 3, 18, 31, 5, 10, 18, 17, 8, 6, 10, 16], [6, 3, 17, 11, 10, 18, 11, 9, 31, 25, 9, 7, 3, 18, 15, 17, 6, 7, 3, 3, 9, 7, 10, 16, 1, 11, 4], [0, 0, 3, 9, 0, 3, 10, 3], [28, 19, 22, 13, 7, 9, 9, 6, 3, 3, 12, 13, 6, 3, 9, 10, 2, 23, 23, 10, 2, 19, 22, 9, 14, 6, 3, 9, 6, 35, 15, 9, 6, 0, 12, 13, 7, 19, 22, 13, 14, 6, 3, 9, 6, 3, 16, 22], [6, 3, 17, 23, 6, 3, 9, 0, 22, 33, 3, 15, 3, 9, 7, 3, 15, 20, 3, 12, 13, 7, 3, 9, 6, 10, 9, 0, 16], [6, 14, 10, 9, 6, 3, 25, 9, 7, 3, 9, 3, 12, 13, 7, 3, 15, 12, 13, 7, 21, 3, 10, 31, 25, 3, 9, 6, 3, 15, 31, 25, 23, 23, 7, 16], [19, 25, 12, 13, 23, 7, 9, 10, 23, 25, 19, 9, 20, 10, 18, 14, 6, 7, 3, 19, 22, 24, 13, 16, 22, 22, 22], [6, 5, 3, 9, 0, 22, 0, 24, 13, 8, 9, 35, 14, 6, 8, 7, 3, 23, 9, 6, 0, 3, 46, 10, 25, 8, 0, 22, 13, 25, 3, 16], [3, 9, 6, 0, 0, 9, 0, 18, 6, 0, 0, 0, 3, 22, 35, 18, 0, 0, 18, 0, 0, 15, 0, 0, 0, 0, 18, 0, 0, 0, 9, 6, 0, 9, 35, 9, 6, 0, 0, 9, 0, 18, 0, 0, 9, 35, 15, 0, 1, 0, 4, 5, 6, 3, 9, 6, 3, 9, 6, 3, 15, 3, 9, 6, 0, 0, 9, 6, 10, 9, 6, 7, 3, 9, 0, 9, 0], [6, 3, 17, 8, 12, 13, 11, 10, 16], [0, 10, 18, 0, 5, 19, 16], [0, 5, 8, 9, 6, 11, 3, 9, 0, 0, 35, 9, 7, 3, 16, 1, 11, 4], [9, 6, 3, 18, 11, 0, 3, 5, 11, 10, 14, 11, 10, 16], [0, 0, 5, 6, 7, 3, 23, 16], [0, 0, 5, 2, 6, 0, 17, 8, 23, 9, 7, 3, 10, 12, 13, 11, 7, 0, 10, 10, 9, 0, 16], [6, 0, 0, 0, 5, 23, 12, 13, 0, 11, 18, 31, 5, 6, 11, 0, 10, 9, 0, 15, 10, 9, 6, 0, 0, 18, 14, 6, 9, 0, 9, 11, 0, 9, 31, 11, 10, 5, 8, 16], [19, 24, 13, 14, 3, 9, 6, 3, 9, 19, 17, 20, 3, 18, 17, 6, 3, 15, 10, 9, 7, 10, 9, 6, 7, 3, 16], [3, 11, 17, 6, 3, 9, 0, 16], [6, 3, 5, 9, 3, 23, 16], [0, 0, 9, 0, 0, 0, 24, 13, 9, 0, 12, 13, 7, 7, 18, 7, 18, 15, 3, 10, 9, 3, 10, 16], [9, 14, 6, 3, 18, 6, 0, 5, 36, 7, 10, 15, 11, 10, 6, 7, 3, 9, 14, 6, 7, 3, 18, 9, 7, 3, 3, 18, 12, 13, 20, 3, 16], [6, 10, 5, 9, 3, 10, 9, 7, 3, 9, 7, 3, 9, 0, 16], [0, 0, 10, 25, 9, 0, 10, 9, 7, 3, 3, 17, 23], [19, 23, 5, 19, 3, 18, 22, 22, 17, 6, 10, 33, 3, 18, 0, 0, 16, 22], [0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 4, 15, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 3, 4, 17, 6, 7, 7, 7, 3, 9, 6, 0, 0, 7, 1, 0, 4, 9, 0, 15, 23, 9, 0, 18, 0, 16], [14, 9, 6, 3, 9, 3, 10, 9, 0, 18, 6, 0, 0, 0, 0, 0, 5, 6, 3, 5, 14, 9, 8, 16], [19, 23, 5, 9, 38, 5, 6, 3, 9, 0, 0, 10, 9, 0, 15, 9, 6, 3, 5, 9, 14, 3, 9, 6, 26, 23, 21, 8, 3, 18, 23, 11, 10, 23], [16, 9, 0, 0, 18, 11, 3, 5, 6, 0, 7, 3, 15, 5, 6, 3, 3, 16], [19, 5, 29, 12, 13, 19, 2, 9, 0, 33, 33, 7, 0, 3, 18, 6, 3, 29, 6, 9, 0, 5, 18, 10, 9, 6, 3, 18, 15, 6, 3, 16], [23, 18, 0, 21, 0, 17, 23, 13, 6, 3, 15, 3, 9, 3, 8, 9, 6, 3, 18, 6, 3, 9, 10, 5, 9, 6, 3, 18, 15, 6, 3, 9, 7, 7, 3, 10, 9, 6, 3, 16], [6, 0, 3, 5, 8, 18, 14, 15, 14, 9, 3, 29, 19, 5, 8, 36, 0, 0, 9, 14, 9, 20, 10, 16], [6, 7, 3, 9, 6, 0, 17, 7, 9, 6, 3, 15, 24, 13, 13, 7, 3, 9, 0, 10, 16], [19, 25, 10, 9, 0, 33, 7, 10, 9, 6, 7, 3, 1, 3, 5, 4, 15, 23, 5, 3, 9, 6, 3, 9, 32, 19, 5, 5, 0, 15, 0, 0, 10, 9, 6, 0, 3, 9, 0, 14, 3, 3, 10, 15, 7, 3, 10, 16], [28, 28, 28, 19, 5, 19, 6, 3, 16], [3, 9, 6, 0, 0, 23, 17, 6, 10, 39, 3, 15, 39, 3, 12, 13, 3, 16], [9, 0, 11, 18, 11, 18, 19, 5, 8, 9, 0, 0, 0, 0, 9, 6, 7, 3, 9, 6, 0, 0, 0, 18, 31, 17, 6, 7, 3, 10, 9, 0, 15, 17, 8, 9, 14, 6, 3, 16], [19, 5, 6, 3, 3, 9, 6, 0, 9, 6, 11, 0, 0, 16]]\n",
            "[[0, 2, 2, 2, 2, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 7, 7, 7, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [7, 7, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 7, 7, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 0, 0, 6, 0, 0, 2, 2, 0], [3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 7, 7, 0, 0, 0, 0, 6, 0, 0, 0, 0, 6, 0, 0, 6, 0, 0, 6, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0], [2, 2, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 14, 14, 14, 0, 0, 6], [0, 0, 0, 0, 4, 4, 0, 6, 0, 3, 0, 0, 0, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 4, 0, 0, 8, 0, 0, 8, 0], [0, 0, 0, 0, 7, 0, 0, 0, 6, 0, 0, 0, 0, 0], [0, 6, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 4, 4, 4, 4, 0, 0, 4, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 8, 8, 8, 0, 14, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 4, 4, 0, 2, 2, 0, 2, 0, 0, 10, 10, 0, 0, 0, 0, 4, 0], [0, 2, 2, 0, 4, 4, 4, 0, 0, 0, 0, 10, 10, 0, 0, 0, 4, 0, 0, 0, 0, 0, 6, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 6, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0], [0, 2, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0], [8, 8, 8, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13, 0, 6, 0, 0, 4, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 0, 0], [0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 14, 14, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 7, 0, 0], [7, 7, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 4, 4, 4, 4, 0], [7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 7, 7, 0, 4, 0, 2, 0, 0, 6, 0, 6, 0, 0, 2, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 4, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 8, 0, 2, 0, 8, 0, 0, 8, 0, 8, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 2, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0], [0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 2, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [8, 8, 8, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 4, 4, 4, 0], [4, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 6, 6, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 6, 0, 2, 0, 0, 0, 0, 0, 4, 4, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 4, 7, 7, 0, 2, 0], [0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0], [0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 18, 18, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 9, 9, 0, 2, 2], [0, 8, 0, 5, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 13, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 14, 14, 14, 0, 6], [0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 8, 0], [4, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 6, 0, 0, 6, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 8, 0, 15, 15, 15, 0], [0, 0, 0, 7, 7, 0, 8, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 2, 2, 2, 2, 0, 0, 3, 0, 7, 7, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13, 0, 6, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 8, 0, 0], [0, 2, 0, 4, 4, 4, 4, 0, 0, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 6, 0, 6, 6, 6, 0, 0, 6, 6, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 5, 0, 0, 6, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 0, 0, 0], [0, 0, 0, 7, 0, 7, 0, 0, 0, 12, 12, 12, 12, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0], [0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 16, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [11, 11, 11, 11, 11, 0, 4, 4, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 4, 0], [0, 0, 4, 4, 4, 4, 4, 0, 4, 0, 0, 6, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 4, 4, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 7, 7, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 3, 0, 0], [8, 8, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [0, 4, 0, 4, 4, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0], [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 3, 0, 0, 7, 7, 7, 7, 0, 0, 0, 0, 6, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 7, 0, 0, 5, 0, 0, 7, 7, 7, 7, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 2, 0, 0, 8, 0], [6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 8, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 0, 2, 2, 2, 2, 0], [4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 4, 4, 4, 4, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 6, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 4, 0, 0, 0, 10, 10, 0, 0], [0, 4, 0, 0, 0, 0, 0, 0], [0, 6, 0, 3, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 0, 6, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 13, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0], [0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0], [4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 4, 0, 0, 8, 0, 0, 13, 13, 13, 13, 0, 0, 6, 0, 6, 0], [4, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0], [0, 0, 7, 0, 0, 0, 17, 17, 17, 17, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 0, 3, 0, 0, 0, 0, 0, 7, 7, 0, 0, 8, 0, 8, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 17, 17, 17, 0, 7, 7, 0, 0, 3, 0, 0, 0, 8, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 2, 0, 7, 0, 0, 0, 6, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 12, 12, 12, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0], [0, 0, 0, 8, 4, 0, 0, 0, 8, 4, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0], [0, 0, 6, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 7, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 0, 6, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6], [0, 0, 0, 0, 5, 0, 2, 2, 0, 0, 0, 0, 0, 6, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 7, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 10, 0, 6], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0], [0, 7, 7, 7, 0, 8, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 7, 7, 2, 0, 0, 3, 0, 0, 0, 7, 7, 7, 7, 0, 6, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 6, 6, 6, 6, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0, 0, 6, 0], [0, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 2, 0, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 4, 0, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 0, 4, 4, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 2, 2, 2, 2, 0], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 8, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0], [0, 7, 7, 7, 0, 8, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 0, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 4, 0, 6, 0], [0, 0, 8, 0, 8, 0, 0, 2, 2, 8, 0, 0, 0, 8, 0, 8, 11, 0, 2, 0, 8, 0, 0, 0, 8, 0, 8, 0, 8, 0, 8], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 0, 0, 0, 0, 6, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 6, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 7, 7, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "align_with_labels(train_lemma, train_pos, train_dep, train_ent, train_labels_encoded)\n",
        "align_with_labels(val_lemma, val_pos, val_dep, val_ent, val_labels_encoded)\n",
        "\n",
        "train_input_index = to_index(train_lemma, word_to_ix)\n",
        "train_output_index = to_index(train_labels, label_to_idx)\n",
        "train_ent_index =  to_index(train_ent,ent_to_ix)\n",
        "train_dep_index = to_index(train_dep,dep_to_ix)\n",
        "train_pos_index =  to_index(train_pos,pos_to_ix)\n",
        "print(len(train_input_index))\n",
        "print(train_pos_index)\n",
        "\n",
        "val_input_index = to_index(val_lemma, word_to_ix)\n",
        "val_output_index = to_index(val_labels, label_to_idx)\n",
        "val_ent_index =  to_index(val_ent,ent_to_ix)\n",
        "val_dep_index = to_index(val_dep,dep_to_ix)\n",
        "val_pos_index =  to_index(val_pos,pos_to_ix)\n",
        "print(val_ent_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "uhu9_vj2PtZR",
        "outputId": "e2739280-b30b-444d-bc8e-47c756ae7d1c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of the model is modified from the pytorch tutorial:\\nhttps://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\\nThe idea of the attention position is from \\nhttps://github.com/JohnnyPeng123/Attention_Based_Bi-LSTM_NER_with_CRF/blob/master/Code.ipynb,\\nbut the code was modified because the dimensions of the hidden outputs are different.\\nAnother modification is there are also a parameter available to choose the attention method.\\n'"
            ]
          },
          "execution_count": 63,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: You should test your NER model with CRF/ without CRF.\n",
        "# Lab 9\n",
        "import math\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        # The Word Embedding\n",
        "        self.word_embeds = nn.Embedding(sem_embedding_matrix.shape[0], 50)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(sem_embedding_matrix))\n",
        "\n",
        "        # The PoS Tag Embedding\n",
        "        if (apply_pos):\n",
        "            self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "            self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        # The Dependency Embedding\n",
        "        if (apply_dep):\n",
        "            self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "            self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        # The Entities Embedding\n",
        "        if (apply_ent):\n",
        "            self.ent_embeds = nn.Embedding(ent_embedding.shape[0], ent_embedding.shape[0])\n",
        "            self.ent_embeds.weight.data.copy_(torch.from_numpy(ent_embedding))\n",
        "            \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=num_layers, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        if attn_type != ATTN_TYPE_NONE:\n",
        "            self.hidden2tag = nn.Linear(hidden_dim * 2, self.tagset_size)\n",
        "        else:\n",
        "            self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        #self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2 * num_layers, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2 * num_layers, 1, self.hidden_dim // 2).to(device))\n",
        "  \n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags, **kw):\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            ent = kw['ent']\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos and apply_dep):\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos):\n",
        "            pos = kw['pos']\n",
        "        elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            feats = self._get_lstm_features(sentence, ent=ent, pos=pos, dep=dep)\n",
        "        elif (apply_pos and apply_dep):\n",
        "            feats = self._get_lstm_features(sentence, pos=pos, dep=dep)\n",
        "        elif (apply_pos):\n",
        "            feats = self._get_lstm_features(sentence, pos=pos)\n",
        "        elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "            feats = self._get_lstm_features(sentence)\n",
        "        else:\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "        \n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def _get_lstm_features(self, sentence, **kw):\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            ent = kw['ent']\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos and apply_dep):\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos):\n",
        "            pos = kw['pos']\n",
        "        elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            ent_embeds = self.ent_embeds(ent).view(len(ent), 1, -1)\n",
        "            pos_embeds = self.pos_embeds(pos).view(len(pos), 1, -1)\n",
        "            dep_embeds = self.dep_embeds(dep).view(len(dep), 1, -1)\n",
        "        elif (apply_pos and apply_dep):\n",
        "            pos_embeds = self.pos_embeds(pos).view(len(pos), 1, -1)\n",
        "            dep_embeds = self.dep_embeds(dep).view(len(dep), 1, -1)\n",
        "        elif (apply_pos):\n",
        "            pos_embeds = self.pos_embeds(pos).view(len(pos), 1, -1)\n",
        "        elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            embeds = torch.cat((word_embeds, ent_embeds, pos_embeds, dep_embeds), 2)  \n",
        "        elif (apply_pos and apply_dep):\n",
        "            embeds = torch.cat((word_embeds, pos_embeds, dep_embeds), 2)  \n",
        "        elif (apply_pos):\n",
        "            embeds = torch.cat((word_embeds, pos_embeds), 2)  \n",
        "        elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "            embeds = word_embeds\n",
        "        else:\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        if attn_type != ATTN_TYPE_NONE:\n",
        "            # hidden_out.shape: (2 * self.num_layers, 1, 25)\n",
        "            hidden_out = torch.cat((self.hidden[0].view(2 * self.num_layers, 1, 25)[:,0,:],self.hidden[0].view(2 * self.num_layers, 1, 25)[:,0,:]), 1)\n",
        "            hidden_out = hidden_out.unsqueeze(0)\n",
        "\n",
        "            att_out = self.cal_attention(lstm_out, hidden_out, attn_type)\n",
        "            att_out = att_out.view(-1, self.hidden_dim * 2)\n",
        "            \n",
        "            lstm_feats = self.hidden2tag(att_out)\n",
        "        else:\n",
        "            lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "            lstm_feats = self.hidden2tag(lstm_out)\n",
        "\n",
        "        return lstm_feats\n",
        "\n",
        "    def cal_attention(self, lstm_out, hidden_out, method):\n",
        "        # Transpose the hidden output\n",
        "        hidden_out_t = torch.transpose(hidden_out, 0, 1)\n",
        "        hidden_out_t = torch.transpose(hidden_out, 1, 2)\n",
        "\n",
        "        hidden_out_t_1 = hidden_out_t   \n",
        "        hidden_out_1 = hidden_out\n",
        "\n",
        "        for i in range(lstm_out.size()[0] - 1):\n",
        "            hidden_out_t = torch.cat((hidden_out_t, hidden_out_t_1), 0)\n",
        "            hidden_out = torch.cat((hidden_out, hidden_out_1), 0)   \n",
        "\n",
        "        if attn_num == 1:\n",
        "            # Dot-product attention: (Luong 2015)\n",
        "            if method == ATTN_TYPE_DOT_PRODUCT:\n",
        "                # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "                attn_weights = F.softmax(torch.bmm(lstm_out, hidden_out_t), dim=-1)\n",
        "                #print(attn_weights.shape)\n",
        "\n",
        "                attn_output = torch.bmm(attn_weights, hidden_out)\n",
        "                concat_output = torch.cat((attn_output, lstm_out), 1)\n",
        "\n",
        "            # Scaled dot-product attention: (Vaswani 2017)\n",
        "            elif method == ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "                attn_weights = F.softmax((torch.bmm(lstm_out, hidden_out_t) / math.sqrt(len(hidden_out))), dim=-1)\n",
        "                attn_output = torch.bmm(attn_weights, hidden_out)\n",
        "                concat_output = torch.cat((attn_output, lstm_out), 1)\n",
        "            \n",
        "            # Content based attention: score(st,hi)=cosine[st,hi] (Graves 2014)\n",
        "            elif method == ATTN_TYPE_CONTENT_BASE:\n",
        "                # cosine_similarity: https://pytorch.org/docs/stable/nn.functional.html\n",
        "                attn_weights = F.softmax(torch.cosine_similarity(lstm_out, hidden_out, dim=2, eps=1e-6), dim=-1)\n",
        "                attn_weights = torch.unsqueeze(attn_weights, 1) # Input shape: (52, 2) -> Target shape: (52, 1, 2)\n",
        "                attn_output = torch.bmm(attn_weights, hidden_out)\n",
        "                concat_output = torch.cat((attn_output, lstm_out), 1)\n",
        "\n",
        "        # Multi-head attention\n",
        "        else:\n",
        "              attn_output_list = []\n",
        "              # Dot-product attention: (Luong 2015)\n",
        "              if method == ATTN_TYPE_CONTENT_BASE:\n",
        "                  # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "                  split_h_out = torch.tensor_split(hidden_out, attn_num)\n",
        "                  split_lstm_out = torch.tensor_split(lstm_out, attn_num)\n",
        "                  \n",
        "                  for i in range(len(split_h_out)):\n",
        "                      attn_weights = F.softmax(torch.cosine_similarity(split_lstm_out[i], split_h_out[i], dim=2, eps=1e-6), dim=-1)\n",
        "                      attn_weights = torch.unsqueeze(attn_weights, 1) # Input shape: (52 / attn_num, 2) -> Target shape: (52 / attn_num, 1, 2)\n",
        "                      attn_output = torch.bmm(attn_weights, split_h_out[i])\n",
        "                      attn_output_list.append(attn_output)\n",
        "\n",
        "                  attn_output_final = reduce(lambda x, y: torch.cat((x, y), 0), attn_output_list)\n",
        "                  concat_output = torch.cat((attn_output_final, lstm_out), 1)\n",
        "        \n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, sentence, **kw):  # dont confuse this with _forward_alg above.\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            ent = kw['ent']\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos and apply_dep):\n",
        "            pos = kw['pos']\n",
        "            dep = kw['dep']\n",
        "        elif (apply_pos):\n",
        "            pos = kw['pos']\n",
        "        elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            lstm_feats = self._get_lstm_features(sentence, ent=ent, pos=pos, dep=dep)\n",
        "        elif (apply_pos and apply_dep):\n",
        "            lstm_feats = self._get_lstm_features(sentence, pos=pos, dep=dep)\n",
        "        elif (apply_pos):\n",
        "            lstm_feats = self._get_lstm_features(sentence, pos=pos)\n",
        "        elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "            lstm_feats = self._get_lstm_features(sentence)\n",
        "        else:\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "'''\n",
        "Code of the model is modified from the pytorch tutorial:\n",
        "https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
        "The idea of the attention position is from \n",
        "https://github.com/JohnnyPeng123/Attention_Based_Bi-LSTM_NER_with_CRF/blob/master/Code.ipynb,\n",
        "but the code was modified because the dimensions of the hidden outputs are different.\n",
        "Another modification is there are also a parameter available to choose the attention method.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1SeeG1FEzCtg",
        "outputId": "04130a10-d2c3-49be-f552-e49a61109bde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe code is modified from lab 9.\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "def cal_acc(model, input_index, output_index, ent_index, pos_index, dep_index):\n",
        "    apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "    predicted = []\n",
        "    ground_truth = []\n",
        "    \n",
        "    for i, idxs in enumerate(input_index):\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            ent = torch.tensor(ent_index[i], dtype=torch.long).to(device)\n",
        "            dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "            pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "        elif (apply_pos and apply_dep):\n",
        "            dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "            pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "        elif (apply_pos):\n",
        "            pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "        elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        if (apply_ent and apply_pos and apply_dep):\n",
        "            _, s_pred = model.forward(sentence_in, ent=ent, pos=pos, dep=dep)\n",
        "        elif (apply_pos and apply_dep):\n",
        "            _, s_pred = model.forward(sentence_in, pos=pos, dep=dep)\n",
        "        elif (apply_pos):\n",
        "            _, s_pred = model.forward(sentence_in, pos=pos)\n",
        "        elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "            _, s_pred = model.forward(sentence_in)\n",
        "        else:\n",
        "            raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "       \n",
        "        predicted += s_pred\n",
        "\n",
        "    for seq in output_index:\n",
        "        ground_truth += seq\n",
        "   \n",
        "    count = 0\n",
        "    for i in range(len(ground_truth)):\n",
        "        if predicted[i] == ground_truth[i]:\n",
        "            count += 1\n",
        "    accuracy = count / len(ground_truth)\n",
        "\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "'''\n",
        "The code is modified from lab 9.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ReO0rmmLDa1l",
        "outputId": "ff70539d-fe1e-4c85-c1d3-cbfe572c8d04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe code is modified from lab 9.\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "def train_crf(model, optimizer, train_input_index, train_output_index, val_input_index, val_output_index):\n",
        "    apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "    for epoch in range(epochs):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            # Step 1. Remember that Pytorch accumulates gradients.\n",
        "            # We need to clear them out before each instance\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 2. Get our inputs ready for the network, that is,\n",
        "            # turn them into Tensors of word indices.\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(train_output_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "            if (apply_ent and apply_pos and apply_dep):\n",
        "                ent = torch.tensor(train_ent_index[i], dtype=torch.long).to(device)\n",
        "                dep = torch.tensor(train_dep_index[i], dtype=torch.long).to(device)\n",
        "                pos = torch.tensor(train_pos_index[i], dtype=torch.long).to(device)\n",
        "            elif (apply_pos and apply_dep):\n",
        "                dep = torch.tensor(train_dep_index[i], dtype=torch.long).to(device)\n",
        "                pos = torch.tensor(train_pos_index[i], dtype=torch.long).to(device)\n",
        "            elif (apply_pos):\n",
        "                pos = torch.tensor(train_pos_index[i], dtype=torch.long).to(device)\n",
        "            elif (not(not apply_ent and not apply_pos and not apply_dep)):\n",
        "                raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "            # Step 3. Run our forward pass.\n",
        "            # NOTE: Only 4 cases are tested: \n",
        "            # word embedding, word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            if (apply_ent and apply_pos and apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, ent=ent, pos=pos, dep=dep)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss+=loss.item()\n",
        "            elif (apply_pos and apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, pos=pos, dep=dep)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss+=loss.item()\n",
        "            elif (apply_pos):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, pos=pos)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss+=loss.item()\n",
        "            elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss+=loss.item()\n",
        "            else:\n",
        "                raise Exception(\"Only 4 cases are allowed: word embedding + pos, word embedding + pos + dep, word embedding + pos + dep + ent\")\n",
        "\n",
        "        model.eval()\n",
        "        \n",
        "        # Call the cal_acc functions you implemented as required\n",
        "        _, _, train_acc = cal_acc(model, train_input_index, train_output_index, train_ent_index, train_pos_index, train_dep_index)\n",
        "        _, _, val_acc = cal_acc(model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "        \n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(val_output_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "            if (apply_ent and apply_pos and apply_dep):\n",
        "                ent = torch.tensor(val_ent_index[i], dtype=torch.long).to(device)\n",
        "                dep = torch.tensor(val_dep_index[i], dtype=torch.long).to(device)\n",
        "                pos = torch.tensor(val_pos_index[i], dtype=torch.long).to(device)\n",
        "            elif (apply_pos and apply_dep):\n",
        "                dep = torch.tensor(val_dep_index[i], dtype=torch.long).to(device)\n",
        "                pos = torch.tensor(val_pos_index[i], dtype=torch.long).to(device)\n",
        "            elif (apply_pos):\n",
        "                pos = torch.tensor(val_pos_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "            if (apply_ent and apply_pos and apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, ent=ent, pos=pos, dep=dep)\n",
        "            elif (apply_pos and apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, pos=pos, dep=dep)\n",
        "            elif (apply_pos):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets, pos=pos)\n",
        "            elif (not apply_ent and not apply_pos and not apply_dep):\n",
        "                loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "            val_loss+=loss.item()\n",
        "\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        print(\"Epoch: %d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "'''\n",
        "The code is modified from lab 9.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caqMyIE238jD",
        "outputId": "e462dcf2-3d39-435c-8f94-26040b7cb634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 10462.77, train acc: 0.8534, val loss: 2685.54, val acc: 0.7823, time: 98.47s\n",
            "Epoch: 2, Training loss: 4586.10, train acc: 0.8997, val loss: 2326.13, val acc: 0.7984, time: 99.04s\n",
            "Epoch: 3, Training loss: 3051.16, train acc: 0.9239, val loss: 2394.95, val acc: 0.7964, time: 98.87s\n",
            "Epoch: 4, Training loss: 2421.32, train acc: 0.9282, val loss: 2805.39, val acc: 0.7895, time: 100.04s\n",
            "Epoch: 5, Training loss: 2187.65, train acc: 0.9292, val loss: 2972.29, val acc: 0.7874, time: 99.17s\n",
            "Epoch: 6, Training loss: 1909.39, train acc: 0.9251, val loss: 3414.92, val acc: 0.7873, time: 99.14s\n",
            "Epoch: 7, Training loss: 1832.98, train acc: 0.9433, val loss: 3202.15, val acc: 0.7924, time: 99.23s\n",
            "Epoch: 8, Training loss: 1656.74, train acc: 0.9513, val loss: 3191.36, val acc: 0.7884, time: 98.88s\n",
            "Epoch: 9, Training loss: 1859.00, train acc: 0.9489, val loss: 3387.04, val acc: 0.7977, time: 98.93s\n",
            "Epoch: 10, Training loss: 1720.47, train acc: 0.9559, val loss: 3564.69, val acc: 0.7965, time: 98.61s\n"
          ]
        }
      ],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "num_layers = 1\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 1\n",
        "\n",
        "embedding_config = [True, True, True] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_crf = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer_crf = optim.Adam(model_crf.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_crf, optimizer_crf, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xleF8krtDwQy"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "def save(model, name):\n",
        "    path = F\"/content/gdrive/My Drive/{name}\" \n",
        "    torch.save(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2VPzwHLuF1VA"
      },
      "outputs": [],
      "source": [
        "torch.save(model_crf, 'our_model.pt')\n",
        "save(model_crf, 'our_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4x8J5MuXZRt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3.2 Additional Components [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8qUeY4SXYI_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObqbMoyLXhoP"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## 4. Evaluation and Testing\n",
        "\n",
        "NOTE: Each model will take **15-20 minutes** to train and evaluate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWBEDfZXXnw-"
      },
      "source": [
        "### 4.1 Performance Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcGFPbDlNZ5h"
      },
      "source": [
        "#### 4.1.1 The Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_nOR93ENdJo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(Baseline, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(base_embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "IJ7i7dt_T8I5",
        "outputId": "847c7309-bc9f-4d4c-d620-67f7dedea939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of the baseline model is from lab 9.\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "base_word_emb_model = api.load(\"glove-twitter-25\") \n",
        "\n",
        "base_embedding_matrix = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        base_embedding_matrix.append(base_word_emb_model.wv[word])\n",
        "    except:\n",
        "        base_embedding_matrix.append([0]*25)\n",
        "base_embedding_matrix = np.array(base_embedding_matrix)\n",
        "base_embedding_matrix.shape\n",
        "\n",
        "'''\n",
        "Code of the baseline model is from lab 9.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUzP8meXRx7_",
        "outputId": "16677074-3fc7-419a-c479-c2b085bb41a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 15010.39, train acc: 0.7238, val loss: 4790.61, val acc: 0.6763, time: 115.75s\n",
            "Epoch: 2, Training loss: 9804.34, train acc: 0.7589, val loss: 3956.37, val acc: 0.7148, time: 117.77s\n",
            "Epoch: 3, Training loss: 8091.52, train acc: 0.7821, val loss: 3505.46, val acc: 0.7308, time: 118.42s\n",
            "Epoch: 4, Training loss: 7024.61, train acc: 0.7971, val loss: 3149.58, val acc: 0.7421, time: 118.35s\n",
            "Epoch: 5, Training loss: 6221.92, train acc: 0.8109, val loss: 2858.82, val acc: 0.7497, time: 121.44s\n",
            "Epoch: 6, Training loss: 5593.91, train acc: 0.8312, val loss: 2656.78, val acc: 0.7600, time: 121.12s\n",
            "Epoch: 7, Training loss: 5073.71, train acc: 0.8361, val loss: 2577.77, val acc: 0.7662, time: 121.08s\n",
            "Epoch: 8, Training loss: 4591.77, train acc: 0.8511, val loss: 2459.27, val acc: 0.7755, time: 121.75s\n",
            "Epoch: 9, Training loss: 4185.82, train acc: 0.8662, val loss: 2352.06, val acc: 0.7799, time: 121.70s\n",
            "Epoch: 10, Training loss: 3800.04, train acc: 0.8764, val loss: 2323.39, val acc: 0.7869, time: 122.67s\n",
            "Epoch: 11, Training loss: 3500.38, train acc: 0.8799, val loss: 2328.45, val acc: 0.7848, time: 115.90s\n",
            "Epoch: 12, Training loss: 3227.84, train acc: 0.8932, val loss: 2281.75, val acc: 0.7952, time: 115.32s\n",
            "Epoch: 13, Training loss: 2961.78, train acc: 0.8958, val loss: 2352.98, val acc: 0.7918, time: 116.36s\n",
            "Epoch: 14, Training loss: 2769.20, train acc: 0.9074, val loss: 2297.21, val acc: 0.7973, time: 120.14s\n",
            "Epoch: 15, Training loss: 2586.50, train acc: 0.9132, val loss: 2353.77, val acc: 0.7986, time: 121.51s\n",
            "Epoch: 16, Training loss: 2399.55, train acc: 0.9150, val loss: 2375.73, val acc: 0.7958, time: 122.94s\n",
            "Epoch: 17, Training loss: 2243.92, train acc: 0.9170, val loss: 2472.04, val acc: 0.7884, time: 121.11s\n",
            "Epoch: 18, Training loss: 2126.12, train acc: 0.9165, val loss: 2565.85, val acc: 0.7861, time: 122.18s\n",
            "Epoch: 19, Training loss: 1974.99, train acc: 0.9136, val loss: 2654.00, val acc: 0.7863, time: 121.63s\n",
            "Epoch: 20, Training loss: 1836.88, train acc: 0.9237, val loss: 2643.65, val acc: 0.7874, time: 123.13s\n"
          ]
        }
      ],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "num_layers = 1\n",
        "SEM_EMBEDDING_DIM = 25 # Without pos tag\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 20 # The original configuration\n",
        "attn_type = ATTN_TYPE_NONE\n",
        "\n",
        "embedding_config = [False, False, False] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM\n",
        "\n",
        "base_model = Baseline(len(word_to_ix), label_to_idx, 25, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(base_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(base_model, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Oz8DLriopC",
        "outputId": "c17aa04f-547e-4929-fabd-28de4022f7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "save(base_model, 'baseline_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leuYww-dNdiB"
      },
      "source": [
        "#### 4.1.2 Our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKk0q8AEXmCz",
        "outputId": "308019d1-ec54-4021-aa82-e8cbf17f63b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.71%\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "\n",
        "our_model = torch.load('our_model.pt')\n",
        "\n",
        "_, _, val_acc = cal_acc(our_model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "print(\"Accuracy: \" + str(round(val_acc * 100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTn7IaA8Xrd0"
      },
      "source": [
        "### 4.2 Ablation Study - different input embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lIXdff9Xume"
      },
      "outputs": [],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "num_layers = 1\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqDvEWi90gJk"
      },
      "source": [
        "#### 4.2.1 Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n5NHT5-Miom",
        "outputId": "efec093c-db31-4e65-e33f-421f55d32656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 10726.34, train acc: 0.8585, val loss: 2847.94, val acc: 0.7613, time: 121.81s\n",
            "Epoch: 2, Training loss: 4316.34, train acc: 0.9162, val loss: 2502.80, val acc: 0.7823, time: 123.25s\n",
            "Epoch: 3, Training loss: 2603.91, train acc: 0.9366, val loss: 2775.41, val acc: 0.7783, time: 121.82s\n",
            "Epoch: 4, Training loss: 2029.57, train acc: 0.9444, val loss: 2993.74, val acc: 0.7780, time: 120.64s\n",
            "Epoch: 5, Training loss: 1605.54, train acc: 0.9551, val loss: 3239.98, val acc: 0.7630, time: 120.81s\n",
            "Epoch: 6, Training loss: 1437.58, train acc: 0.9563, val loss: 3544.06, val acc: 0.7749, time: 121.19s\n",
            "Epoch: 7, Training loss: 1321.26, train acc: 0.9587, val loss: 3734.16, val acc: 0.7717, time: 121.34s\n",
            "Epoch: 8, Training loss: 1192.50, train acc: 0.9666, val loss: 3855.21, val acc: 0.7736, time: 127.42s\n",
            "Epoch: 9, Training loss: 1026.22, train acc: 0.9740, val loss: 4322.08, val acc: 0.7766, time: 126.89s\n",
            "Epoch: 10, Training loss: 928.29, train acc: 0.9687, val loss: 4735.40, val acc: 0.7742, time: 127.75s\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [False, False, False] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_sem = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_sem.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_sem, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOOd_3l5jbAb",
        "outputId": "426375b0-5db2-48bc-ae60-4da762de4e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "save(model_sem, 'model_sem.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9u8hFwFMyJq"
      },
      "source": [
        "#### 4.2.2 Word Embedding + PoS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUVjQMSUM17A",
        "outputId": "c87328e2-caa5-4608-b842-3b963fc3fd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 14442.39, train acc: 0.8446, val loss: 3177.79, val acc: 0.7759, time: 126.57s\n",
            "Epoch: 2, Training loss: 4832.65, train acc: 0.9049, val loss: 2643.78, val acc: 0.7861, time: 124.58s\n",
            "Epoch: 3, Training loss: 2986.43, train acc: 0.9321, val loss: 2587.58, val acc: 0.7941, time: 123.78s\n",
            "Epoch: 4, Training loss: 2133.67, train acc: 0.9374, val loss: 3010.04, val acc: 0.7819, time: 125.41s\n",
            "Epoch: 5, Training loss: 1677.84, train acc: 0.9532, val loss: 3045.98, val acc: 0.7941, time: 125.56s\n",
            "Epoch: 6, Training loss: 1529.34, train acc: 0.9573, val loss: 3196.31, val acc: 0.7928, time: 122.17s\n",
            "Epoch: 7, Training loss: 1348.26, train acc: 0.9579, val loss: 3410.50, val acc: 0.7886, time: 119.81s\n",
            "Epoch: 8, Training loss: 1315.07, train acc: 0.9611, val loss: 3357.65, val acc: 0.7857, time: 118.59s\n",
            "Epoch: 9, Training loss: 1233.01, train acc: 0.9535, val loss: 3764.49, val acc: 0.7736, time: 118.96s\n",
            "Epoch: 10, Training loss: 1367.30, train acc: 0.9624, val loss: 4109.25, val acc: 0.7865, time: 118.69s\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [False, False, True] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0]\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_sem_pos = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_sem_pos.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_sem_pos, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx9JcbNXjhBC",
        "outputId": "95064d7a-576c-4350-df1d-fdaa6bdf67ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "save(model_sem_pos, 'model_sem_pos.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCITXypXNF3y"
      },
      "source": [
        "#### 4.2.3 Word Embedding + PoS Tagging + Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hdORLpJNPH9",
        "outputId": "885637be-f395-46e2-bc75-a585b4b5fa2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 11537.83, train acc: 0.8511, val loss: 2936.48, val acc: 0.7753, time: 119.70s\n",
            "Epoch: 2, Training loss: 4641.92, train acc: 0.8956, val loss: 2330.46, val acc: 0.7994, time: 118.41s\n",
            "Epoch: 3, Training loss: 3020.86, train acc: 0.9127, val loss: 2627.27, val acc: 0.7895, time: 119.26s\n",
            "Epoch: 4, Training loss: 2305.39, train acc: 0.9081, val loss: 3029.41, val acc: 0.7666, time: 120.06s\n",
            "Epoch: 5, Training loss: 2093.75, train acc: 0.9416, val loss: 2931.60, val acc: 0.7763, time: 119.39s\n",
            "Epoch: 6, Training loss: 1814.11, train acc: 0.9395, val loss: 3135.58, val acc: 0.7854, time: 119.76s\n",
            "Epoch: 7, Training loss: 1688.02, train acc: 0.9387, val loss: 3345.52, val acc: 0.7753, time: 119.70s\n",
            "Epoch: 8, Training loss: 1559.84, train acc: 0.9625, val loss: 3360.65, val acc: 0.7888, time: 118.07s\n",
            "Epoch: 9, Training loss: 1506.06, train acc: 0.9549, val loss: 3566.86, val acc: 0.7912, time: 118.06s\n",
            "Epoch: 10, Training loss: 1358.25, train acc: 0.9629, val loss: 3552.95, val acc: 0.7854, time: 117.84s\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [False, True, True] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0]\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_sem_pos_dep = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_sem_pos_dep.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_sem_pos_dep, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE0_8l0gjkBA",
        "outputId": "fd6b7143-704d-4453-a28e-c451a6d4a5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "save(model_sem_pos_dep, 'model_sem_pos_dep.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KalGQlpNLcw"
      },
      "source": [
        "#### 4.2.4 Word Embedding + PoS Tagging + Dependency + Entity (Our Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lhy3v_vNOKH",
        "outputId": "cfd8216a-dd42-4618-e4bf-2806c2b32297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.43%\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "\n",
        "our_model = torch.load('our_model.pt')\n",
        "_, _, val_acc = cal_acc(our_model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "print(\"Accuracy: \" + str(round(val_acc * 100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LunrH4rQXu6z"
      },
      "source": [
        "### 4.3 Ablation Study - different attention strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxVQCdO6QSZh"
      },
      "outputs": [],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "num_layers = 1\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "\n",
        "embedding_config = [True, True, True] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "attn_num = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RHoRtBm3hXh"
      },
      "source": [
        "#### 4.3.0 No Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxNUTysw3knR",
        "outputId": "719c4f32-a67c-4737-a96a-b1e7bea4c376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 9506.67, train acc: 0.8590, val loss: 2623.50, val acc: 0.7867, time: 103.91s\n",
            "Epoch: 2, Training loss: 4209.53, train acc: 0.8967, val loss: 2272.01, val acc: 0.8032, time: 103.80s\n",
            "Epoch: 3, Training loss: 2893.37, train acc: 0.9164, val loss: 2415.11, val acc: 0.8039, time: 104.84s\n",
            "Epoch: 4, Training loss: 2300.79, train acc: 0.9380, val loss: 2410.50, val acc: 0.8015, time: 104.01s\n",
            "Epoch: 5, Training loss: 1887.07, train acc: 0.9321, val loss: 2848.22, val acc: 0.7948, time: 104.21s\n",
            "Epoch: 6, Training loss: 1758.94, train acc: 0.9407, val loss: 2929.00, val acc: 0.7871, time: 105.19s\n",
            "Epoch: 7, Training loss: 1700.94, train acc: 0.9545, val loss: 2872.68, val acc: 0.8026, time: 105.54s\n",
            "Epoch: 8, Training loss: 1518.99, train acc: 0.9608, val loss: 2813.65, val acc: 0.7994, time: 104.73s\n",
            "Epoch: 9, Training loss: 1382.17, train acc: 0.9463, val loss: 3371.29, val acc: 0.7859, time: 105.03s\n",
            "Epoch: 10, Training loss: 1383.01, train acc: 0.9575, val loss: 3343.39, val acc: 0.7954, time: 104.88s\n"
          ]
        }
      ],
      "source": [
        "attn_type = ATTN_TYPE_NONE\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_no_attn = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_no_attn.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_no_attn, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63RVAD7Mkf_d"
      },
      "outputs": [],
      "source": [
        "save(model_no_attn, 'model_no_attn.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgIu7_FQPYCo"
      },
      "source": [
        "#### 4.3.1 Dot-Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Z-aFMgX9bV",
        "outputId": "8cd07a07-5963-4b36-93c7-e91cbc8214a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 11569.67, train acc: 0.8574, val loss: 3003.09, val acc: 0.7935, time: 104.81s\n",
            "Epoch: 2, Training loss: 4728.40, train acc: 0.8970, val loss: 2508.46, val acc: 0.7886, time: 106.08s\n",
            "Epoch: 3, Training loss: 3086.33, train acc: 0.9240, val loss: 2804.63, val acc: 0.7928, time: 106.80s\n",
            "Epoch: 4, Training loss: 2388.01, train acc: 0.9369, val loss: 2669.07, val acc: 0.7911, time: 107.55s\n",
            "Epoch: 5, Training loss: 1898.64, train acc: 0.9405, val loss: 2950.74, val acc: 0.7971, time: 107.03s\n",
            "Epoch: 6, Training loss: 1748.82, train acc: 0.9446, val loss: 3188.11, val acc: 0.8017, time: 106.42s\n",
            "Epoch: 7, Training loss: 1874.92, train acc: 0.9434, val loss: 3220.46, val acc: 0.7933, time: 106.41s\n",
            "Epoch: 8, Training loss: 1727.14, train acc: 0.9511, val loss: 3197.54, val acc: 0.8007, time: 106.38s\n",
            "Epoch: 9, Training loss: 1667.40, train acc: 0.9580, val loss: 3510.20, val acc: 0.8003, time: 105.94s\n",
            "Epoch: 10, Training loss: 1568.44, train acc: 0.9491, val loss: 3722.73, val acc: 0.7848, time: 106.25s\n"
          ]
        }
      ],
      "source": [
        "attn_type = ATTN_TYPE_DOT_PRODUCT\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_dot_prod = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_dot_prod.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_dot_prod, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E4i8mT9ktcH"
      },
      "outputs": [],
      "source": [
        "save(model_dot_prod, 'model_dot_prod.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OojjCTtiPcaR"
      },
      "source": [
        "#### 4.3.2 Scaled Dot-Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es-MbGsZPeik",
        "outputId": "c68534be-0fc8-4390-91b4-0c615bff2555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 11544.38, train acc: 0.8523, val loss: 2851.70, val acc: 0.7783, time: 105.69s\n",
            "Epoch: 2, Training loss: 4821.00, train acc: 0.8977, val loss: 2498.97, val acc: 0.8043, time: 106.06s\n",
            "Epoch: 3, Training loss: 3217.47, train acc: 0.9179, val loss: 2514.90, val acc: 0.7933, time: 107.06s\n",
            "Epoch: 4, Training loss: 2330.40, train acc: 0.9363, val loss: 2792.36, val acc: 0.7914, time: 106.48s\n",
            "Epoch: 5, Training loss: 1785.24, train acc: 0.9473, val loss: 3041.39, val acc: 0.7929, time: 106.53s\n",
            "Epoch: 6, Training loss: 1657.50, train acc: 0.9452, val loss: 2884.56, val acc: 0.7926, time: 107.28s\n",
            "Epoch: 7, Training loss: 1835.62, train acc: 0.9380, val loss: 3355.47, val acc: 0.7986, time: 107.27s\n",
            "Epoch: 8, Training loss: 1712.29, train acc: 0.9485, val loss: 3271.46, val acc: 0.7931, time: 106.23s\n",
            "Epoch: 9, Training loss: 1728.55, train acc: 0.9485, val loss: 3599.71, val acc: 0.7918, time: 107.13s\n",
            "Epoch: 10, Training loss: 1608.15, train acc: 0.9534, val loss: 3500.35, val acc: 0.7996, time: 106.35s\n"
          ]
        }
      ],
      "source": [
        "attn_type = ATTN_TYPE_SCALE_DOT_PRODUCT\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_s_dot_prod = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_s_dot_prod.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_s_dot_prod, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awjySznRkxZ3"
      },
      "outputs": [],
      "source": [
        "save(model_s_dot_prod, 'model_s_dot_prod.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C--Vzh2pPeue"
      },
      "source": [
        "#### 4.3.3 Content-Based Attention (Our Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8BvYe6HPjMd",
        "outputId": "11726759-799d-4e6e-ade2-db76c962fbdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 78.86%\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "\n",
        "our_model = torch.load('our_model.pt')\n",
        "\n",
        "_, _, val_acc = cal_acc(our_model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "print(\"Accuracy: \" + str(round(val_acc * 100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8RYBbiCnjq"
      },
      "source": [
        "#### 4.3.4 1 Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtHayjrlCsNm",
        "outputId": "2b4f7b1e-7da1-4eb3-d191-35efb2f67497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.37%\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 1\n",
        "\n",
        "our_model = torch.load('our_model.pt')\n",
        "\n",
        "_, _, val_acc = cal_acc(our_model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "print(\"Accuracy: \" + str(round(val_acc * 100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzsJ_QmsC7Dn"
      },
      "source": [
        "#### 4.3.5 2 Attentions (Multi-head Attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0NKJl2RC_Dm",
        "outputId": "40db4de7-c903-4c9e-a819-227e15bca26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 10061.31, train acc: 0.8479, val loss: 2651.95, val acc: 0.7810, time: 99.86s\n",
            "Epoch: 2, Training loss: 4464.67, train acc: 0.8962, val loss: 2416.99, val acc: 0.7933, time: 100.59s\n",
            "Epoch: 3, Training loss: 2907.85, train acc: 0.9277, val loss: 2540.85, val acc: 0.7945, time: 100.86s\n",
            "Epoch: 4, Training loss: 2256.57, train acc: 0.9368, val loss: 2586.97, val acc: 0.7901, time: 100.57s\n",
            "Epoch: 5, Training loss: 2111.71, train acc: 0.9374, val loss: 3097.03, val acc: 0.7884, time: 99.94s\n",
            "Epoch: 6, Training loss: 1925.89, train acc: 0.9448, val loss: 2983.19, val acc: 0.7827, time: 100.62s\n",
            "Epoch: 7, Training loss: 1658.45, train acc: 0.9494, val loss: 3087.65, val acc: 0.7876, time: 100.08s\n",
            "Epoch: 8, Training loss: 1670.27, train acc: 0.9472, val loss: 3475.97, val acc: 0.7816, time: 100.12s\n",
            "Epoch: 9, Training loss: 1585.41, train acc: 0.9573, val loss: 3204.14, val acc: 0.7914, time: 100.32s\n",
            "Epoch: 10, Training loss: 1520.89, train acc: 0.9539, val loss: 3479.64, val acc: 0.7867, time: 99.50s\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 2\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_attn_2 = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_attn_2.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_attn_2, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk4Nay89Mltb"
      },
      "outputs": [],
      "source": [
        "save(model_attn_2, 'model_attn_2.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXEaQK4hDKjj"
      },
      "source": [
        "#### 4.3.6 4 Attentions (Multi-head Attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBj-KImnDM6e",
        "outputId": "6dea7ad9-8e5f-4fb8-812f-d8b08083fbb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 9499.52, train acc: 0.8456, val loss: 2522.55, val acc: 0.7863, time: 100.86s\n",
            "Epoch: 2, Training loss: 4303.27, train acc: 0.9011, val loss: 2220.68, val acc: 0.8032, time: 102.27s\n",
            "Epoch: 3, Training loss: 3017.09, train acc: 0.9169, val loss: 2433.04, val acc: 0.7947, time: 102.75s\n",
            "Epoch: 4, Training loss: 2347.64, train acc: 0.9309, val loss: 2602.25, val acc: 0.7975, time: 102.90s\n",
            "Epoch: 5, Training loss: 2094.71, train acc: 0.9389, val loss: 2747.24, val acc: 0.7922, time: 100.54s\n",
            "Epoch: 6, Training loss: 1751.99, train acc: 0.9461, val loss: 2886.18, val acc: 0.7920, time: 101.58s\n",
            "Epoch: 7, Training loss: 1729.54, train acc: 0.9492, val loss: 3183.56, val acc: 0.7859, time: 100.90s\n",
            "Epoch: 8, Training loss: 1763.81, train acc: 0.9575, val loss: 3170.85, val acc: 0.7964, time: 99.95s\n",
            "Epoch: 9, Training loss: 1434.41, train acc: 0.9611, val loss: 3357.13, val acc: 0.7990, time: 100.04s\n",
            "Epoch: 10, Training loss: 1286.02, train acc: 0.9582, val loss: 3626.91, val acc: 0.7882, time: 98.95s\n"
          ]
        }
      ],
      "source": [
        "embedding_config = [True, True, True]\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 4\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_attn_4 = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_attn_4.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_attn_4, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFtwke35MSf9"
      },
      "outputs": [],
      "source": [
        "save(model_attn_4, 'model_attn_4.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaYWj9WUXx6w"
      },
      "source": [
        "### 4.4 Ablation Study - different Stacked layer or # of encoder/decoder strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLEYHoXCXAAR"
      },
      "outputs": [],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM + pos_embedding.shape[0] + dep_embedding.shape[0] + ent_embedding.shape[0]\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "\n",
        "embedding_config = [True, True, True] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "attn_num = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugKTXPi0PEpK"
      },
      "source": [
        "#### 4.4.1 With 1 Stacked Layer (Our Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxkMwbDUXyH_",
        "outputId": "b59494c5-a0ec-4b44-c0bb-19f8e6f38903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.14%\n"
          ]
        }
      ],
      "source": [
        "our_model = torch.load('our_model.pt')\n",
        "\n",
        "_, _, val_acc = cal_acc(our_model, val_input_index, val_output_index, val_ent_index, val_pos_index, val_dep_index)\n",
        "print(\"Accuracy: \" + str(round(val_acc * 100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIzokWJHPJht"
      },
      "source": [
        "#### 4.4.2 With 2 Stacked Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1v3Pn2aPPiX",
        "outputId": "b13c2117-5614-43c8-b9ec-0b9c62984edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 12200.74, train acc: 0.7914, val loss: 2997.48, val acc: 0.7421, time: 114.32s\n",
            "Epoch: 2, Training loss: 5962.80, train acc: 0.8627, val loss: 2251.65, val acc: 0.7873, time: 115.62s\n",
            "Epoch: 3, Training loss: 4195.30, train acc: 0.8895, val loss: 2090.15, val acc: 0.7865, time: 116.47s\n",
            "Epoch: 4, Training loss: 3225.31, train acc: 0.9080, val loss: 2233.94, val acc: 0.7979, time: 115.74s\n",
            "Epoch: 5, Training loss: 2712.09, train acc: 0.9094, val loss: 2530.53, val acc: 0.7903, time: 115.34s\n",
            "Epoch: 6, Training loss: 2545.04, train acc: 0.9236, val loss: 2375.53, val acc: 0.7907, time: 115.96s\n",
            "Epoch: 7, Training loss: 2392.34, train acc: 0.9137, val loss: 2687.86, val acc: 0.7840, time: 116.22s\n",
            "Epoch: 8, Training loss: 2302.19, train acc: 0.9375, val loss: 2466.24, val acc: 0.7941, time: 115.32s\n",
            "Epoch: 9, Training loss: 2109.84, train acc: 0.9293, val loss: 2683.61, val acc: 0.7795, time: 116.14s\n",
            "Epoch: 10, Training loss: 1890.90, train acc: 0.9478, val loss: 2829.54, val acc: 0.7782, time: 115.42s\n"
          ]
        }
      ],
      "source": [
        "num_layers = 2\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_2 = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_2, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13irRKh4k3wi"
      },
      "outputs": [],
      "source": [
        "save(model_2, 'model_2.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITbpNp9gPQGe"
      },
      "source": [
        "#### 4.4.3 With 4 Stacked Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk8BC01EPTbk",
        "outputId": "75d4535f-1d82-44b6-c552-bd44c57dffed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 14196.10, train acc: 0.7371, val loss: 3542.23, val acc: 0.6871, time: 132.13s\n",
            "Epoch: 2, Training loss: 7629.90, train acc: 0.7446, val loss: 2894.79, val acc: 0.6898, time: 131.56s\n",
            "Epoch: 3, Training loss: 6380.14, train acc: 0.7864, val loss: 2611.17, val acc: 0.7321, time: 133.31s\n",
            "Epoch: 4, Training loss: 5538.39, train acc: 0.8019, val loss: 2708.97, val acc: 0.7349, time: 133.98s\n",
            "Epoch: 5, Training loss: 4985.87, train acc: 0.8323, val loss: 2488.33, val acc: 0.7505, time: 133.23s\n",
            "Epoch: 6, Training loss: 4588.07, train acc: 0.8495, val loss: 2569.72, val acc: 0.7584, time: 133.91s\n",
            "Epoch: 7, Training loss: 4353.65, train acc: 0.8458, val loss: 2559.48, val acc: 0.7584, time: 134.34s\n",
            "Epoch: 8, Training loss: 4036.16, train acc: 0.8556, val loss: 2544.93, val acc: 0.7539, time: 133.83s\n",
            "Epoch: 9, Training loss: 3861.35, train acc: 0.8552, val loss: 2809.23, val acc: 0.7594, time: 133.55s\n",
            "Epoch: 10, Training loss: 3846.70, train acc: 0.8610, val loss: 2539.34, val acc: 0.7622, time: 134.04s\n"
          ]
        }
      ],
      "source": [
        "num_layers = 4\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_4 = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_4, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZP4AwFbk60T"
      },
      "outputs": [],
      "source": [
        "save(model_4, 'model_4.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpeXJCjMPTw7"
      },
      "source": [
        "#### 4.4.4 With 8 Stacked Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3tbL6aNodRg",
        "outputId": "7db1e832-ceb2-4aa7-b9ae-f3af219b6570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 17362.52, train acc: 0.6999, val loss: 5567.75, val acc: 0.6485, time: 166.34s\n",
            "Epoch: 2, Training loss: 12418.12, train acc: 0.6999, val loss: 4864.92, val acc: 0.6485, time: 166.32s\n",
            "Epoch: 3, Training loss: 11491.93, train acc: 0.6711, val loss: 4504.19, val acc: 0.6088, time: 167.99s\n",
            "Epoch: 4, Training loss: 11261.05, train acc: 0.6906, val loss: 4398.35, val acc: 0.6369, time: 166.99s\n",
            "Epoch: 5, Training loss: 11065.94, train acc: 0.6370, val loss: 4338.61, val acc: 0.5694, time: 167.17s\n",
            "Epoch: 6, Training loss: 10912.08, train acc: 0.5707, val loss: 4443.54, val acc: 0.5309, time: 166.60s\n",
            "Epoch: 7, Training loss: 10822.95, train acc: 0.6467, val loss: 4361.04, val acc: 0.6022, time: 167.73s\n",
            "Epoch: 8, Training loss: 10709.92, train acc: 0.6338, val loss: 4337.43, val acc: 0.5641, time: 167.55s\n",
            "Epoch: 9, Training loss: 10549.57, train acc: 0.5906, val loss: 4248.03, val acc: 0.5434, time: 168.27s\n",
            "Epoch: 10, Training loss: 10471.50, train acc: 0.5682, val loss: 4458.39, val acc: 0.4547, time: 166.84s\n"
          ]
        }
      ],
      "source": [
        "num_layers = 8\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_8 = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_8.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_8, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zAzARERk9X2"
      },
      "outputs": [],
      "source": [
        "save(model_8, 'model_8.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7eueywMX-Bu"
      },
      "source": [
        "### 4.5. Ablation Study - with/without CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeFXy0A9XDqT"
      },
      "source": [
        "#### 4.5.1 The Plain Bi-LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "nk-k25Be21w4",
        "outputId": "708404f3-732d-4889-8e35-b7b4c9074f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "154\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of padding and batching is from lab 6.\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: The preprocessing code is from lab 6.\n",
        "# Pad to max_length\n",
        "max_length = len(max(train_input_index, key=len))\n",
        "print(max_length) \n",
        "\n",
        "def pad_sequence(seq_list, max_length, index_dict):\n",
        "    res = []\n",
        "    for seq in seq_list:\n",
        "        temp = seq[:]\n",
        "        if len(seq) > max_length:\n",
        "            res.append(temp[:max_length])\n",
        "        else:\n",
        "            temp += [index_dict['<PAD>']] * (max_length - len(seq))\n",
        "            res.append(temp)\n",
        "    return np.array(res)\n",
        "\n",
        "train_input_index_pad = pad_sequence(train_input_index, max_length, word_to_ix)\n",
        "val_input_index_pad = pad_sequence(val_input_index, max_length, word_to_ix)\n",
        "train_output_index_pad = pad_sequence(train_output_index, max_length, label_to_idx)\n",
        "val_output_index_pad = pad_sequence(val_output_index, max_length, label_to_idx)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load batches\n",
        "from torch.utils.data import TensorDataset\n",
        "#More detailed info about the TensorDataset, https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataset.html#TensorDataset\n",
        "train_data = TensorDataset(torch.from_numpy(train_input_index_pad), torch.from_numpy(train_output_index_pad))\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "#More detailed info about the dataLoader, https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n",
        "# shuffle (bool, optional) â€“ set to True to have the data reshuffled at every epoch (default: False).\n",
        "\n",
        "'''\n",
        "Code of padding and batching is from lab 6.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lhw11GWrruOp",
        "outputId": "efe24146-fbb9-4f1e-ba16-49f9b3b10b61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of the plain bi-lstm model is from lab 6.\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: You should test your NER model with CRF/ without CRF.\n",
        "# Lab 9\n",
        "import math\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ATTN_TYPE_NONE = None\n",
        "\n",
        "class Bi_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tagset_size):\n",
        "        super(Bi_LSTM, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(sem_embedding_matrix.shape[0], embedding_dim)\n",
        "        self.word_embeddings.weight.data.copy_(torch.from_numpy(sem_embedding_matrix))\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)    \n",
        "        return tag_space\n",
        "\n",
        "'''\n",
        "Code of the plain bi-lstm model is from lab 6.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ7ZZ6rmOal1"
      },
      "outputs": [],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "\n",
        "num_layers = 1\n",
        "\n",
        "embedding_config = [False, False, False] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "attn_type = ATTN_TYPE_CONTENT_BASE\n",
        "\n",
        "loss_function = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFs-KkpeXWtI"
      },
      "outputs": [],
      "source": [
        "#def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tagset_size):\n",
        "model_no_crf = Bi_LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), len(label_to_idx)).to(device)\n",
        "optimizer = optim.Adam(model_no_crf.parameters(), lr=0.01, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "2Vscnptj4j0X",
        "outputId": "548b8e72-30d3-4bba-8c4d-8c44bb288af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, training loss: -0.5862, training accuracy: 52.88%\n",
            "Epoch: 2, training loss: -5.3683, training accuracy: 86.68%\n",
            "Epoch: 3, training loss: -11.0700, training accuracy: 89.70%\n",
            "Epoch: 4, training loss: -16.4700, training accuracy: 89.61%\n",
            "Epoch: 5, training loss: -21.8635, training accuracy: 89.61%\n",
            "Epoch: 6, training loss: -27.3235, training accuracy: 89.79%\n",
            "Epoch: 7, training loss: -32.8070, training accuracy: 90.23%\n",
            "Epoch: 8, training loss: -38.3046, training accuracy: 90.55%\n",
            "Epoch: 9, training loss: -43.8387, training accuracy: 91.18%\n",
            "Epoch: 10, training loss: -49.3816, training accuracy: 92.33%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of the training process is from lab 6.\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    loss_now = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for sentence,targets in train_loader:\n",
        "        sentence = sentence.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        temp_batch_size = sentence.shape[0]\n",
        "\n",
        "        model_no_crf.train()\n",
        "        optimizer.zero_grad()               \n",
        "        tag_space = model_no_crf(sentence)\n",
        "        loss = loss_function(tag_space.view(-1, tag_space.shape[-1]), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_now += loss.item() * temp_batch_size\n",
        "        predicted = torch.argmax(tag_space, -1)\n",
        "        # Note: The training accuracy here is calculated with \"PAD\", which will result in a relative higher accuracy.\n",
        "        correct += accuracy_score(predicted.view(-1).cpu().numpy(),targets.view(-1).cpu().numpy())*temp_batch_size\n",
        "\n",
        "    print('Epoch: %d, training loss: %.4f, training accuracy: %.2f%%'%(epoch+1,loss_now/len(train_data),100*correct/len(train_data)))\n",
        "\n",
        "'''\n",
        "Code of the training process is from lab 6.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xsfzQiuW5USz",
        "outputId": "f590f9a6-88e1-4d04-b7e4-97a70997e559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 55.23%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCode of the testing process is from lab 6.\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_no_crf.eval()\n",
        "sentence = torch.from_numpy(val_input_index_pad).to(device)\n",
        "tag_space = model_no_crf(sentence)\n",
        "predicted = torch.argmax(tag_space, -1)\n",
        "predicted = predicted.cpu().numpy()\n",
        "\n",
        "# cut off the PAD part\n",
        "test_len_list = [len(s) for s in val_input_index]\n",
        "actual_predicted_list= []\n",
        "for i in range(predicted.shape[0]):\n",
        "    actual_predicted_list+=list(predicted[i])[:test_len_list[i]]\n",
        "\n",
        "# get actual tag list\n",
        "actual_tags = sum(val_output_index, [])\n",
        "\n",
        "print('Test Accuracy: %.2f%%'%(accuracy_score(actual_predicted_list,actual_tags)*100))\n",
        "\n",
        "'''\n",
        "Code of the testing process is from lab 6.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvlcOMqrlBDt"
      },
      "outputs": [],
      "source": [
        "save(model_no_crf, 'model_no_crf.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaePXslnOz_Z"
      },
      "source": [
        "#### 4.5.2 Model With CRF (No Attention, Only Word Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfU8NkDCO3kD",
        "outputId": "374ce3b3-1e43-4ea3-a808-714ac8f07fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 11706.90, train acc: 0.8597, val loss: 2904.21, val acc: 0.7689, time: 100.76s\n",
            "Epoch: 2, Training loss: 4294.48, train acc: 0.9033, val loss: 2601.35, val acc: 0.7810, time: 102.84s\n",
            "Epoch: 3, Training loss: 2680.14, train acc: 0.9147, val loss: 2727.44, val acc: 0.7774, time: 103.55s\n",
            "Epoch: 4, Training loss: 1954.10, train acc: 0.9433, val loss: 2972.54, val acc: 0.7840, time: 104.62s\n",
            "Epoch: 5, Training loss: 1622.60, train acc: 0.9564, val loss: 3309.45, val acc: 0.7789, time: 104.76s\n",
            "Epoch: 6, Training loss: 1278.68, train acc: 0.9608, val loss: 3467.75, val acc: 0.7766, time: 104.44s\n",
            "Epoch: 7, Training loss: 1177.16, train acc: 0.9660, val loss: 3745.75, val acc: 0.7749, time: 104.97s\n",
            "Epoch: 8, Training loss: 1114.30, train acc: 0.9662, val loss: 3999.14, val acc: 0.7787, time: 104.94s\n",
            "Epoch: 9, Training loss: 1018.63, train acc: 0.9640, val loss: 4300.25, val acc: 0.7670, time: 103.77s\n",
            "Epoch: 10, Training loss: 1037.18, train acc: 0.9692, val loss: 4110.35, val acc: 0.7770, time: 104.88s\n"
          ]
        }
      ],
      "source": [
        "# Three types of attentions are available: \n",
        "# - ATTN_TYPE_DOT_PRODUCT\n",
        "# - ATTN_TYPE_SCALE_DOT_PRODUCT \n",
        "# - ATTN_TYPE_CONTENT_BASE\n",
        "# NOTE: If you don't use attention, just initialize attn_type = ATTN_TYPE_NONE\n",
        "ATTN_TYPE_NONE = None\n",
        "ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "ATTN_TYPE_CONTENT_BASE = \"Content Base\"\n",
        "\n",
        "num_layers = 1\n",
        "SEM_EMBEDDING_DIM = 50 # Without pos tag\n",
        "\n",
        "HIDDEN_DIM = 50\n",
        "epochs = 10\n",
        "attn_type = ATTN_TYPE_NONE\n",
        "attn_num = 1\n",
        "\n",
        "embedding_config = [False, False, False] # [apply_ent, apply_dep, apply_pos]\n",
        "apply_ent, apply_dep, apply_pos = embedding_config\n",
        "\n",
        "EMBEDDING_DIM = SEM_EMBEDDING_DIM\n",
        "\n",
        "# def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tag_to_ix):\n",
        "model_comp = BiLSTM_CRF(EMBEDDING_DIM, HIDDEN_DIM, num_layers, len(word_to_ix), label_to_idx).to(device)\n",
        "optimizer = optim.Adam(model_comp.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "train_crf(model_comp, optimizer, train_input_index, train_output_index, val_input_index, val_output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9prjxnPHlFF0"
      },
      "outputs": [],
      "source": [
        "save(model_comp, 'model_comp.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}