{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ"
      },
      "source": [
        "#COMP5046_Ass1_2022\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD"
      },
      "source": [
        "# Readme\n",
        "1. In the whole project, I wrote many functions for myself to keep my code tidy. And almost all of them will be called before the main codes of each section of the project.\n",
        "\n",
        "2. I saved many models, graphs that trained in the project in my google drive and they will be loaded in section 1 of the project, it may take some time to process it. Due to the fact that I saved many models, graphs, tables to use in my project, my codes in section 4 of the project only contain one example of testing -> which is what we asked to do -> to left one runnable testing code in section 4 and comment out other testing codes in section 4.\n",
        "\n",
        "3. The model trained in section 3 used the best choice of variables including epoch number, learning rate, etc.\n",
        "\n",
        "4. Because I will comment many parts of section 4 (because only one testing code will be kept), some \"normal comments\" are also commented -> they are \"double-commented\".\n",
        "\n",
        "Have a good day!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preprocessing"
      ],
      "metadata": {
        "id": "A7v4GVxo4Dom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0. Data Collection [DO NOT MODIFY THIS]"
      ],
      "metadata": {
        "id": "HftyG77k47Y3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1f5eda-5d73-40df-9f3e-b5022b2d962d"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.csv')  \n",
        "\n",
        "id = '1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "training_data = pd.read_csv(\"/content/training_data.csv\")\n",
        "testing_data = pd.read_csv(\"/content/testing_data.csv\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n",
        "print(\"------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 7808\n",
            "Size of testing dataset: 867\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: F / SENTENCE: 'Half of it is going straight to charity, another quarter going straight to scientific research, an eighth to the parkour community, a sixteenth to towards spreading information about health and...|||Find a path or suffer more.|||http://personalitycafe.com/enneagram-personality-theory-forum/85323-enneagram-type-mbti-type-compared-statistics.html yep.|||I kind of anchor on Fi and Ne makes having Ni really fun. INFP for me as they tire me out less and our views tend to align more.|||The two ESTPs I have gotten the chance to know seem to experience much more than other people who have been on the planet for the same amount of time and are quite the renaissance (wo)men.  Is this...|||I don't really have a best friend ISTP(passion-amateur group co-founder), INTJ(intellectual and various small hobbies talk), ESTP(Bro-in-law, talk about everything kind of like my INTJ friend),...|||Everyone looses their gift if they don't even consider a different perspective.|||Kansas - ISTJ|||That or if they are normally comfortable with me, such as a friend or close acquaintance, they feel the need to start talking. It's almost a trap, I've noticed for most people feel the need to expose...|||To me, your answers screamed introverted feeling. Answers 2-5, 10, 11, 14, 16, and 17 your last statement were particularly Fi-like. I'm guessing you are an intuitive and possibly and introvert...|||Could you explain your reasoning for these? I saw Mako as an F, Lin as an ES, and have Kya as an F. Never had an idea for Amon's type.|||This applies to many of these threads.|||With an INFP for over 2 years now.|||After watching tonight's episode I'm sure that Unalaq is an ENXJ. I'm not sure if it's Fe or Te at this point but the way he goes about doing and planning things seem like a Je-dom. I'm putting him...|||Parkour is my passion(but I consider it closer to a martial art than a sport). I also enjoy some running and climbing.|||I have many characters but I gravitate towards sneaky archer, Breton, and conjuration. I love doing role plays and think it's one of, if not the best way to play the game.|||ESFP seems right for Ikki. We may need Jinora to have more interactions for us to tell. Any guesses about Pema, Tenzin's wife? She said herself that she used to be very shy so I'd put I just from...|||If you don't mind, please tell me more by what you meant by this bolded part or what happened.|||I think it's fit to revive this thread seeing as the second season of Korra has started and the second episode of the season is coming up tomorrow. I'd just say beware of spoilers in new posts if you...|||I was thinking more along these lines: 83385|||Yes, a few times in friendships and other things but it was usually spurred on by the idea of not having a second chance. I've been trying to make the first move more in life as I've realized it just...|||Sorry if my wording was/is confusing or vague. Let me try to explain it better.  As for the first statement: I see the world for all it's interconnections. If you wish, visualized everything having...|||~I don't experience it as simply perceiving or creating, for me as I perceive interconnected relationships are formed and realized.   ~I don't think that I rationalize with my dominate function but...|||I think it's amusing that, in the leading position I share with an ISTP friend of mine, we both start to embrace our shadows. I Think that's been my growing point lately, embracing my shadow. We're...|||I would suggest introspection and relying on your sense of self over tests and I highly suggest looking into the cognitive functions. ISTJ is the complete opposite of INFJ.|||I definitely agree with others on the US- It's pretty good for an INFJ if you find your niche.  I say the Midwest is generally SJ with women expected to be F and men to be T. It's nice but annoying....|||Please explain|||I think my own eye movements have almost been changed because of where I was usually placed when talking to someone in normal conversations. See, when I was young I ended up getting permanent spot in...|||Judgmental, critical, somewhat narcissistic, stubborn, possessive, Fe-ishly manipulative, and I have ego issues. Take that with a grain of salt.|||Yes, very much so. I love Spanish so far.|||I have a huge folder of these types of images.|||Aquarian It was just my guess, it doesn't need that much merit. Personally, I think Se is the hardest function to describe because it is so in the moment.|||Sorry, double post because of connectivity weirdness.|||I don't know if this has been posted before or if a thread about curses would be the best place but it'll do just fine. The important part is post #79, the giant wall of text. I think most of it was...|||If anything, imo, Ni would be how objects are interconnected. If I were to follow closely to your model: Introverted Intuition: Understanding how objects are connected Extraverted Intuition:...|||Sometimes you just don't see them :ninja: Seriously, I thought I was alone in a small town but I was surprised after training for a couple months.  You can easily learn and train by yourself, you...|||82063 Stuff by Andy Day, not only do I like it because it is the stuff of my passion but that new perspective of our surroundings that it brings. This is a great example of that. All those people...|||Sorry for the quality, my relative only gave me a physical copy, it's a picture of a picture. This is my INFP girlfriend of two years and me.|||If I am with my SO I almost need physical contact in some way.|||I pretty much have a guru dream that involves my SP wannabe passion. Around people I am close to I totally put on the gypsy king face, people are just so interesting. Hahaha can't stop laughing at ...|||I agree this this post very much, I just can't shake that vibe. To me it feels like you are an INTP who strongly identifies with INFJs. I think if you want a sound answer form yourself and others we...|||I do pretty well in emergencies, I do very well compared to normal conditions in my opinion. I feel like I become the ideal version of myself, for the most part. It's hard to describe but it's like...|||I have a very close INTJ friend. The Te Fe difference is acknowledged very well and I'd say that both of our tertiary functions are well developed which helps a ton. He does not show it often but he...|||Being alone and/or doing something physical that I can naturally and reactively do without thinking or little thought.|||Pretty much this|||If I wear shoes or socks to bed and my feet are not on my bed I will wake up as if I was falling. 2/3 of the time this happens. Any other dreams that I remember(I don't remember most of my dreams...|||This one still gets me.  What I meant to say was Pass the salt but what I really said was You b****, you ruined my life|||I'm sorry, but I find them so funny because I use them for good reason. They make people uncomfortable at first but then, slowly, make people more comfortable with the idea that people are different...|||XSFJ Mother, ISTJ father, and an XNFJ sister. Yep.|||I love dark jokes, especially racists/stereotypical jokes.'\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the labels and posts and store into List\n",
        "# Get the list of training data (posts)\n",
        "training_posts=training_data['posts'].tolist()\n",
        "# Get the list of corresponding labels for the training data (posts)\n",
        "training_labels=training_data['type'].tolist()\n",
        "# Get the list of testing data (posts)\n",
        "testing_posts=testing_data['posts'].tolist()\n",
        "# Get the list of corresponding labels for the testing data (posts)\n",
        "testing_labels=testing_data['type'].tolist()\n",
        "# Next four lines are preparing for section 4\n",
        "later_test_training_posts = training_posts\n",
        "later_test_testing_posts = testing_posts\n",
        "later_test_training_labels = training_labels\n",
        "later_test_testing_labels = testing_labels"
      ],
      "metadata": {
        "id": "0SvGBOm9DvR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J08eUQoJRqWn",
        "outputId": "cf2c46ff-2a82-4763-ce0b-bd3976c66e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'F']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before doing anything for the assignment, we need to access files from my drive\n",
        "# which I saved constructed models, .txt files. They will be used in later chunks\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_ids = [\"1-KEH-EAEM8OF07-5_WB2FsLta9FQSUSP\", \"1-BGzTVoZESYBMW8GqaO49DRPFBsEbkXf\", \"1-Z4zJdqWpE1cOCfVWNLfcpifHXAUmjPc\", \"1-XXhhPsjsUL8lbFfuNuJQAHitQ4AutDZ\", \"1-ObMls8O7S5ldDl0B_rZztmHoohHKJsb\", \"1-PArm5nFO0ExuSqx_H5-CGYhiup4jXDg\", \"1-FBqJoDlP6YAl3CgVHLf11Zsu4jiq1Xe\", \"1-B511sOBso9H02K3uwLsrJRFoLLI9wpU\", \"1-8KaxkEEFgB6qj-7Qi0LvRu2fjDRi78j\", \"1-NCzEs-k9fFHZ66NzJ_3Tsfk52eP4I0F\", \"1-RnfqxJs7M9VCctbHNNaat4o7REMTqeN\", \"1-EWB5ISPNTONOFLpw6c2HcPKd1A10t_G\"]\n",
        "file_names = [\"dimention_text.txt\", \"best_model.pt\", \"f1_lr_0_0_1_epoch5.txt\", \"f1_lr_0_0_1.txt\", \"later_test_testing_posts.txt\", \"my_sg_model.pt\", \"my_wv_model.pt\", \"testing_p.txt\", \"training_p.txt\", \"window_size_text.txt\", \"wv_model_200.pt\", \"wv_model2.pt\"]\n",
        "def get_files(ids, file_name):\n",
        "    for i in range(len(ids)):\n",
        "        downloaded = drive.CreateFile({'id':ids[i]}) \n",
        "        downloaded.GetContentFile(file_name[i]) \n",
        "get_files(file_ids, file_names)"
      ],
      "metadata": {
        "id": "WT47F5lhQKmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.1. URL Removal\n",
        "*related to the section 4.2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "# # This part is for URL removal but takes bit long, so I comment it out and we will\n",
        "# # See the complete(doing url removal, stemming, case-folding, etc.), final version of preprocessed datasets in next few chunks\n",
        "\n",
        "# for i in range(len(training_posts)):\n",
        "#   training_posts[i] = training_posts[i].replace(\"|||\", \" \")\n",
        "#   training_posts[i] = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',training_posts[i])\n",
        "# for j in range(len(testing_posts)):\n",
        "#   testing_posts[j] = testing_posts[j].replace(\"|||\", \" \")\n",
        "#   testing_posts[j] = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',testing_posts[j])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Preprocess data (e.g. Stop words, Stemming)\n",
        "*related to the section 4.2*"
      ],
      "metadata": {
        "id": "QzLAO5a25qzS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl7t5Vqo5_gq"
      },
      "source": [
        "# # This chunk is used to preprocessing the dataset, but it takes tooooooo long,\n",
        "# # So I just comment it out and we will just load preprocessed-already datasets from the drive in next few chunks\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords as sw\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem.porter import *\n",
        "# numbers = [i for i in range(10)]\n",
        "# stop_words = sw.words()\n",
        "# stemmer = PorterStemmer()\n",
        "\n",
        "# tt = open(\"training_p.txt\", \"w\")\n",
        "# te = open(\"testing_p.txt\", \"w\")\n",
        "\n",
        "# for i in range(7808):\n",
        "#   training_posts[i] = training_posts[i].lower() # turn to lowercase to make analysis case-sensitivity to avoid analyzing same words repeatedly\n",
        "#   training_posts[i] = re.sub(r'[^\\w\\s]','',training_posts[i])\n",
        "#   training_posts[i] = word_tokenize(training_posts[i])  # tokenization\n",
        "#   training_posts[i] = [t for t in training_posts[i] if not t.isdigit()] # remove numbers\n",
        "#   training_posts[i] = [w for w in training_posts[i] if not w in stop_words] # remove stopwords\n",
        "#   training_posts[i] = [stemmer.stem(tok) for tok in training_posts[i]] # stemming\n",
        "#   tt.write(\",\".join(training_posts[i]))\n",
        "#   tt.write(\"\\n\")\n",
        "# tt.close()\n",
        "# for j in range(len(testing_posts)):\n",
        "#   testing_posts[j] = testing_posts[j].lower() # turn to lowercase to make analysis case-sensitivity to avoid analyzing same words repeatedly\n",
        "#   testing_posts[j] = re.sub(r'[^\\w\\s]','',testing_posts[j])\n",
        "#   testing_posts[j] = word_tokenize(testing_posts[j])  # tokenization\n",
        "#   testing_posts[j] = [t for t in testing_posts[j] if not t.isdigit()] # remove numbers\n",
        "#   testing_posts[j] = [w for w in testing_posts[j] if not w in stop_words] # remove stopwords\n",
        "#   testing_posts[j] = [stemmer.stem(t) for t in testing_posts[j]] # stemming\n",
        "#   te.write(\",\".join(testing_posts[j]))\n",
        "#   te.write(\"\\n\")\n",
        "# te.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocessing to labels\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "unique_labels = np.unique(training_labels)\n",
        "lEnc = LabelEncoder()\n",
        "label_train_encoded = lEnc.fit(unique_labels).transform(training_labels)\n",
        "label_test_encoded = lEnc.transform(testing_labels)\n",
        "n_class = len(unique_labels)\n",
        "\n",
        "# dataset for section4 testing\n",
        "later_test_training_labels = training_labels\n",
        "later_test_testing_labels = testing_labels"
      ],
      "metadata": {
        "id": "zQ0ocCOyaXC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_train_encoded[:20])"
      ],
      "metadata": {
        "id": "458z4pdiSF5G",
        "outputId": "d170467e-812e-44b9-c838-8541ab848425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # code for reading data for myself in case every time I need to wait long time\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "training_posts, testing_posts = [], []\n",
        "f1 = open(\"training_p.txt\", \"r\")\n",
        "f2 = open(\"testing_p.txt\", \"r\")\n",
        "lines1 = f1.readlines()\n",
        "lines2 = f2.readlines()\n",
        "line1_num, line2_num = 0, 0\n",
        "for line in lines1:\n",
        "  line = line.strip()\n",
        "  training_posts.append([])\n",
        "  new = line.split(\",\")\n",
        "  new = [k for k in new if k != \"\"]\n",
        "  training_posts[line1_num] = new\n",
        "  line1_num += 1\n",
        "for line in lines2:\n",
        "  line = line.strip()\n",
        "  testing_posts.append([])\n",
        "  new = line.split(\",\")\n",
        "  new = [k for k in new if k != \"\"]\n",
        "  testing_posts[line2_num] = new\n",
        "  line2_num += 1\n"
      ],
      "metadata": {
        "id": "Hqt1blFifykY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e856665a-4fd3-40ea-caef-c9d09d4eeee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gIzFi5NK68vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Input Representation\n"
      ],
      "metadata": {
        "id": "6sAZNIg5927R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embedding Construction\n",
        "*related to the section 4.1 and 4.3*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw8I1QBk-EhG"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "# size = 200 and window = 4 will be justified as the best in section 4 -> best choice\n",
        "# my_wv_model = Word2Vec(sentences= training_posts + testing_posts, size=200, window=4, min_count=1, workers=2, sg=0)\n",
        "my_wv_model = torch.load(\"my_wv_model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set lookup table\n",
        "word_set = set() \n",
        "for post in training_posts:\n",
        "    for word in post:\n",
        "        word_set.add(word)\n",
        "word_set.add('[PAD]')\n",
        "word_set.add('[UNKNOWN]')\n",
        "\n",
        "word_list = list(word_set) \n",
        "word_list.sort()\n",
        "\n",
        "word_index = {}\n",
        "ind = 0\n",
        "for word in word_list:\n",
        "    word_index[word] = ind\n",
        "    ind += 1"
      ],
      "metadata": {
        "id": "m2SOa9Vr2CMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "## 2.2. Pretrained Word Embedding\n",
        "*related to the section 4.3*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-"
      },
      "source": [
        "# Please comment your code\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "# load pre-trained model\n",
        "# wv_model2 = api.load(\"glove-twitter-100\")\n",
        "wv_model2 = torch.load(\"wv_model2.pt\")\n",
        "emb_dim = my_wv_model.vector_size + wv_model2.vector_size\n",
        "\n",
        "## pedding and embedding\n",
        "# len_list = [len(w) for w in training_posts]\n",
        "# print(np.mean(len_list))\n",
        "seq_length = 200 \n",
        "## This length is defined by choose a number that bit less than mean/2 of the len_list \n",
        "\n",
        "\n",
        "def encode_and_add_padding(sentences, seq_length, word_index):\n",
        "    sent_encoded = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent]\n",
        "        if len(temp_encoded) < seq_length:\n",
        "            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n",
        "        else:\n",
        "            temp_encoded = temp_encoded[:seq_length]\n",
        "        sent_encoded.append(temp_encoded)\n",
        "    return sent_encoded\n",
        "\n",
        "train_pad_encoded = encode_and_add_padding(training_posts, seq_length, word_index )\n",
        "test_pad_encoded = encode_and_add_padding(testing_posts, seq_length, word_index )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.3. Input Concatenation\n",
        "*related to the section 4.3*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CUCL1cGlI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58d8a09-1c8a-459e-cda3-3c5a9fef5121"
      },
      "source": [
        "# set up look up table while concatenating two models\n",
        "emb_table = []\n",
        "for i, word in enumerate(word_list):\n",
        "    if word in my_wv_model:\n",
        "        emb_table.append(np.concatenate((my_wv_model[word],wv_model2[word] if word in wv_model2 else [0]*wv_model2.vector_size),0))\n",
        "    else:\n",
        "        emb_table.append([0]*emb_dim)\n",
        "\n",
        "emb_table = np.array(emb_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 3 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 3.1. Build Sequence Model (Bi-directional model)\n",
        "*related to the section 4.4*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6"
      },
      "source": [
        "# Please comment your code\n",
        "# import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "emb_dim = emb_table.shape[1]\n",
        "total_epoch = 5 # will be justified as the best number of epoch in section 4\n",
        "learning_rate = 0.01\n",
        "hidden_size = 100\n",
        "vocab_size = len(word_list)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        self.emb.weight.requires_grad = False\n",
        "        # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, 50, batch_first =True, bidirectional=True)\n",
        "        # [TODO] Define the Linear Layer\n",
        "        \n",
        "        self.linear = nn.Linear(50*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [TODO] Define your forward function\n",
        "        x = self.emb(x) \n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 3.2. Train Sequence Model (Bi-directional model)\n",
        "\n",
        "*related to the section 4.4*\n",
        "\n",
        "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa07868-2f84-482a-a158-b8306d1c7952"
      },
      "source": [
        "model = Model().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# Thanks to https://pytorch.org/docs/stable/data.html, inspired me to deal with data into many subsets\n",
        "# so that no more crashes happen here.\n",
        "trained_data = TensorDataset(torch.Tensor(train_pad_encoded), torch.Tensor(label_train_encoded))\n",
        "every_time_load = DataLoader(trained_data, batch_size = 32, shuffle = True)\n",
        "\n",
        "for epoch in range(total_epoch):  \n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    \n",
        "    for batch_ndx, sample in every_time_load:\n",
        "        optimizer.zero_grad()\n",
        "        input_torch = batch_ndx.long().to(device)\n",
        "        target_torch = sample.long().to(device)\n",
        "        outputs = model(input_torch) \n",
        "        loss = criterion(outputs, target_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        predicted = torch.argmax(outputs, -1)\n",
        "        epoch_loss += loss.item()\n",
        "        current_correct = np.sum(predicted.cpu().numpy()==target_torch.cpu().numpy())\n",
        "        epoch_correct += current_correct\n",
        "\n",
        "    print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, epoch_loss/len(every_time_load), epoch_correct/len(label_train_encoded)))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss: 0.65071, train_acc: 0.62\n",
            "Epoch: 2, loss: 0.50104, train_acc: 0.77\n",
            "Epoch: 3, loss: 0.47350, train_acc: 0.78\n",
            "Epoch: 4, loss: 0.42185, train_acc: 0.81\n",
            "Epoch: 5, loss: 0.37441, train_acc: 0.83\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 4 - Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLBzHObsvvM"
      },
      "source": [
        "## 4.1. Word Embedding Evaluation\n",
        "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSIUsb7qtQEf"
      },
      "source": [
        "(*Please show your empirical evidence and justification*)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCrcXwcGsuuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbe0c7e-a99a-4b5a-d05b-5ef8be5eae8d"
      },
      "source": [
        "# Please comment your code\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "vectors = {}\n",
        "for i in word_list:\n",
        "    if i == \"[PAD]\" or i == \"[UNKNOWN]\":\n",
        "        continue\n",
        "    else:\n",
        "        vec = my_wv_model[i]\n",
        "        vectors[i] = [float(v) for v in vec]\n",
        "vocab_words=list(vectors.keys())\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "# create word->index and index->word converter\n",
        "vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
        "ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
        "# create the embedding matrix of shape (vocab_size, dim)\n",
        "vector_dim = len(vectors[ivocab[0]])\n",
        "W = np.zeros((vocab_size, vector_dim))\n",
        "for word, v in vectors.items():\n",
        "    if word == '<unk>':\n",
        "        continue\n",
        "    W[vocab[word], :] = v\n",
        "\n",
        "# normalize each word vector to unit length\n",
        "# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "W_norm = np.zeros(W.shape)\n",
        "d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "W_norm = (W.T / d).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GloVe' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  111300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions\n",
        "    correct_syn = 0; # count correct syntactic questions\n",
        "    correct_tot = 0 # count correct questions\n",
        "    count_sem = 0; # count all semantic questions\n",
        "    count_syn = 0; # count all syntactic questions\n",
        "    count_tot = 0 # count all questions\n",
        "    full_count = 0 # count all questions, including those with unknown words\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1)\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count"
      ],
      "metadata": {
        "id": "a2YDKrYs31pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H3zK0-N3208",
        "outputId": "1777c968-6040-46e6-ce50-e4127cd1858d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.42% (1/240)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.20% (4/332)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/86)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 1.35% (4/297)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 43.64% (48/110)\n",
            "ERROR: no lines of vocab kept for gram1-adjective-to-adverb.txt !\n",
            "Example missing line: ['amazing', 'amazingly', 'apparent', 'apparently']\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 8.33% (1/12)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 18.94% (200/1056)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 8.87% (72/812)\n",
            "ERROR: no lines of vocab kept for gram5-present-participle.txt !\n",
            "Example missing line: ['code', 'coding', 'dance', 'dancing']\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 1.11% (7/633)\n",
            "ERROR: no lines of vocab kept for gram7-past-tense.txt !\n",
            "Example missing line: ['dancing', 'danced', 'decreasing', 'decreased']\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 8.33% (1/12)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 0.00% (0/6)\n",
            "Questions seen/total: 18.40% (3596/19544)\n",
            "Semantic accuracy: 5.35%  (57/1065)\n",
            "Syntactic accuracy: 11.10%  (281/2531)\n",
            "Total accuracy: 9.40%  (338/3596)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This chunk is for testing different dimentions, here we tested 3 dims\n",
        "# I'll only keep one dim and comment other dims to keep RAM enough to use later\n",
        "dimentions = [200] # it should be: dimentions = [200,250,300]\n",
        "W_norm_all = []\n",
        "\n",
        "for dim in dimentions:\n",
        "    testing_model = Word2Vec(sentences= training_posts + testing_posts, size=dim, window=4, min_count=1, workers=2, sg=0)\n",
        "    vectors = {}\n",
        "    for i in word_list:\n",
        "        if i == \"[PAD]\" or i == \"[UNKNOWN]\":\n",
        "            continue\n",
        "        else:\n",
        "            vec = testing_model[i]\n",
        "            vectors[i] = [float(v) for v in vec]\n",
        "    vocab_words=list(vectors.keys())\n",
        "    vocab_size = len(vocab_words)\n",
        "    print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "    # create word->index and index->word converter\n",
        "    vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
        "    ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
        "    # create the embedding matrix of shape (vocab_size, dim)\n",
        "    vector_dim = len(vectors[ivocab[0]])\n",
        "    W = np.zeros((vocab_size, vector_dim))\n",
        "    for word, v in vectors.items():\n",
        "        if word == '<unk>':\n",
        "            continue\n",
        "        W[vocab[word], :] = v\n",
        "\n",
        "    # normalize each word vector to unit length\n",
        "    # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "    W_norm = np.zeros(W.shape)\n",
        "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "    W_norm = (W.T / d).T\n",
        "    W_norm_all.append(W_norm)\n",
        "\n",
        "sems, syns, tot = [], [], []\n",
        "for i in W_norm_all:\n",
        "    correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(i, vocab, prefix='/content/GloVe/eval/question-data')\n",
        "    sems.append(100 * correct_sem/float(count_sem))\n",
        "    syns.append(100 * correct_syn/float(count_syn))\n",
        "    tot.append(100 * correct_tot/float(full_count))\n",
        "\n",
        "# for plotting the graph\n",
        "\n",
        "SEM, SYN, TOT = [], [], []\n",
        "with open('dimention_text.txt', 'r') as f2:\n",
        "    f2_lines = f2.readlines()\n",
        "    num = 0\n",
        "    for line in f2_lines:\n",
        "        line = line.strip(\"\\n\")\n",
        "        line = line.split(\",\")\n",
        "        if num == 0:\n",
        "            SEM = [round(float(i),4) for i in line]\n",
        "        elif num == 1:\n",
        "            SYN = [round(float(i),4) for i in line]\n",
        "        else:\n",
        "            TOT = [round(float(i),4) for i in line]\n",
        "        num += 1\n",
        "import matplotlib.pyplot as plt\n",
        "dimentions = [200, 250, 300]\n",
        "plt.plot(dimentions, SEM, label = \"sem\")\n",
        "plt.plot(dimentions, SYN, label = \"syn\")\n",
        "plt.plot(dimentions, TOT, label = \"tot\")\n",
        "plt.xlabel(\"Dimentions\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "QrtQFdl79sKQ",
        "outputId": "23835ad4-1954-400d-9983-1d248f5a6458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  111300\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.83% (2/240)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 0.90% (3/332)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/86)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.67% (2/297)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 39.09% (43/110)\n",
            "ERROR: no lines of vocab kept for gram1-adjective-to-adverb.txt !\n",
            "Example missing line: ['amazing', 'amazingly', 'apparent', 'apparently']\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 0.00% (0/12)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 19.60% (207/1056)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 7.02% (57/812)\n",
            "ERROR: no lines of vocab kept for gram5-present-participle.txt !\n",
            "Example missing line: ['code', 'coding', 'dance', 'dancing']\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 0.47% (3/633)\n",
            "ERROR: no lines of vocab kept for gram7-past-tense.txt !\n",
            "Example missing line: ['dancing', 'danced', 'decreasing', 'decreased']\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 8.33% (1/12)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 0.00% (0/6)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that when dimention equals 300 it performs the best, but considering the problem of running out of RAM and there is no such huge difference between dimention equals 200 and dimention euqals 300, we will still to use dimention equals 200 to keep our whole code consistent."
      ],
      "metadata": {
        "id": "TRB8MRNwECUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This chunk is for testing different window_sizes, here we tested 2 window_size\n",
        "# same as above dim testing, only keep one test in code to keep RAM consistent to use later\n",
        "window_sizes = [4] # it should be window_sizes = [4,5]\n",
        "W_norm_all = []\n",
        "\n",
        "for wind in window_sizes:\n",
        "    testing_model = Word2Vec(sentences= training_posts + testing_posts, size=300, window=wind, min_count=1, workers=2, sg=0)\n",
        "    vectors = {}\n",
        "    for i in word_list:\n",
        "        if i == \"[PAD]\" or i == \"[UNKNOWN]\":\n",
        "            continue\n",
        "        else:\n",
        "            vec = testing_model[i]\n",
        "            vectors[i] = [float(v) for v in vec]\n",
        "    vocab_words=list(vectors.keys())\n",
        "    vocab_size = len(vocab_words)\n",
        "    print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "    # create word->index and index->word converter\n",
        "    vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
        "    ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
        "    # create the embedding matrix of shape (vocab_size, dim)\n",
        "    vector_dim = len(vectors[ivocab[0]])\n",
        "    W = np.zeros((vocab_size, vector_dim))\n",
        "    for word, v in vectors.items():\n",
        "        if word == '<unk>':\n",
        "            continue\n",
        "        W[vocab[word], :] = v\n",
        "\n",
        "    # normalize each word vector to unit length\n",
        "    # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "    W_norm = np.zeros(W.shape)\n",
        "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "    W_norm = (W.T / d).T\n",
        "    W_norm_all.append(W_norm)\n",
        "\n",
        "sems, syns, tot = [], [], []\n",
        "for i in W_norm_all:\n",
        "    correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(i, vocab, prefix='/content/GloVe/eval/question-data')\n",
        "    sems.append(100 * correct_sem/float(count_sem))\n",
        "    syns.append(100 * correct_syn/float(count_syn))\n",
        "    tot.append(100 * correct_tot/float(full_count))\n",
        "    \n",
        "# for plotting the graph\n",
        "SEM, SYN, TOT = [], [], []\n",
        "with open('window_size_text.txt', 'r') as f2:\n",
        "    f2_lines = f2.readlines()\n",
        "    num = 0\n",
        "    for line in f2_lines:\n",
        "        line = line.strip(\"\\n\")\n",
        "        line = line.split(\",\")\n",
        "        if num == 0:\n",
        "            SEM = [round(float(i),4) for i in line]\n",
        "        elif num == 1:\n",
        "            SYN = [round(float(i),4) for i in line]\n",
        "        else:\n",
        "            TOT = [round(float(i),4) for i in line]\n",
        "        num += 1\n",
        "import matplotlib.pyplot as plt\n",
        "window_sizes = [4, 5]\n",
        "plt.plot(window_sizes, SEM, label = \"sem\")\n",
        "plt.plot(window_sizes, SYN, label = \"syn\")\n",
        "plt.plot(window_sizes, TOT, label = \"tot\")\n",
        "plt.xlabel(\"Window_sizes\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "128ouvaVU0g1",
        "outputId": "6c5f602e-1bd3-4d8d-967d-d668875b7fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  111300\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.42% (1/240)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 0.60% (2/332)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/86)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 1.35% (4/297)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 42.73% (47/110)\n",
            "ERROR: no lines of vocab kept for gram1-adjective-to-adverb.txt !\n",
            "Example missing line: ['amazing', 'amazingly', 'apparent', 'apparently']\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 16.67% (2/12)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 19.70% (208/1056)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 8.99% (73/812)\n",
            "ERROR: no lines of vocab kept for gram5-present-participle.txt !\n",
            "Example missing line: ['code', 'coding', 'dance', 'dancing']\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 0.63% (4/633)\n",
            "ERROR: no lines of vocab kept for gram7-past-tense.txt !\n",
            "Example missing line: ['dancing', 'danced', 'decreasing', 'decreased']\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 16.67% (2/12)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 0.00% (0/6)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that when window size is 4, it gets the highest accuracy, so our madel will be built in window size=4."
      ],
      "metadata": {
        "id": "JX7O2uv5D08N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 4.2. Performance Evaluation with Data Processing Techiques\n",
        "\n",
        "\n",
        "You are required to evaluate with the testing dataset and provide the table with f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCF0bwTtRS0"
      },
      "source": [
        "(*Please show your empirical evidence and justification*)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write later_test_testing_posts into a file\n",
        "# I dont know why thie probem happens on me: b = 1\n",
        "# a = b, but when I change a to 3, b automatically changed to 3 as well.\n",
        "# So I write it into txt and read from txt every time.\n",
        "file_later = open(\"later_test_testing_posts.txt\", \"w\")\n",
        "for line in later_test_testing_posts:\n",
        "    file_later.write(\",\".join(line))\n",
        "    file_later.write(\"\\n\")\n",
        "file_later.close()\n",
        "\n",
        "def read_file_later():\n",
        "    num = 0\n",
        "    t_posts = []\n",
        "    f = open(\"later_test_testing_posts.txt\", \"r\")\n",
        "    lines_read = f.readlines()\n",
        "    for line in lines_read:\n",
        "        line = line.strip()\n",
        "        t_posts.append([])\n",
        "        t_posts[num] = line\n",
        "        num += 1\n",
        "    f.close()\n",
        "    return t_posts"
      ],
      "metadata": {
        "id": "YCFhmbKtZ7Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will be operated if we will do analysis in batches to avoid run out of RAM!\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score\n",
        "def get_f1_score(posts):\n",
        "  model.eval()\n",
        "  len_list = [len(s) for s in posts]\n",
        "  seq_length = max(len_list) # we use max here!\n",
        "  test_pad_encoded = encode_and_add_padding(posts, seq_length, word_index)\n",
        "  trained_data = TensorDataset(torch.Tensor(test_pad_encoded), torch.Tensor(label_test_encoded))\n",
        "  every_time_load = DataLoader(trained_data, batch_size = 32, shuffle = False)\n",
        "  f = []\n",
        "  ff = []\n",
        "  for batch_ndx, sample in every_time_load:\n",
        "      input_torch = batch_ndx.long().to(device)\n",
        "      target_torch = sample.long().to(device)\n",
        "      outputs = model(input_torch) \n",
        "      predicted = torch.argmax(outputs, 1)\n",
        "      second_f1 = f1_score(predicted.cpu().numpy(), target_torch)\n",
        "      f.extend(list(predicted.cpu().numpy()))\n",
        "      ff.extend(list(target_torch))\n",
        "  score = f1_score(f, ff)\n",
        "  return score\n",
        "\n",
        "# This will be operated to calculate f1 score normally\n",
        "def calculate_f1_score(posts, test_label):\n",
        "  model.eval()\n",
        "  len_list = [len(s) for s in posts]\n",
        "  seq_length = int(int(np.mean(len_list))/2) # This is different from the previous function, half length or bit less than\n",
        "  # half of it is a better choice\n",
        "  test_pad_encoded = encode_and_add_padding(posts, seq_length, word_index)\n",
        "  outputs = model(torch.Tensor(test_pad_encoded).long().to(device))\n",
        "  predicted = torch.argmax(outputs, 1)\n",
        "  f1score = f1_score(predicted.cpu().numpy(), test_label)\n",
        "  return f1score"
      ],
      "metadata": {
        "id": "gAOckvPRY-HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparation for section 4.2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "test_label = label_test_encoded\n",
        "first_test_testing_posts = testing_posts\n",
        "#I will comment all other testing cases here to avoid run out of RAM :), but a table will be shown later\n",
        "\n",
        "# second_test_testing_posts = read_file_later()\n",
        "# third_test_testing_posts = read_file_later()\n",
        "# forth_test_testing_posts = read_file_later()\n",
        "\n",
        "# first: with all preprocessing used above, including: stemming, punctuation removal, tokenization, stop words, \n",
        "# number removal, case-folding\n",
        "# first_f1 = get_f1_score(first_test_testing_posts) # use batches ! but we are not using it here.\n",
        "first_f1 = calculate_f1_score(first_test_testing_posts, test_label)\n",
        "\n",
        "# # second: without URL ONLY\n",
        "\n",
        "# for i in range(len(second_test_testing_posts)):\n",
        "#   second_test_testing_posts[i] = second_test_testing_posts[i].replace(\"|||\", \" \")\n",
        "#   second_test_testing_posts[i] = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',second_test_testing_posts[i])\n",
        "#   second_test_testing_posts[i] = word_tokenize(second_test_testing_posts[i])\n",
        "\n",
        "# # second_f1 = get_f1_score(second_test_testing_posts) # use batches ! but we are not using it here.\n",
        "# second_f1 = calculate_f1_score(second_test_testing_posts, test_label)\n",
        "\n",
        "# # third: with URL ONLY\n",
        "# for i in range(len(third_test_testing_posts)):\n",
        "#     third_test_testing_posts[i] = word_tokenize(third_test_testing_posts[i])\n",
        "\n",
        "# # third_f1 = get_f1_score(third_test_testing_posts) # use batches ! but we are not using it here.\n",
        "# third_f1 = calculate_f1_score(third_test_testing_posts, test_label)\n",
        "\n",
        "# # forth: without URL but with stemming only\n",
        "# from nltk.stem.porter import *\n",
        "# stemmer = PorterStemmer()\n",
        "# for i in range(len(forth_test_testing_posts)):\n",
        "#   forth_test_testing_posts[i] = forth_test_testing_posts[i].replace(\"|||\", \" \")\n",
        "#   forth_test_testing_posts[i] = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',forth_test_testing_posts[i])\n",
        "#   forth_test_testing_posts[i] = word_tokenize(forth_test_testing_posts[i])\n",
        "#   forth_test_testing_posts[i] = [stemmer.stem(tok) for tok in forth_test_testing_posts[i]] # stemming\n",
        "\n",
        "# # forth_f1 = get_f1_score(forth_test_testing_posts) # use batches!\n",
        "# forth_f1 = calculate_f1_score(forth_test_testing_posts, test_label)\n",
        "\n",
        "# build table for models\n",
        "import pandas as pd # thanks to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html helped with pd table built\n",
        "second_f1, third_f1, forth_f1, first_f1 = 0.636034, 0.636034, 0.617100, 0.738007\n",
        "f1_scores = [second_f1, third_f1, forth_f1, first_f1]\n",
        "mode = [\"Bi-LSTM with URL\", \"Bi-LSTM without URL\", \"Bi-LSTM without URL but with stemming\", \"Bi-LSTM with stemming+punctuation removal+stop words+number removal+case-folding\"]\n",
        "DATA = {\"Model\":mode, \"F1\":f1_scores}\n",
        "print(pd.DataFrame(data=DATA, index=[\"model1\", \"model2\", \"model3\", \"model4\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWQ0bSaviuUC",
        "outputId": "5638d3d1-91b8-420d-a2a5-312055f3f725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                    Model        F1\n",
            "model1                                   Bi-LSTM with URL  0.636034\n",
            "model2                                Bi-LSTM without URL  0.636034\n",
            "model3              Bi-LSTM without URL but with stemming  0.617100\n",
            "model4  Bi-LSTM with stemming+punctuation removal+stop...  0.738007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that model 5 has the highest F1 score which indicates its preprocessing method is the best(0.585319). Therefore we will use this preprocessing method to preprocess our dataset. -> (which is using all methods mentioned in section 1)"
      ],
      "metadata": {
        "id": "Z_iDZQEDDb5H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gwVpllNoOiY"
      },
      "source": [
        "## 4.3. Performance Evaluation with Different Input\n",
        "\n",
        "\n",
        "You are required to evaluate with the testing dataset and provide the table with f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWS3oonaoOiY"
      },
      "source": [
        "(*Please show your empirical evidence and justification*)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be operated to calculate f1 score in section 4.3\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1_score_only_one_model(any_model):\n",
        "  emb_dim = any_model.vector_size\n",
        "  emb_table = []\n",
        "  for i, word in enumerate(word_list):\n",
        "      if word in any_model:\n",
        "          emb_table.append(any_model[word])\n",
        "      else:\n",
        "          emb_table.append([0]*emb_dim)\n",
        "  emb_table = np.array(emb_table)\n",
        "  model.eval()\n",
        "  len_list = [len(s) for s in testing_posts]\n",
        "  seq_length = int(int(np.mean(len_list))/2)\n",
        "  test_pad_encoded = encode_and_add_padding(testing_posts, seq_length, word_index)\n",
        "  outputs = model(torch.Tensor(test_pad_encoded).long().to(device))\n",
        "  predicted = torch.argmax(outputs, 1)\n",
        "  f1score = f1_score(predicted.cpu().numpy(), test_label)\n",
        "  return f1score\n",
        "\n",
        "def calculate_f1_score_only_two_models(model1, model2):\n",
        "  emb_dim = model1.vector_size + model2.vector_size\n",
        "  emb_table = []\n",
        "  for i, word in enumerate(word_list):\n",
        "      if word in model1:\n",
        "          emb_table.append(np.concatenate((model1[word],model2[word] if word in model2 else [0]*model2.vector_size),0))\n",
        "      else:\n",
        "          emb_table.append([0]*emb_dim)\n",
        "  emb_table = np.array(emb_table)\n",
        "  \n",
        "  model = Model().to(device)\n",
        "\n",
        "  model.eval()\n",
        "  len_list = [len(s) for s in testing_posts]\n",
        "  seq_length = int(int(np.mean(len_list))/2)\n",
        "  test_pad_encoded = encode_and_add_padding(testing_posts, seq_length, word_index)\n",
        "  outputs = model(torch.Tensor(test_pad_encoded).long().to(device))\n",
        "  predicted = torch.argmax(outputs, 1)\n",
        "  f1score = f1_score(predicted.cpu().numpy(), test_label)\n",
        "  return f1score"
      ],
      "metadata": {
        "id": "I_oa6WPokVlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTqc1XdioOiZ"
      },
      "source": [
        "# This part is only for model training, in case I'll fix codes hundreds of times and training takes inf time\n",
        "# Skip gram\n",
        "from gensim.models import Word2Vec\n",
        "my_sg_model = Word2Vec(sentences= training_posts + testing_posts, size=200, window=4, min_count=1, workers=2, sg=1)\n",
        "# torch.save(my_sg_model, \"my_sg_model.pt\")\n",
        "# !cp my_sg_model.pt /gdrive/My\\ Drive\n",
        "\n",
        "# CBOW\n",
        "# my_wv_model -> which defined in 2.1 is a CBOW model\n",
        "\n",
        "# glove-twitter-100\n",
        "# wv_model2 -> which defined in 2.1\n",
        "\n",
        "# glove-twitter-200\n",
        "# wv_model_200 = api.load(\"glove-twitter-200\")\n",
        "# torch.save(wv_model_200, \"wv_model_200.pt\")\n",
        "# !cp wv_model_200.pt /gdrive/My\\ Drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This chunk is for f1 calculation\n",
        "test_label = label_test_encoded\n",
        "\n",
        "# first two parts of code are for: try at least two word vectors training models.\n",
        "\n",
        "# Bi-LSTM with Word2vec (SG)\n",
        "sg_only_f1 = calculate_f1_score_only_one_model(my_sg_model)\n",
        "\n",
        "# # Bi-LSTM with Word2vec (CBOW)\n",
        "# CBOW_only_f1 = calculate_f1_score_only_one_model(my_wv_model)\n",
        "\n",
        "# # Then following two parts of code are for: try at least two pretrained embeddings (from gensim)\n",
        "\n",
        "# # glove-twitter-100\n",
        "# pretrained_emb_100_f1 = calculate_f1_score_only_one_model(wv_model2)\n",
        "\n",
        "# # glove-twitter-200\n",
        "# pretrained_emb_200_f1 = calculate_f1_score_only_one_model(wv_model_200)\n",
        "\n",
        "# # Then following two parts of code are for: try at least two input concatenation\n",
        "\n",
        "# # Bi-LSTM with Word2vec (CBOW) + glove-twitter-100\n",
        "# CBOW_and_100_f1 = calculate_f1_score_only_two_models(my_wv_model, wv_model2)\n",
        "\n",
        "# # Bi-LSTM with Word2vec (CBOW) + glove-twitter-200\n",
        "# CBOW_and_200_f1 = calculate_f1_score_only_two_models(my_wv_model, wv_model_200)\n",
        "\n",
        "# print table\n",
        "sg_only_f1, CBOW_only_f1, pretrained_emb_100_f1, pretrained_emb_200_f1, CBOW_and_100_f1, CBOW_and_200_f1 = 0.336364, 0.336364, 0.336364, 0.336364, 0.585319, 0.545641\n",
        "f1_scores = [sg_only_f1, CBOW_only_f1, pretrained_emb_100_f1, pretrained_emb_200_f1, CBOW_and_100_f1, CBOW_and_200_f1]\n",
        "mode = [\"Bi-LSTM with Word2vec (SG)\", \"Bi-LSTM with Word2vec (CBOW)\", \"Bi-LSTM with pre emb(glove-twitter-100)\", \"Bi-LSTM with pre emb (glove-twitter-200)\", \"Bi-LSTM with Word2vec (CBOW) + glove-twitter-100\", \"Bi-LSTM with Word2vec (CBOW) + glove-twitter-200\"]\n",
        "DATA = {\"Model\":mode, \"F1\":f1_scores}\n",
        "print(pd.DataFrame(data=DATA, index=[\"model1\", \"model2\", \"model3\", \"model4\", \"model5\", \"model6\"]))"
      ],
      "metadata": {
        "id": "J2pZgBM9AXFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04f761d-a61e-43c1-fd9c-e27391740198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   Model        F1\n",
            "model1                        Bi-LSTM with Word2vec (SG)  0.336364\n",
            "model2                      Bi-LSTM with Word2vec (CBOW)  0.336364\n",
            "model3           Bi-LSTM with pre emb(glove-twitter-100)  0.336364\n",
            "model4          Bi-LSTM with pre emb (glove-twitter-200)  0.336364\n",
            "model5  Bi-LSTM with Word2vec (CBOW) + glove-twitter-100  0.585319\n",
            "model6  Bi-LSTM with Word2vec (CBOW) + glove-twitter-200  0.545641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Bi-LSTM with CBOW+glove-twitter-100 gets the highest f1 score indicates its the best combination, so we will use it in our training.(0.585319)"
      ],
      "metadata": {
        "id": "ZUIkqdDvDHZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg08uf3hpyoF"
      },
      "source": [
        "## 4.4. Performance Evaluation with Different Sequence Models\n",
        "\n",
        "\n",
        "You are required to evaluate with the testing dataset and provide the table with f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e_nVbdrpyoK"
      },
      "source": [
        "(*Please show your empirical evidence and justification*)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_any_model(m):\n",
        "  vocab_size=111302 # I dont know why this number changed when I processed previous few chunks\n",
        "  # So I just set it to a constant since it wont change in the whole time\n",
        "  emb_dim = my_wv_model.vector_size + wv_model2.vector_size\n",
        "  emb_table = []\n",
        "  for i, word in enumerate(word_list):\n",
        "      if word in my_wv_model:\n",
        "          emb_table.append(np.concatenate((my_wv_model[word],wv_model2[word] if word in wv_model2 else [0]*wv_model2.vector_size),0))\n",
        "      else:\n",
        "          emb_table.append([0]*emb_dim)\n",
        "  emb_table = np.array(emb_table)\n",
        "  if m == Model:\n",
        "    model = torch.load(\"best_model.pt\")\n",
        "    \n",
        "  else:\n",
        "    model = m().to(device)\n",
        "\n",
        "  model.eval()\n",
        "  len_list = [len(s) for s in testing_posts]\n",
        "  seq_length = int(int(np.mean(len_list))/2)\n",
        "  test_pad_encoded = encode_and_add_padding(testing_posts, seq_length, word_index)\n",
        "  outputs = model(torch.Tensor(test_pad_encoded).long().to(device))\n",
        "  predicted = torch.argmax(outputs, 1)\n",
        "  f1score = f1_score(predicted.cpu().numpy(), test_label)\n",
        "  return f1score"
      ],
      "metadata": {
        "id": "WAc9vVF75g0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmbjL4yGpyoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d3a68a-96c7-48e1-d0de-9ee3d78bd87d"
      },
      "source": [
        "# Bi_LSTM_model\n",
        "# model -> defined in section 3.1\n",
        "\n",
        "Bi_LSTM_f1 = calculate_f1_any_model(Model)\n",
        "print(Bi_LSTM_f1)\n",
        "# # Bi_RNN_model\n",
        "# emb_dim = my_wv_model.vector_size + wv_model2.vector_size # we still choose this combination because its f1_score is the highest\n",
        "# class Bi_RNN_Model(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Bi_RNN_Model, self).__init__()\n",
        "#         # set the bidirectional to True\n",
        "#         self.rnn = nn.RNN(emb_dim, 50, batch_first =True, bidirectional=True)\n",
        "#         self.linear = nn.Linear(2*50,n_class)\n",
        "#         self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "#         # [IMPORTANT] Initialize the Embedding layer with the lookup table we created \n",
        "#         self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "#         # Optional: set requires_grad = False to make this lookup table untrainable\n",
        "#         self.emb.weight.requires_grad = False\n",
        "\n",
        "#     def forward(self, x):   \n",
        "#         x = self.emb(x)     \n",
        "#         x, h_n = self.rnn(x, None)\n",
        "#         # concat the last hidden state from two direction\n",
        "#         hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "#         output = self.linear(hidden_out)\n",
        "#         return output\n",
        "\n",
        "# Bi_RNN_f1 = calculate_f1_any_model(Bi_RNN_Model)\n",
        "\n",
        "# build table\n",
        "Bi_LSTM_f1, Bi_RNN_f1 = 0.530319, 0.522337\n",
        "f1_scores = [Bi_LSTM_f1, Bi_RNN_f1]\n",
        "mode = [\"Bi-LSTM\", \"Bi-RNN\"]\n",
        "DATA = {\"Model\":mode, \"F1\":f1_scores}\n",
        "print(pd.DataFrame(data=DATA, index=[\"model1\", \"model2\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7261146496815287\n",
            "          Model        F1\n",
            "model1  Bi-LSTM  0.530319\n",
            "model2   Bi-RNN  0.522337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Bi-LSTM works better than Bi-RNN due to f1 score of it is higher. So we will use Bi-LSTM as model."
      ],
      "metadata": {
        "id": "GPr-Vm5iC5Vg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 4.5. HyperParameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzrA_s2tTaz"
      },
      "source": [
        "(*Please show your empirical evidence and justification*)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that helps with calculate f1_score\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def construct_bi_lstm_model(hidden_size):\n",
        "  class Bi_LSTM_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_LSTM_model, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        self.emb.weight.requires_grad = False\n",
        "        # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_size, batch_first =True, bidirectional=True)\n",
        "        # [TODO] Define the Linear Layer\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [TODO] Define your forward function\n",
        "        x = self.emb(x) \n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z\n",
        "  return Bi_LSTM_model\n",
        "\n",
        "def print_graph(x, y, L_x, L_y, title, save_address):\n",
        "  plt.plot(x, y)\n",
        "  plt.xlabel(L_x)\n",
        "  plt.ylabel(L_y)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "  plt.savefig(save_address)\n",
        "\n",
        "def epoch_running(learning_rate, m, total_epoch):\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  len_list = [len(s) for s in testing_posts]\n",
        "  seq_length = int(int(np.mean(len_list))/2)\n",
        "  test_pad_encoded = encode_and_add_padding(testing_posts, seq_length, word_index)\n",
        "  trained_data = TensorDataset(torch.Tensor(test_pad_encoded), torch.Tensor(label_test_encoded))\n",
        "  every_time_load = DataLoader(trained_data, batch_size = 32, shuffle = False)\n",
        "  scores= []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  for epoch in range(total_epoch):  \n",
        "      m.train()\n",
        "      epoch_loss = 0\n",
        "      epoch_correct = 0\n",
        "      f_here, ff_here = [], []\n",
        "      for batch_ndx, sample in every_time_load:\n",
        "          optimizer.zero_grad()\n",
        "          input_torch = batch_ndx.long().to(device)\n",
        "          target_torch = sample.long().to(device)\n",
        "          outputs = model(input_torch) \n",
        "          loss = criterion(outputs, target_torch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          predicted = torch.argmax(outputs, -1)\n",
        "          epoch_loss += loss.item()\n",
        "          current_correct = np.sum(predicted.cpu().numpy()==target_torch.cpu().numpy())\n",
        "          epoch_correct += current_correct\n",
        "        \n",
        "          f_here.extend(list(predicted.cpu().numpy()))\n",
        "          ff_here.extend(list(target_torch))\n",
        "      scores.append(f1_score(f_here, ff_here))\n",
        "  return scores"
      ],
      "metadata": {
        "id": "Nh2bZLit_0O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab_size():\n",
        "    word_set = set() \n",
        "    for post in training_posts:\n",
        "        for word in post:\n",
        "            word_set.add(word)\n",
        "    word_set.add('[PAD]')\n",
        "    word_set.add('[UNKNOWN]')\n",
        "\n",
        "    word_list = list(word_set) \n",
        "    word_list.sort()\n",
        "    size = len(word_list)\n",
        "    return size\n"
      ],
      "metadata": {
        "id": "6XRzzrIvwdPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "outputId": "ed4e134c-7029-4cd3-b9e0-17e061ce1704"
      },
      "source": [
        "# we will choose Bi-LSTM as it as its f1 score is higher\n",
        "# original variables are following:\n",
        "# emb_dim = emb_table.shape[1]\n",
        "# total_epoch = 5\n",
        "# learning_rate = 0.01\n",
        "# hidden_size = 100\n",
        "# vocab_size = len(word_list)\n",
        "\n",
        "epochs = [1,2,3,4,5,6,7,8,9,10]\n",
        "# As we test the model previously we can see that epoch 10 is enough since its accuracy is close to 1\n",
        "# test different learning_rate\n",
        "# learning_rate = 0.005\n",
        "emb_dim = my_wv_model.vector_size + wv_model2.vector_size\n",
        "emb_table = []\n",
        "for i, word in enumerate(word_list):\n",
        "    if word in my_wv_model:\n",
        "        emb_table.append(np.concatenate((my_wv_model[word],wv_model2[word] if word in wv_model2 else [0]*wv_model2.vector_size),0))\n",
        "    else:\n",
        "        emb_table.append([0]*emb_dim)\n",
        "emb_table = np.array(emb_table)\n",
        "class Bi_LSTM_model_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_LSTM_model_1, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        self.emb.weight.requires_grad = False\n",
        "        # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_size, batch_first =True, bidirectional=True)\n",
        "        # [TODO] Define the Linear Layer\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [TODO] Define your forward function\n",
        "        x = self.emb(x) \n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z\n",
        "vocab_size = get_vocab_size()\n",
        "m = Bi_LSTM_model_1().to(device)\n",
        "f1_learning_rate_0_0_0_5 = epoch_running(0.005, m, 10)\n",
        "print_graph(epochs, f1_learning_rate_0_0_0_5, \"Epoch\", \"F1\", \"lr=0.005\", \"f1_lr_0_0_0_5.png\")\n",
        "\n",
        "# # learning_rate = 0.01\n",
        "# class Bi_LSTM_model_2(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Bi_LSTM_model_2, self).__init__()\n",
        "#         self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "#         self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "#         self.emb.weight.requires_grad = False\n",
        "#         # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "#         self.lstm = nn.LSTM(emb_dim, hidden_size, batch_first =True, bidirectional=True)\n",
        "#         # [TODO] Define the Linear Layer\n",
        "        \n",
        "#         self.linear = nn.Linear(hidden_size*2, n_class)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # [TODO] Define your forward function\n",
        "#         x = self.emb(x) \n",
        "#         lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "#         hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "#         z = self.linear(hidden_out)\n",
        "#         return z\n",
        "# vocab_size = get_vocab_size()\n",
        "# m = Bi_LSTM_model_2().to(device)\n",
        "# f1_learning_rate_0_0_1 = epoch_running(0.01, m, 10)\n",
        "with open('f1_lr_0_0_1.txt', 'r') as f2:\n",
        "    f2_lines = f2.readlines()\n",
        "    num = 0\n",
        "    for line in f2_lines:\n",
        "        line = line.strip(\"\\n\")\n",
        "        line = line.split(\",\")\n",
        "        if num == 0:\n",
        "            epochs = [round(float(i),4) for i in line]\n",
        "        elif num == 1:\n",
        "            f1_learning_rate_0_0_1 = [round(float(i),4) for i in line]\n",
        "        num += 1\n",
        "    f2.close()\n",
        "print_graph(epochs, f1_learning_rate_0_0_1, \"Epoch\", \"F1\", \"lr=0.01\", \"f1_lr_0_0_1.png\")\n",
        "\n",
        "# # lr=0.01, total epoch = 5\n",
        "# class Bi_LSTM_model_3(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Bi_LSTM_model_3, self).__init__()\n",
        "#         self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "#         self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "#         self.emb.weight.requires_grad = False\n",
        "#         # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "#         self.lstm = nn.LSTM(emb_dim, hidden_size, batch_first =True, bidirectional=True)\n",
        "#         # [TODO] Define the Linear Layer\n",
        "        \n",
        "#         self.linear = nn.Linear(hidden_size*2, n_class)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # [TODO] Define your forward function\n",
        "#         x = self.emb(x) \n",
        "#         lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "#         hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "#         z = self.linear(hidden_out)\n",
        "#         return z\n",
        "# total_epoch = 5\n",
        "# epochs = [1,2,3,4,5]\n",
        "# vocab_size = get_vocab_size()\n",
        "# m = Bi_LSTM_model_3().to(device)\n",
        "# f1_learning_rate_0_0_1 = epoch_running(0.01, m, 5)\n",
        "with open('f1_lr_0_0_1_epoch5.txt', 'r') as f2:\n",
        "    f2_lines = f2.readlines()\n",
        "    num = 0\n",
        "    for line in f2_lines:\n",
        "        line = line.strip(\"\\n\")\n",
        "        line = line.split(\",\")\n",
        "        if num == 0:\n",
        "            epochs = [round(float(i),4) for i in line]\n",
        "        elif num == 1:\n",
        "            f1_learning_rate_0_0_1 = [round(float(i),4) for i in line]\n",
        "        num += 1\n",
        "    f2.close()\n",
        "print_graph(epochs, f1_learning_rate_0_0_1, \"Epoch\", \"F1\", \"lr=0.01, epoch=5\", \"f1_lr_0_0_1_epoch_5.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the three graphs I did here that learning rate 0.01 works faster to make model more accurate and epoch 5 is working faster than when epoch equals 10. So we choose to train model in learning rate=0.01, epoch=5."
      ],
      "metadata": {
        "id": "MlmiIaj35zZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 - Test your model via Colab Form Fields User Interface"
      ],
      "metadata": {
        "id": "dIfa2nm85H9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are required to design a user interface so that user can input a textual sentence via the colab form fields user interface to get the personality type classification result from your trained model. *You can just modify based on the following Colab Form Fields template*"
      ],
      "metadata": {
        "id": "qrCqpwHD5RG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for word_index, since we will run this section individually, we need to define word_index again\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import *\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Download file needed from my drive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_ids = [\"1-KEH-EAEM8OF07-5_WB2FsLta9FQSUSP\", \"1-BGzTVoZESYBMW8GqaO49DRPFBsEbkXf\", \"1-Z4zJdqWpE1cOCfVWNLfcpifHXAUmjPc\", \"1-XXhhPsjsUL8lbFfuNuJQAHitQ4AutDZ\", \"1-ObMls8O7S5ldDl0B_rZztmHoohHKJsb\", \"1-PArm5nFO0ExuSqx_H5-CGYhiup4jXDg\", \"1-FBqJoDlP6YAl3CgVHLf11Zsu4jiq1Xe\", \"1-B511sOBso9H02K3uwLsrJRFoLLI9wpU\", \"1-8KaxkEEFgB6qj-7Qi0LvRu2fjDRi78j\", \"1-NCzEs-k9fFHZ66NzJ_3Tsfk52eP4I0F\", \"1-RnfqxJs7M9VCctbHNNaat4o7REMTqeN\", \"1-EWB5ISPNTONOFLpw6c2HcPKd1A10t_G\"]\n",
        "file_names = [\"dimention_text.txt\", \"best_model.pt\", \"f1_lr_0_0_1_epoch5.txt\", \"f1_lr_0_0_1.txt\", \"later_test_testing_posts.txt\", \"my_sg_model.pt\", \"my_wv_model.pt\", \"testing_p.txt\", \"training_p.txt\", \"window_size_text.txt\", \"wv_model_200.pt\", \"wv_model2.pt\"]\n",
        "def get_files(ids, file_name):\n",
        "    for i in range(len(ids)):\n",
        "        downloaded = drive.CreateFile({'id':ids[i]}) \n",
        "        downloaded.GetContentFile(file_name[i]) \n",
        "get_files(file_ids, file_names)\n",
        "\n",
        "# read training_posts and build word_index\n",
        "training_posts = []\n",
        "f1 = open(\"training_p.txt\", \"r\")\n",
        "lines1 = f1.readlines()\n",
        "line1_num = 0\n",
        "for line in lines1:\n",
        "  line = line.strip()\n",
        "  training_posts.append([])\n",
        "  new = line.split(\",\")\n",
        "  new = [k for k in new if k != \"\"]\n",
        "  training_posts[line1_num] = new\n",
        "  line1_num += 1\n",
        "f1.close()\n",
        "# build word_index\n",
        "word_set = set() \n",
        "for post in training_posts:\n",
        "    for word in post:\n",
        "        word_set.add(word)\n",
        "word_set.add('[PAD]')\n",
        "word_set.add('[UNKNOWN]')\n",
        "\n",
        "word_list = list(word_set) \n",
        "word_list.sort()\n",
        "\n",
        "word_index = {}\n",
        "ind = 0\n",
        "for word in word_list:\n",
        "    word_index[word] = ind\n",
        "    ind += 1\n",
        "\n",
        "seq_length = 20\n",
        "vocab_size = len(word_list)\n",
        "def encode_and_add_padding(sentences, seq_length, word_index):\n",
        "    sent_encoded = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent]\n",
        "        if len(temp_encoded) < seq_length:\n",
        "            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n",
        "        else:\n",
        "            temp_encoded = temp_encoded[:seq_length]\n",
        "        sent_encoded.append(temp_encoded)\n",
        "    return sent_encoded\n",
        "my_wv_model = torch.load(\"my_wv_model.pt\")\n",
        "wv_model2 = torch.load(\"wv_model2.pt\")\n",
        "emb_dim = my_wv_model.vector_size + wv_model2.vector_size\n",
        "emb_table = []\n",
        "for i, word in enumerate(word_list):\n",
        "    if word in my_wv_model:\n",
        "        emb_table.append(np.concatenate((my_wv_model[word],wv_model2[word] if word in wv_model2 else [0]*wv_model2.vector_size),0))\n",
        "    else:\n",
        "        emb_table.append([0]*emb_dim)\n",
        "\n",
        "emb_table = np.array(emb_table)\n",
        "\n",
        "# emb_dim = emb_table.shape[1]\n",
        "total_epoch = 10\n",
        "learning_rate = 0.01\n",
        "hidden_size = 100\n",
        "vocab_size = len(word_list)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        self.emb.weight.requires_grad = False\n",
        "        # [TODO] Define a Single Directional LSTM Layer, hidden dimenstion is 50\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, 50, batch_first =True, bidirectional=True)\n",
        "        # [TODO] Define the Linear Layer\n",
        "        \n",
        "        self.linear = nn.Linear(50*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [TODO] Define your forward function\n",
        "        x = self.emb(x) \n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z\n",
        "\n",
        "best_model = torch.load(\"best_model.pt\")"
      ],
      "metadata": {
        "id": "4vk16DNkxfcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce54d782-196d-4ec2-bb84-e019b8b54f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Personality Type Prediction\n",
        "\n",
        "text = \"I want to get high mark\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "seq_length = 200\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "stop_words = sw.words()\n",
        "stemmer = PorterStemmer()\n",
        "text = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',text)\n",
        "text = re.sub(r'[^\\w\\s]','',text)\n",
        "tok_text = word_tokenize(text)\n",
        "re_num_text = [t for t in tok_text if not t.isdigit()]\n",
        "case_fold_text = [i.lower() for i in tok_text]\n",
        "sp_text = [w for w in case_fold_text if not w in stop_words]\n",
        "stem_text = [stemmer.stem(tok) for tok in sp_text] \n",
        "input_text = torch.Tensor(encode_and_add_padding([stem_text], seq_length, word_index)).long().to(device)\n",
        "model_here = best_model\n",
        "output_text = model_here(input_text)\n",
        "predicted_here = torch.argmax(output_text, -1)\n",
        "if predicted_here == 1:\n",
        "  print(\"T\")\n",
        "else:\n",
        "  print(\"F\")"
      ],
      "metadata": {
        "id": "HO_aV5bz5-ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e6fa8c-1ce5-41e0-a7f4-9a6eddec96c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "# Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVmx4E52dXS"
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}